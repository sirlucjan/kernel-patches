From 442b9acb28e362e3054f1fafcc8f9dc55d3f626c Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Mon, 9 Nov 2020 11:08:36 +0800
Subject: [PATCH 071/204] sched/alt: Enhance best_mask_cpu() for better
 performance.

Enhance best_mask_cpu() performance when NR_CPUS <= 64.
---
 kernel/sched/alt_core.c  |  6 ++++--
 kernel/sched/alt_sched.h | 31 ++++++++++++++++++++++---------
 2 files changed, 26 insertions(+), 11 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 7cb0edc7fe8c..3a4281ba65e6 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -90,7 +90,7 @@ int sched_yield_type __read_mostly = 1;
 #ifdef CONFIG_SMP
 static cpumask_t sched_rq_pending_mask ____cacheline_aligned_in_smp;
 
-DEFINE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_CHK_LEVEL], sched_cpu_affinity_masks);
+DEFINE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_LEVELS], sched_cpu_affinity_masks);
 DEFINE_PER_CPU(cpumask_t *, sched_cpu_affinity_end_mask);
 DEFINE_PER_CPU(cpumask_t *, sched_cpu_llc_mask);
 
@@ -5867,7 +5867,7 @@ static void sched_init_topology_cpumask_early(void)
 	cpumask_t *tmp;
 
 	for_each_possible_cpu(cpu) {
-		for (level = 0; level < NR_CPU_AFFINITY_CHK_LEVEL; level++) {
+		for (level = 0; level < NR_CPU_AFFINITY_LEVELS; level++) {
 			tmp = &(per_cpu(sched_cpu_affinity_masks, cpu)[level]);
 			cpumask_copy(tmp, cpu_possible_mask);
 			cpumask_clear_cpu(cpu, tmp);
@@ -5898,6 +5898,8 @@ static void sched_init_topology_cpumask(void)
 
 		chk = &(per_cpu(sched_cpu_affinity_masks, cpu)[0]);
 
+		cpumask_copy(chk++, cpumask_of(cpu));
+
 		cpumask_complement(chk, cpumask_of(cpu));
 #ifdef CONFIG_SCHED_SMT
 		TOPOLOGY_CPUMASK(smt, topology_sibling_cpumask(cpu), false);
diff --git a/kernel/sched/alt_sched.h b/kernel/sched/alt_sched.h
index 03f8b8b1aa27..4698d6d16a2d 100644
--- a/kernel/sched/alt_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -213,30 +213,43 @@ static inline void unregister_sched_domain_sysctl(void)
 extern bool sched_smp_initialized;
 
 enum {
-	BASE_CPU_AFFINITY_CHK_LEVEL = 1,
+	ITSELF_LEVEL_SPACE_HOLDER,
 #ifdef CONFIG_SCHED_SMT
-	SMT_CPU_AFFINITY_CHK_LEVEL_SPACE_HOLDER,
+	SMT_LEVEL_SPACE_HOLDER,
 #endif
-#ifdef CONFIG_SCHED_MC
-	MC_CPU_AFFINITY_CHK_LEVEL_SPACE_HOLDER,
-#endif
-	NR_CPU_AFFINITY_CHK_LEVEL
+	COREGROUP_LEVEL_SPACE_HOLDER,
+	CORE_LEVEL_SPACE_HOLDER,
+	OTHER_LEVEL_SPACE_HOLDER,
+	NR_CPU_AFFINITY_LEVELS
 };
 
-DECLARE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_CHK_LEVEL], sched_cpu_affinity_masks);
+DECLARE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_LEVELS], sched_cpu_affinity_masks);
 
 static inline int __best_mask_cpu(int cpu, const cpumask_t *cpumask,
 				  const cpumask_t *mask)
 {
+#if NR_CPUS <= 64
+	unsigned long t;
+
+	while ((t = cpumask->bits[0] & mask->bits[0]) == 0UL)
+		mask++;
+
+	return __ffs(t);
+#else
 	while ((cpu = cpumask_any_and(cpumask, mask)) >= nr_cpu_ids)
 		mask++;
 	return cpu;
+#endif
 }
 
 static inline int best_mask_cpu(int cpu, const cpumask_t *cpumask)
 {
-	return cpumask_test_cpu(cpu, cpumask)? cpu :
-		__best_mask_cpu(cpu, cpumask, &(per_cpu(sched_cpu_affinity_masks, cpu)[0]));
+#if NR_CPUS <= 64
+	return __best_mask_cpu(cpu, cpumask, per_cpu(sched_cpu_affinity_masks, cpu));
+#else
+	return cpumask_test_cpu(cpu, cpumask) ? cpu:
+		__best_mask_cpu(cpu, cpumask, per_cpu(sched_cpu_affinity_masks, cpu) + 1);
+#endif
 }
 
 extern void flush_smp_call_function_from_idle(void);
-- 
2.33.0

