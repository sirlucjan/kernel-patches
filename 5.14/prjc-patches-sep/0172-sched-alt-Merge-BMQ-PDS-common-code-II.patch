From a8457795f8a838d4b00a69201b39a35ad3f85831 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Sun, 6 Jun 2021 09:32:26 +0000
Subject: [PATCH 172/204] sched/alt: Merge BMQ&PDS common code (II)

---
 kernel/sched/alt_core.c |  89 +++++++++++++++++++++++++++---
 kernel/sched/bmq.h      | 117 ++++++++++------------------------------
 kernel/sched/pds.h      |  96 ++-------------------------------
 3 files changed, 113 insertions(+), 189 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index db8f5b24089d..626bd8d20c4f 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -142,6 +142,8 @@ static cpumask_t sched_sg_idle_mask ____cacheline_aligned_in_smp;
 #endif
 static cpumask_t sched_rq_watermark[SCHED_BITS] ____cacheline_aligned_in_smp;
 
+static inline void requeue_task(struct task_struct *p, struct rq *rq);
+
 #ifdef CONFIG_SCHED_BMQ
 #include "bmq.h"
 #endif
@@ -171,8 +173,7 @@ static inline void sched_queue_init_idle(struct sched_queue *q,
 	list_add(&idle->sq_node, &q->heads[idle->sq_idx]);
 }
 
-
-/* water mark related functions*/
+/* water mark related functions */
 static inline void update_sched_rq_watermark(struct rq *rq)
 {
 	unsigned long watermark = find_first_bit(rq->queue.bitmap, SCHED_QUEUE_BITS);
@@ -180,8 +181,6 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 	unsigned long i;
 	int cpu;
 
-	/*printk(KERN_INFO "sched: watermark(%d) %d, last %d\n",
-	       cpu_of(rq), watermark, last_wm);*/
 	if (watermark == last_wm)
 		return;
 
@@ -216,6 +215,34 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 #endif
 }
 
+/*
+ * This routine assume that the idle task always in queue
+ */
+static inline struct task_struct *sched_rq_first_task(struct rq *rq)
+{
+	unsigned long idx = find_first_bit(rq->queue.bitmap, SCHED_QUEUE_BITS);
+	const struct list_head *head = &rq->queue.heads[sched_prio2idx(idx, rq)];
+
+	return list_first_entry(head, struct task_struct, sq_node);
+}
+
+static inline struct task_struct *
+sched_rq_next_task(struct task_struct *p, struct rq *rq)
+{
+	unsigned long idx = p->sq_idx;
+	struct list_head *head = &rq->queue.heads[idx];
+
+	if (list_is_last(&p->sq_node, head)) {
+		idx = find_next_bit(rq->queue.bitmap, SCHED_QUEUE_BITS,
+				    sched_idx2prio(idx, rq) + 1);
+		head = &rq->queue.heads[sched_prio2idx(idx, rq)];
+
+		return list_first_entry(head, struct task_struct, sq_node);
+	}
+
+	return list_next_entry(p, sq_node);
+}
+
 static inline struct task_struct *rq_runnable_task(struct rq *rq)
 {
 	struct task_struct *next = sched_rq_first_task(rq);
@@ -563,6 +590,25 @@ static inline void sched_update_tick_dependency(struct rq *rq) { }
  * Add/Remove/Requeue task to/from the runqueue routines
  * Context: rq->lock
  */
+#define __SCHED_DEQUEUE_TASK(p, rq, flags, func)		\
+	psi_dequeue(p, flags & DEQUEUE_SLEEP);			\
+	sched_info_dequeued(rq, p);				\
+								\
+	list_del(&p->sq_node);					\
+	if (list_empty(&rq->queue.heads[p->sq_idx])) {		\
+		clear_bit(sched_idx2prio(p->sq_idx, rq),	\
+			  rq->queue.bitmap);			\
+		func;						\
+	}
+
+#define __SCHED_ENQUEUE_TASK(p, rq, flags)				\
+	sched_info_queued(rq, p);					\
+	psi_enqueue(p, flags);						\
+									\
+	p->sq_idx = task_sched_prio_idx(p, rq);				\
+	list_add_tail(&p->sq_node, &rq->queue.heads[p->sq_idx]);	\
+	set_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);
+
 static inline void dequeue_task(struct task_struct *p, struct rq *rq, int flags)
 {
 	lockdep_assert_held(&rq->lock);
@@ -602,12 +648,25 @@ static inline void enqueue_task(struct task_struct *p, struct rq *rq, int flags)
 
 static inline void requeue_task(struct task_struct *p, struct rq *rq)
 {
+	int idx;
+
 	lockdep_assert_held(&rq->lock);
 	/*printk(KERN_INFO "sched: requeue(%d) %px %016llx\n", cpu_of(rq), p, p->priodl);*/
 	WARN_ONCE(task_rq(p) != rq, "sched: cpu[%d] requeue task reside on cpu%d\n",
 		  cpu_of(rq), task_cpu(p));
 
-	__SCHED_REQUEUE_TASK(p, rq, update_sched_rq_watermark(rq));
+	idx = task_sched_prio_idx(p, rq);
+
+	list_del(&p->sq_node);
+	list_add_tail(&p->sq_node, &rq->queue.heads[idx]);
+	if (idx != p->sq_idx) {
+		if (list_empty(&rq->queue.heads[p->sq_idx]))
+			clear_bit(sched_idx2prio(p->sq_idx, rq),
+				  rq->queue.bitmap);
+		p->sq_idx = idx;
+		set_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);
+		update_sched_rq_watermark(rq);
+	}
 }
 
 /*
@@ -4565,7 +4624,7 @@ EXPORT_SYMBOL(default_wake_function);
 static inline void check_task_changed(struct task_struct *p, struct rq *rq)
 {
 	/* Trigger resched if task sched_prio has been modified. */
-	if (task_on_rq_queued(p) && sched_task_need_requeue(p, rq)) {
+	if (task_on_rq_queued(p) && task_sched_prio_idx(p, rq) != p->sq_idx) {
 		requeue_task(p, rq);
 		check_preempt_curr(rq);
 	}
@@ -4755,6 +4814,24 @@ SYSCALL_DEFINE1(nice, int, increment)
 
 #endif
 
+/**
+ * task_prio - return the priority value of a given task.
+ * @p: the task in question.
+ *
+ * Return: The priority value as seen by users in /proc.
+ *
+ * sched policy         return value   kernel prio    user prio/nice
+ *
+ * (BMQ)normal, batch, idle[0 ... 53]  [100 ... 139]          0/[-20 ... 19]/[-7 ... 7]
+ * (PDS)normal, batch, idle[0 ... 39]            100          0/[-20 ... 19]
+ * fifo, rr             [-1 ... -100]     [99 ... 0]  [0 ... 99]
+ */
+int task_prio(const struct task_struct *p)
+{
+	return (p->prio < MAX_RT_PRIO) ? p->prio - MAX_RT_PRIO :
+		task_sched_prio_normal(p, task_rq(p));
+}
+
 /**
  * idle_cpu - is a given CPU idle currently?
  * @cpu: the processor in question.
diff --git a/kernel/sched/bmq.h b/kernel/sched/bmq.h
index 7299b5cc9a87..840173f29e42 100644
--- a/kernel/sched/bmq.h
+++ b/kernel/sched/bmq.h
@@ -36,6 +36,33 @@ static inline void deboost_task(struct task_struct *p)
 /*
  * Common interfaces
  */
+static inline int
+task_sched_prio_normal(const struct task_struct *p, const struct rq *rq)
+{
+	return p->prio + p->boost_prio - MAX_RT_PRIO;
+}
+
+static inline int task_sched_prio(const struct task_struct *p)
+{
+	return (p->prio < MAX_RT_PRIO)? p->prio : MAX_RT_PRIO / 2 + (p->prio + p->boost_prio) / 2;
+}
+
+static inline int
+task_sched_prio_idx(const struct task_struct *p, const struct rq *rq)
+{
+	return task_sched_prio(p);
+}
+
+static inline unsigned long sched_prio2idx(unsigned long prio, struct rq *rq)
+{
+	return prio;
+}
+
+static inline unsigned long sched_idx2prio(unsigned long idx, struct rq *rq)
+{
+	return idx;
+}
+
 static inline void sched_imp_init(void) {}
 
 static inline int normal_prio(struct task_struct *p)
@@ -46,13 +73,6 @@ static inline int normal_prio(struct task_struct *p)
 	return p->static_prio + MAX_PRIORITY_ADJ;
 }
 
-static inline int task_sched_prio(struct task_struct *p)
-{
-	return (p->prio < MAX_RT_PRIO)? p->prio : MAX_RT_PRIO / 2 + (p->prio + p->boost_prio) / 2;
-}
-
-static inline void requeue_task(struct task_struct *p, struct rq *rq);
-
 static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
 {
 	p->time_slice = sched_timeslice_ns;
@@ -71,95 +91,12 @@ inline int task_running_nice(struct task_struct *p)
 	return (p->prio + p->boost_prio > DEFAULT_PRIO + MAX_PRIORITY_ADJ);
 }
 
-/*
- * This routine used in bmq scheduler only which assume the idle task in the bmq
- */
-static inline struct task_struct *sched_rq_first_task(struct rq *rq)
-{
-	unsigned long idx = find_first_bit(rq->queue.bitmap, SCHED_QUEUE_BITS);
-	const struct list_head *head = &rq->queue.heads[idx];
-
-	return list_first_entry(head, struct task_struct, sq_node);
-}
-
-static inline struct task_struct *
-sched_rq_next_task(struct task_struct *p, struct rq *rq)
-{
-	unsigned long idx = p->sq_idx;
-	struct list_head *head = &rq->queue.heads[idx];
-
-	if (list_is_last(&p->sq_node, head)) {
-		idx = find_next_bit(rq->queue.bitmap, SCHED_QUEUE_BITS, idx + 1);
-		head = &rq->queue.heads[idx];
-
-		return list_first_entry(head, struct task_struct, sq_node);
-	}
-
-	return list_next_entry(p, sq_node);
-}
-
-#define __SCHED_DEQUEUE_TASK(p, rq, flags, func)	\
-	psi_dequeue(p, flags & DEQUEUE_SLEEP);		\
-	sched_info_dequeued(rq, p);			\
-							\
-	list_del(&p->sq_node);				\
-	if (list_empty(&rq->queue.heads[p->sq_idx])) {	\
-		clear_bit(p->sq_idx, rq->queue.bitmap);	\
-		func;					\
-	}
-
-#define __SCHED_ENQUEUE_TASK(p, rq, flags)				\
-	sched_info_queued(rq, p);					\
-	psi_enqueue(p, flags);						\
-									\
-	p->sq_idx = task_sched_prio(p);					\
-	list_add_tail(&p->sq_node, &rq->queue.heads[p->sq_idx]);	\
-	set_bit(p->sq_idx, rq->queue.bitmap)
-
-#define __SCHED_REQUEUE_TASK(p, rq, func)				\
-{									\
-	int idx = task_sched_prio(p);					\
-\
-	list_del(&p->sq_node);						\
-	list_add_tail(&p->sq_node, &rq->queue.heads[idx]);		\
-	if (idx != p->sq_idx) {						\
-		if (list_empty(&rq->queue.heads[p->sq_idx]))		\
-			clear_bit(p->sq_idx, rq->queue.bitmap);		\
-		p->sq_idx = idx;					\
-		set_bit(p->sq_idx, rq->queue.bitmap);			\
-		func;							\
-	}								\
-}
-
-static inline bool sched_task_need_requeue(struct task_struct *p, struct rq *rq)
-{
-	return (task_sched_prio(p) != p->sq_idx);
-}
-
 static void sched_task_fork(struct task_struct *p, struct rq *rq)
 {
 	p->boost_prio = (p->boost_prio < 0) ?
 		p->boost_prio + MAX_PRIORITY_ADJ : MAX_PRIORITY_ADJ;
 }
 
-/**
- * task_prio - return the priority value of a given task.
- * @p: the task in question.
- *
- * Return: The priority value as seen by users in /proc.
- *
- * sched policy         return value   kernel prio    user prio/nice/boost
- *
- * normal, batch, idle     [0 ... 53]  [100 ... 139]          0/[-20 ... 19]/[-7 ... 7]
- * fifo, rr             [-1 ... -100]     [99 ... 0]  [0 ... 99]
- */
-int task_prio(const struct task_struct *p)
-{
-	if (p->prio < MAX_RT_PRIO)
-		return (p->prio - MAX_RT_PRIO);
-	return (p->prio - MAX_RT_PRIO + p->boost_prio);
-}
-
 static void do_sched_yield_type_1(struct task_struct *p, struct rq *rq)
 {
 	p->boost_prio = MAX_PRIORITY_ADJ;
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index 5ce0a16eb454..31c6bd4d29c8 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -6,6 +6,9 @@ extern int alt_debug[20];
 
 #define NORMAL_PRIO_MOD(x)	((x) & (NORMAL_PRIO_NUM - 1))
 
+/*
+ * Common interfaces
+ */
 static inline int
 task_sched_prio_normal(const struct task_struct *p, const struct rq *rq)
 {
@@ -55,9 +58,6 @@ static inline void sched_renew_deadline(struct task_struct *p, const struct rq *
 			(MAX_PRIO - NICE_WIDTH)];
 }
 
-/*
- * Common interfaces
- */
 static inline void sched_imp_init(void)
 {
 	int i;
@@ -111,8 +111,6 @@ static inline void update_rq_time_edge(struct rq *rq)
 	}
 }
 
-static inline void requeue_task(struct task_struct *p, struct rq *rq);
-
 static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
 {
 	p->time_slice = sched_timeslice_ns;
@@ -127,99 +125,11 @@ static inline void sched_task_sanity_check(struct task_struct *p, struct rq *rq)
 		p->deadline = rq->clock + user_prio2deadline[NICE_WIDTH - 1];
 }
 
-/*
- * This routine assume that the idle task always in queue
- */
-static inline struct task_struct *sched_rq_first_task(struct rq *rq)
-{
-	unsigned long idx = find_first_bit(rq->queue.bitmap, SCHED_QUEUE_BITS);
-	const struct list_head *head = &rq->queue.heads[sched_prio2idx(idx, rq)];
-
-	return list_first_entry(head, struct task_struct, sq_node);
-}
-
-static inline struct task_struct *
-sched_rq_next_task(struct task_struct *p, struct rq *rq)
-{
-	unsigned long idx = p->sq_idx;
-	struct list_head *head = &rq->queue.heads[idx];
-
-	if (list_is_last(&p->sq_node, head)) {
-		idx = find_next_bit(rq->queue.bitmap, SCHED_QUEUE_BITS,
-				    sched_idx2prio(idx, rq) + 1);
-		head = &rq->queue.heads[sched_prio2idx(idx, rq)];
-
-		return list_first_entry(head, struct task_struct, sq_node);
-	}
-
-	return list_next_entry(p, sq_node);
-}
-
-#define __SCHED_DEQUEUE_TASK(p, rq, flags, func)		\
-	psi_dequeue(p, flags & DEQUEUE_SLEEP);			\
-	sched_info_dequeued(rq, p);				\
-								\
-	list_del(&p->sq_node);					\
-	if (list_empty(&rq->queue.heads[p->sq_idx])) {		\
-		clear_bit(sched_idx2prio(p->sq_idx, rq),	\
-			  rq->queue.bitmap);			\
-		func;						\
-	}
-
-#define __SCHED_ENQUEUE_TASK(p, rq, flags)				\
-	sched_info_queued(rq, p);					\
-	psi_enqueue(p, flags);						\
-									\
-	p->sq_idx = task_sched_prio_idx(p, rq);				\
-	list_add_tail(&p->sq_node, &rq->queue.heads[p->sq_idx]);	\
-	set_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);
-
-/*
- * Requeue a task @p to @rq
- */
-#define __SCHED_REQUEUE_TASK(p, rq, func)					\
-{\
-	int idx = task_sched_prio_idx(p, rq);					\
-\
-	list_del(&p->sq_node);							\
-	list_add_tail(&p->sq_node, &rq->queue.heads[idx]);			\
-	if (idx != p->sq_idx) {							\
-		if (list_empty(&rq->queue.heads[p->sq_idx]))			\
-			clear_bit(sched_idx2prio(p->sq_idx, rq),		\
-				  rq->queue.bitmap);				\
-		p->sq_idx = idx;						\
-		set_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);	\
-		func;								\
-	}									\
-}
-
-static inline bool sched_task_need_requeue(struct task_struct *p, struct rq *rq)
-{
-	return (task_sched_prio_idx(p, rq) != p->sq_idx);
-}
-
 static void sched_task_fork(struct task_struct *p, struct rq *rq)
 {
 	sched_renew_deadline(p, rq);
 }
 
-/**
- * task_prio - return the priority value of a given task.
- * @p: the task in question.
- *
- * Return: The priority value as seen by users in /proc.
- *
- * sched policy         return value   kernel prio    user prio/nice
- *
- * normal, batch, idle     [0 ... 39]            100          0/[-20 ... 19]
- * fifo, rr             [-1 ... -100]     [99 ... 0]  [0 ... 99]
- */
-int task_prio(const struct task_struct *p)
-{
-	return (p->prio < MAX_RT_PRIO) ? p->prio - MAX_RT_PRIO :
-		task_sched_prio_normal(p, task_rq(p));
-}
-
 static void do_sched_yield_type_1(struct task_struct *p, struct rq *rq)
 {
 	time_slice_expired(p, rq);
-- 
2.33.0

