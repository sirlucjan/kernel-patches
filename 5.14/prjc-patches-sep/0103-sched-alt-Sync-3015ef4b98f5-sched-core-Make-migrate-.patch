From 838beaba6e48ded996bd64e219fe353f59a0bf6c Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Tue, 9 Feb 2021 17:27:46 +0800
Subject: [PATCH 103/204] sched/alt: [Sync] 3015ef4b98f5 sched/core: Make
 migrate disable and CPU hotplug cooperative

---
 kernel/sched/alt_core.c  | 28 ++++++++++++++++++----------
 kernel/sched/alt_sched.h |  1 +
 2 files changed, 19 insertions(+), 10 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index c61cb93915c9..556c27911635 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -1177,7 +1177,7 @@ void migrate_disable(void)
 	}
 
 	preempt_disable();
-	/*this_rq()->nr_pinned++;*/
+	this_rq()->nr_pinned++;
 	p->migration_disabled = 1;
 	preempt_enable();
 }
@@ -1206,11 +1206,16 @@ void migrate_enable(void)
 	 */
 	barrier();
 	p->migration_disabled = 0;
-	/*this_rq()->nr_pinned--;*/
+	this_rq()->nr_pinned--;
 	preempt_enable();
 }
 EXPORT_SYMBOL_GPL(migrate_enable);
 
+static inline bool rq_has_pinned_tasks(struct rq *rq)
+{
+	return rq->nr_pinned;
+}
+
 /*
  * Per-CPU kthreads are allowed to run on !active && online CPUs, see
  * __set_cpus_allowed_ptr() and select_fallback_rq().
@@ -1791,6 +1796,11 @@ __set_cpus_allowed_ptr(struct task_struct *p,
 
 static inline void migrate_disable_switch(struct rq *rq, struct task_struct *p) { }
 
+static inline bool rq_has_pinned_tasks(struct rq *rq)
+{
+	return false;
+}
+
 #endif /* !CONFIG_SMP */
 
 static void
@@ -5934,9 +5944,8 @@ static void balance_push(struct rq *rq)
 	 * histerical raisins.
 	 */
 	if (rq->idle == push_task ||
-	    ((push_task->flags & PF_KTHREAD) && kthread_is_per_cpu(push_task))) {
-	    /*((push_task->flags & PF_KTHREAD) && kthread_is_per_cpu(push_task)) ||
-	    is_migration_disabled(push_task)) {*/
+	    ((push_task->flags & PF_KTHREAD) && kthread_is_per_cpu(push_task)) ||
+	    is_migration_disabled(push_task)) {
 
 		/*
 		 * If this is the idle task on the outgoing CPU try to wake
@@ -5949,9 +5958,8 @@ static void balance_push(struct rq *rq)
 		 * pinned and scheduled out tasks on the runqueue. They
 		 * need to leave the migrate disabled section first.
 		 */
-		if (!rq->nr_running && rcuwait_active(&rq->hotplug_wait)) {
-		/*if (!rq->nr_running && !rq_has_pinned_tasks(rq) &&
-		    rcuwait_active(&rq->hotplug_wait)) {*/
+		if (!rq->nr_running && !rq_has_pinned_tasks(rq) &&
+		    rcuwait_active(&rq->hotplug_wait)) {
 			raw_spin_unlock(&rq->lock);
 			rcuwait_wake_up(&rq->hotplug_wait);
 			raw_spin_lock(&rq->lock);
@@ -6002,8 +6010,7 @@ static void balance_hotplug_wait(void)
 	struct rq *rq = this_rq();
 
 	rcuwait_wait_event(&rq->hotplug_wait,
-			   rq->nr_running == 1,
-/*			   rq->nr_running == 1 && !rq_has_pinned_tasks(rq),*/
+			   rq->nr_running == 1 && !rq_has_pinned_tasks(rq),
 			   TASK_UNINTERRUPTIBLE);
 }
 
@@ -6233,6 +6240,7 @@ int sched_cpu_dying(unsigned int cpu)
 	sched_tick_stop(cpu);
 
 	raw_spin_lock_irqsave(&rq->lock, flags);
+	BUG_ON(rq->nr_running != 1 || rq_has_pinned_tasks(rq));
 	raw_spin_unlock_irqrestore(&rq->lock, flags);
 
 	/*
diff --git a/kernel/sched/alt_sched.h b/kernel/sched/alt_sched.h
index 30e80c4b0825..cc2739f843af 100644
--- a/kernel/sched/alt_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -133,6 +133,7 @@ struct rq {
 #ifdef CONFIG_HOTPLUG_CPU
 	struct rcuwait		hotplug_wait;
 #endif
+	unsigned int		nr_pinned;
 #endif /* CONFIG_SMP */
 #ifdef CONFIG_IRQ_TIME_ACCOUNTING
 	u64 prev_irq_time;
-- 
2.33.0

