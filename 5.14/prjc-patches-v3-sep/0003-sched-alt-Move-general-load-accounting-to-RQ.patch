From c60437cf95c8feea1d626ddbab8599d2527ecfdf Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Sun, 19 Sep 2021 16:25:34 +0000
Subject: [PATCH 3/4] sched/alt: Move general load accounting to RQ.

This commit move the general load accounting from cpufreq_schedutil to
RQ in core file.

Also implement sched_cpu_util() using the general load accounting, which
fix compilation error of missing sched_cpu_util().
---
 kernel/sched/alt_core.c          | 95 ++++++++++++++++++++++++++++++++
 kernel/sched/alt_sched.h         | 42 +++-----------
 kernel/sched/cpufreq_schedutil.c | 68 ++---------------------
 3 files changed, 107 insertions(+), 98 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 2cfc83c87d71..ee6fc0307135 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -584,6 +584,101 @@ static inline void update_rq_clock(struct rq *rq)
 	update_rq_clock_task(rq, delta);
 }
 
+/*
+ * RQ Load update routine
+ */
+#define RQ_LOAD_HISTORY_BITS		(sizeof(s32) * 8ULL)
+#define RQ_UTIL_SHIFT			(8)
+#define RQ_LOAD_HISTORY_TO_UTIL(l)	(((l) >> (RQ_LOAD_HISTORY_BITS - 1 - RQ_UTIL_SHIFT)) & 0xff)
+
+#define LOAD_BLOCK(t)		((t) >> 17)
+#define LOAD_HALF_BLOCK(t)	((t) >> 16)
+#define BLOCK_MASK(t)		((t) & ((0x01 << 18) - 1))
+#define LOAD_BLOCK_BIT(b)	(1UL << (RQ_LOAD_HISTORY_BITS - 1 - (b)))
+#define CURRENT_LOAD_BIT	LOAD_BLOCK_BIT(0)
+
+static inline void rq_load_update(struct rq *rq)
+{
+	u64 time = rq->clock;
+	u64 delta = min(LOAD_BLOCK(time) - LOAD_BLOCK(rq->load_stamp),
+			RQ_LOAD_HISTORY_BITS - 1);
+	u64 prev = !!(rq->load_history & CURRENT_LOAD_BIT);
+	u64 curr = !!cpu_rq(rq->cpu)->nr_running;
+
+	if (delta) {
+		rq->load_history = rq->load_history >> delta;
+
+		if (delta < RQ_UTIL_SHIFT) {
+			rq->load_block += (~BLOCK_MASK(rq->load_stamp)) * prev;
+			if (!!LOAD_HALF_BLOCK(rq->load_block) ^ curr)
+				rq->load_history ^= LOAD_BLOCK_BIT(delta);
+		}
+
+		rq->load_block = BLOCK_MASK(time) * prev;
+	} else {
+		rq->load_block += (time - rq->load_stamp) * prev;
+	}
+	if (prev ^ curr)
+		rq->load_history ^= CURRENT_LOAD_BIT;
+	rq->load_stamp = time;
+}
+
+unsigned long rq_load_util(struct rq *rq, unsigned long max)
+{
+	return RQ_LOAD_HISTORY_TO_UTIL(rq->load_history) * (max >> RQ_UTIL_SHIFT);
+}
+
+#ifdef CONFIG_SMP
+unsigned long sched_cpu_util(int cpu, unsigned long max)
+{
+	return rq_load_util(cpu_rq(cpu), max);
+}
+#endif /* CONFIG_SMP */
+
+#ifdef CONFIG_CPU_FREQ
+/**
+ * cpufreq_update_util - Take a note about CPU utilization changes.
+ * @rq: Runqueue to carry out the update for.
+ * @flags: Update reason flags.
+ *
+ * This function is called by the scheduler on the CPU whose utilization is
+ * being updated.
+ *
+ * It can only be called from RCU-sched read-side critical sections.
+ *
+ * The way cpufreq is currently arranged requires it to evaluate the CPU
+ * performance state (frequency/voltage) on a regular basis to prevent it from
+ * being stuck in a completely inadequate performance level for too long.
+ * That is not guaranteed to happen if the updates are only triggered from CFS
+ * and DL, though, because they may not be coming in if only RT tasks are
+ * active all the time (or there are RT tasks only).
+ *
+ * As a workaround for that issue, this function is called periodically by the
+ * RT sched class to trigger extra cpufreq updates to prevent it from stalling,
+ * but that really is a band-aid.  Going forward it should be replaced with
+ * solutions targeted more specifically at RT tasks.
+ */
+static inline void cpufreq_update_util(struct rq *rq, unsigned int flags)
+{
+	struct update_util_data *data;
+
+#ifdef CONFIG_SMP
+	rq_load_update(rq);
+#endif
+	data = rcu_dereference_sched(*per_cpu_ptr(&cpufreq_update_util_data,
+						  cpu_of(rq)));
+	if (data)
+		data->func(data, rq_clock(rq), flags);
+}
+#else
+static inline void cpufreq_update_util(struct rq *rq, unsigned int flags)
+{
+#ifdef CONFIG_SMP
+	rq_load_update(rq);
+#endif
+}
+#endif /* CONFIG_CPU_FREQ */
+
 #ifdef CONFIG_NO_HZ_FULL
 /*
  * Tick may be needed by tasks in the runqueue depending on their policy and
diff --git a/kernel/sched/alt_sched.h b/kernel/sched/alt_sched.h
index f03af9ab9123..289058a09bd5 100644
--- a/kernel/sched/alt_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -197,6 +197,7 @@ struct rq {
 	struct rcuwait		hotplug_wait;
 #endif
 	unsigned int		nr_pinned;
+
 #endif /* CONFIG_SMP */
 #ifdef CONFIG_IRQ_TIME_ACCOUNTING
 	u64 prev_irq_time;
@@ -208,6 +209,11 @@ struct rq {
 	u64 prev_steal_time_rq;
 #endif /* CONFIG_PARAVIRT_TIME_ACCOUNTING */
 
+	/* For genenal cpu load util */
+	s32 load_history;
+	u64 load_block;
+	u64 load_stamp;
+
 	/* calc_load related fields */
 	unsigned long calc_load_update;
 	long calc_load_active;
@@ -260,6 +266,8 @@ struct rq {
 #endif /* CONFIG_NO_HZ_COMMON */
 };
 
+extern unsigned long rq_load_util(struct rq *rq, unsigned long max);
+
 extern unsigned long calc_load_update;
 extern atomic_long_t calc_load_tasks;
 
@@ -572,40 +580,6 @@ static inline u64 irq_time_read(int cpu)
 
 #ifdef CONFIG_CPU_FREQ
 DECLARE_PER_CPU(struct update_util_data __rcu *, cpufreq_update_util_data);
-
-/**
- * cpufreq_update_util - Take a note about CPU utilization changes.
- * @rq: Runqueue to carry out the update for.
- * @flags: Update reason flags.
- *
- * This function is called by the scheduler on the CPU whose utilization is
- * being updated.
- *
- * It can only be called from RCU-sched read-side critical sections.
- *
- * The way cpufreq is currently arranged requires it to evaluate the CPU
- * performance state (frequency/voltage) on a regular basis to prevent it from
- * being stuck in a completely inadequate performance level for too long.
- * That is not guaranteed to happen if the updates are only triggered from CFS
- * and DL, though, because they may not be coming in if only RT tasks are
- * active all the time (or there are RT tasks only).
- *
- * As a workaround for that issue, this function is called periodically by the
- * RT sched class to trigger extra cpufreq updates to prevent it from stalling,
- * but that really is a band-aid.  Going forward it should be replaced with
- * solutions targeted more specifically at RT tasks.
- */
-static inline void cpufreq_update_util(struct rq *rq, unsigned int flags)
-{
-	struct update_util_data *data;
-
-	data = rcu_dereference_sched(*per_cpu_ptr(&cpufreq_update_util_data,
-						  cpu_of(rq)));
-	if (data)
-		data->func(data, rq_clock(rq), flags);
-}
-#else
-static inline void cpufreq_update_util(struct rq *rq, unsigned int flags) {}
 #endif /* CONFIG_CPU_FREQ */
 
 #ifdef CONFIG_NO_HZ_FULL
diff --git a/kernel/sched/cpufreq_schedutil.c b/kernel/sched/cpufreq_schedutil.c
index 4057e51cef45..f0e9c7543542 100644
--- a/kernel/sched/cpufreq_schedutil.c
+++ b/kernel/sched/cpufreq_schedutil.c
@@ -57,13 +57,6 @@ struct sugov_cpu {
 	unsigned long		bw_dl;
 	unsigned long		max;
 
-#ifdef CONFIG_SCHED_ALT
-	/* For genenal cpu load util */
-	s32			load_history;
-	u64			load_block;
-	u64			load_stamp;
-#endif
-
 	/* The field below is for single-CPU policies only: */
 #ifdef CONFIG_NO_HZ_COMMON
 	unsigned long		saved_idle_calls;
@@ -168,66 +161,21 @@ static unsigned int get_next_freq(struct sugov_policy *sg_policy,
 	return cpufreq_driver_resolve_freq(policy, freq);
 }
 
-#ifndef CONFIG_SCHED_ALT
 static void sugov_get_util(struct sugov_cpu *sg_cpu)
 {
 	struct rq *rq = cpu_rq(sg_cpu->cpu);
 	unsigned long max = arch_scale_cpu_capacity(sg_cpu->cpu);
 
 	sg_cpu->max = max;
+#ifndef CONFIG_SCHED_ALT
 	sg_cpu->bw_dl = cpu_bw_dl(rq);
 	sg_cpu->util = effective_cpu_util(sg_cpu->cpu, cpu_util_cfs(rq), max,
 					  FREQUENCY_UTIL, NULL);
-}
-
-#else /* CONFIG_SCHED_ALT */
-
-#define SG_CPU_LOAD_HISTORY_BITS	(sizeof(s32) * 8ULL)
-#define SG_CPU_UTIL_SHIFT		(8)
-#define SG_CPU_LOAD_HISTORY_SHIFT	(SG_CPU_LOAD_HISTORY_BITS - 1 - SG_CPU_UTIL_SHIFT)
-#define SG_CPU_LOAD_HISTORY_TO_UTIL(l)	(((l) >> SG_CPU_LOAD_HISTORY_SHIFT) & 0xff)
-
-#define LOAD_BLOCK(t)		((t) >> 17)
-#define LOAD_HALF_BLOCK(t)	((t) >> 16)
-#define BLOCK_MASK(t)		((t) & ((0x01 << 18) - 1))
-#define LOAD_BLOCK_BIT(b)	(1UL << (SG_CPU_LOAD_HISTORY_BITS - 1 - (b)))
-#define CURRENT_LOAD_BIT	LOAD_BLOCK_BIT(0)
-
-static void sugov_get_util(struct sugov_cpu *sg_cpu)
-{
-	unsigned long max = arch_scale_cpu_capacity(sg_cpu->cpu);
-
-	sg_cpu->max = max;
+#else
 	sg_cpu->bw_dl = 0;
-	sg_cpu->util = SG_CPU_LOAD_HISTORY_TO_UTIL(sg_cpu->load_history) *
-		(max >> SG_CPU_UTIL_SHIFT);
-}
-
-static inline void sugov_cpu_load_update(struct sugov_cpu *sg_cpu, u64 time)
-{
-	u64 delta = min(LOAD_BLOCK(time) - LOAD_BLOCK(sg_cpu->load_stamp),
-			SG_CPU_LOAD_HISTORY_BITS - 1);
-	u64 prev = !!(sg_cpu->load_history & CURRENT_LOAD_BIT);
-	u64 curr = !!cpu_rq(sg_cpu->cpu)->nr_running;
-
-	if (delta) {
-		sg_cpu->load_history = sg_cpu->load_history >> delta;
-
-		if (delta <= SG_CPU_UTIL_SHIFT) {
-			sg_cpu->load_block += (~BLOCK_MASK(sg_cpu->load_stamp)) * prev;
-			if (!!LOAD_HALF_BLOCK(sg_cpu->load_block) ^ curr)
-				sg_cpu->load_history ^= LOAD_BLOCK_BIT(delta);
-		}
-
-		sg_cpu->load_block = BLOCK_MASK(time) * prev;
-	} else {
-		sg_cpu->load_block += (time - sg_cpu->load_stamp) * prev;
-	}
-	if (prev ^ curr)
-		sg_cpu->load_history ^= CURRENT_LOAD_BIT;
-	sg_cpu->load_stamp = time;
-}
+	sg_cpu->util = rq_load_util(rq, max);
 #endif /* CONFIG_SCHED_ALT */
+}
 
 /**
  * sugov_iowait_reset() - Reset the IO boost status of a CPU.
@@ -378,10 +326,6 @@ static inline void ignore_dl_rate_limit(struct sugov_cpu *sg_cpu)
 static inline bool sugov_update_single_common(struct sugov_cpu *sg_cpu,
 					      u64 time, unsigned int flags)
 {
-#ifdef CONFIG_SCHED_ALT
-	sugov_cpu_load_update(sg_cpu, time);
-#endif /* CONFIG_SCHED_ALT */
-
 	sugov_iowait_boost(sg_cpu, time, flags);
 	sg_cpu->last_update = time;
 
@@ -502,10 +446,6 @@ sugov_update_shared(struct update_util_data *hook, u64 time, unsigned int flags)
 
 	raw_spin_lock(&sg_policy->update_lock);
 
-#ifdef CONFIG_SCHED_ALT
-	sugov_cpu_load_update(sg_cpu, time);
-#endif /* CONFIG_SCHED_ALT */
-
 	sugov_iowait_boost(sg_cpu, time, flags);
 	sg_cpu->last_update = time;
 
-- 
2.33.0.328.g8b7c11b866

