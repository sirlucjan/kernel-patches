From aae2c0d4caa2e681a633f24a5f0b7a4d5eca9676 Mon Sep 17 00:00:00 2001
From: Oleksandr Natalenko <oleksandr@natalenko.name>
Date: Sat, 29 May 2021 11:28:54 +0200
Subject: [PATCH 039/100] Revert "blk: Fix lock inversion between ioc lock and
 bfqd lock"

This reverts commit 503bfaae997c0ce41d128e945f7f95095e9e03bd.
---
 block/bfq-iosched.c      |  6 ++----
 block/blk-merge.c        | 19 +++++++++++--------
 block/blk-mq-sched.c     |  5 ++---
 block/blk-mq-sched.h     |  3 +--
 block/blk-mq.h           | 11 -----------
 block/blk.h              |  2 +-
 block/elevator.c         | 11 +++--------
 block/mq-deadline.c      |  5 +----
 include/linux/elevator.h |  3 +--
 9 files changed, 22 insertions(+), 43 deletions(-)

diff --git a/block/bfq-iosched.c b/block/bfq-iosched.c
index 695421b08..7083ef983 100644
--- a/block/bfq-iosched.c
+++ b/block/bfq-iosched.c
@@ -2317,9 +2317,9 @@ static bool bfq_bio_merge(struct request_queue *q, struct bio *bio,
 
 	ret = blk_mq_sched_try_merge(q, bio, nr_segs, &free);
 
-	spin_unlock_irq(&bfqd->lock);
 	if (free)
 		blk_mq_free_request(free);
+	spin_unlock_irq(&bfqd->lock);
 
 	return ret;
 }
@@ -5950,16 +5950,14 @@ static void bfq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 	struct bfq_queue *bfqq;
 	bool idle_timer_disabled = false;
 	unsigned int cmd_flags;
-	LIST_HEAD(free);
 
 #ifdef CONFIG_BFQ_GROUP_IOSCHED
 	if (!cgroup_subsys_on_dfl(io_cgrp_subsys) && rq->bio)
 		bfqg_stats_update_legacy_io(q, rq);
 #endif
 	spin_lock_irq(&bfqd->lock);
-	if (blk_mq_sched_try_insert_merge(q, rq, &free)) {
+	if (blk_mq_sched_try_insert_merge(q, rq)) {
 		spin_unlock_irq(&bfqd->lock);
-		blk_mq_free_requests(&free);
 		return;
 	}
 
diff --git a/block/blk-merge.c b/block/blk-merge.c
index 1398b52a2..4d97fb6dd 100644
--- a/block/blk-merge.c
+++ b/block/blk-merge.c
@@ -846,15 +846,18 @@ static struct request *attempt_front_merge(struct request_queue *q,
 	return NULL;
 }
 
-/*
- * Try to merge 'next' into 'rq'. Return true if the merge happened, false
- * otherwise. The caller is responsible for freeing 'next' if the merge
- * happened.
- */
-bool blk_attempt_req_merge(struct request_queue *q, struct request *rq,
-			   struct request *next)
+int blk_attempt_req_merge(struct request_queue *q, struct request *rq,
+			  struct request *next)
 {
-	return attempt_merge(q, rq, next);
+	struct request *free;
+
+	free = attempt_merge(q, rq, next);
+	if (free) {
+		blk_put_request(free);
+		return 1;
+	}
+
+	return 0;
 }
 
 bool blk_rq_merge_ok(struct request *rq, struct bio *bio)
diff --git a/block/blk-mq-sched.c b/block/blk-mq-sched.c
index eb657d471..fdeb9773b 100644
--- a/block/blk-mq-sched.c
+++ b/block/blk-mq-sched.c
@@ -380,10 +380,9 @@ bool __blk_mq_sched_bio_merge(struct request_queue *q, struct bio *bio,
 	return ret;
 }
 
-bool blk_mq_sched_try_insert_merge(struct request_queue *q, struct request *rq,
-				   struct list_head *free)
+bool blk_mq_sched_try_insert_merge(struct request_queue *q, struct request *rq)
 {
-	return rq_mergeable(rq) && elv_attempt_insert_merge(q, rq, free);
+	return rq_mergeable(rq) && elv_attempt_insert_merge(q, rq);
 }
 EXPORT_SYMBOL_GPL(blk_mq_sched_try_insert_merge);
 
diff --git a/block/blk-mq-sched.h b/block/blk-mq-sched.h
index 8b70de4b8..5b18ab915 100644
--- a/block/blk-mq-sched.h
+++ b/block/blk-mq-sched.h
@@ -11,8 +11,7 @@ bool blk_mq_sched_try_merge(struct request_queue *q, struct bio *bio,
 		unsigned int nr_segs, struct request **merged_request);
 bool __blk_mq_sched_bio_merge(struct request_queue *q, struct bio *bio,
 		unsigned int nr_segs);
-bool blk_mq_sched_try_insert_merge(struct request_queue *q, struct request *rq,
-				   struct list_head *free);
+bool blk_mq_sched_try_insert_merge(struct request_queue *q, struct request *rq);
 void blk_mq_sched_mark_restart_hctx(struct blk_mq_hw_ctx *hctx);
 void blk_mq_sched_restart(struct blk_mq_hw_ctx *hctx);
 
diff --git a/block/blk-mq.h b/block/blk-mq.h
index b17517ce3..3616453ca 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -282,17 +282,6 @@ static inline struct blk_plug *blk_mq_plug(struct request_queue *q,
 	return NULL;
 }
 
-/* Free all requests on the list */
-static inline void blk_mq_free_requests(struct list_head *list)
-{
-	while (!list_empty(list)) {
-		struct request *rq = list_entry_rq(list->next);
-
-		list_del_init(&rq->queuelist);
-		blk_mq_free_request(rq);
-	}
-}
-
 /*
  * For shared tag users, we track the number of currently active users
  * and attempt to provide a fair share of the tag depth for each of them.
diff --git a/block/blk.h b/block/blk.h
index 52ff5d4a3..3b53e44b9 100644
--- a/block/blk.h
+++ b/block/blk.h
@@ -224,7 +224,7 @@ ssize_t part_timeout_store(struct device *, struct device_attribute *,
 void __blk_queue_split(struct bio **bio, unsigned int *nr_segs);
 int ll_back_merge_fn(struct request *req, struct bio *bio,
 		unsigned int nr_segs);
-bool blk_attempt_req_merge(struct request_queue *q, struct request *rq,
+int blk_attempt_req_merge(struct request_queue *q, struct request *rq,
 				struct request *next);
 unsigned int blk_recalc_rq_segments(struct request *rq);
 void blk_rq_set_mixed_merge(struct request *rq);
diff --git a/block/elevator.c b/block/elevator.c
index 151b30911..3dd67b507 100644
--- a/block/elevator.c
+++ b/block/elevator.c
@@ -350,11 +350,9 @@ enum elv_merge elv_merge(struct request_queue *q, struct request **req,
  * we can append 'rq' to an existing request, so we can throw 'rq' away
  * afterwards.
  *
- * Returns true if we merged, false otherwise. 'free' will contain all
- * requests that need to be freed.
+ * Returns true if we merged, false otherwise
  */
-bool elv_attempt_insert_merge(struct request_queue *q, struct request *rq,
-			      struct list_head *free)
+bool elv_attempt_insert_merge(struct request_queue *q, struct request *rq)
 {
 	struct request *__rq;
 	bool ret;
@@ -365,10 +363,8 @@ bool elv_attempt_insert_merge(struct request_queue *q, struct request *rq,
 	/*
 	 * First try one-hit cache.
 	 */
-	if (q->last_merge && blk_attempt_req_merge(q, q->last_merge, rq)) {
-		list_add(&rq->queuelist, free);
+	if (q->last_merge && blk_attempt_req_merge(q, q->last_merge, rq))
 		return true;
-	}
 
 	if (blk_queue_noxmerges(q))
 		return false;
@@ -382,7 +378,6 @@ bool elv_attempt_insert_merge(struct request_queue *q, struct request *rq,
 		if (!__rq || !blk_attempt_req_merge(q, __rq, rq))
 			break;
 
-		list_add(&rq->queuelist, free);
 		/* The merged request could be merged with others, try again */
 		ret = true;
 		rq = __rq;
diff --git a/block/mq-deadline.c b/block/mq-deadline.c
index 59178b7f5..df34f773e 100644
--- a/block/mq-deadline.c
+++ b/block/mq-deadline.c
@@ -487,7 +487,6 @@ static void dd_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 	struct request_queue *q = hctx->queue;
 	struct deadline_data *dd = q->elevator->elevator_data;
 	const int data_dir = rq_data_dir(rq);
-	LIST_HEAD(free);
 
 	/*
 	 * This may be a requeue of a write request that has locked its
@@ -495,10 +494,8 @@ static void dd_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 	 */
 	blk_req_zone_write_unlock(rq);
 
-	if (blk_mq_sched_try_insert_merge(q, rq, &free)) {
-		blk_mq_free_requests(&free);
+	if (blk_mq_sched_try_insert_merge(q, rq))
 		return;
-	}
 
 	trace_block_rq_insert(rq);
 
diff --git a/include/linux/elevator.h b/include/linux/elevator.h
index fffc6218a..b5abf740a 100644
--- a/include/linux/elevator.h
+++ b/include/linux/elevator.h
@@ -117,8 +117,7 @@ extern void elv_merge_requests(struct request_queue *, struct request *,
 			       struct request *);
 extern void elv_merged_request(struct request_queue *, struct request *,
 		enum elv_merge);
-extern bool elv_attempt_insert_merge(struct request_queue *, struct request *,
-				     struct list_head *);
+extern bool elv_attempt_insert_merge(struct request_queue *, struct request *);
 extern struct request *elv_former_request(struct request_queue *, struct request *);
 extern struct request *elv_latter_request(struct request_queue *, struct request *);
 
-- 
2.32.0.93.g670b81a890

