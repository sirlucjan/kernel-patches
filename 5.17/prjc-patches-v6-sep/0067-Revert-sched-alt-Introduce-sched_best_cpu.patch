From bff0ba684e17a94811b5401e3bce66b2e55e0987 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Tue, 3 Nov 2020 22:13:28 +0800
Subject: [PATCH 067/270] Revert "sched/alt: Introduce sched_best_cpu()."

This reverts commit 7e6b0567a19b1f9b8beb97255bf3ffee5a287f01.
---
 kernel/sched/alt_core.c  | 51 ++++++----------------------------------
 kernel/sched/alt_sched.h | 14 +++++++++++
 kernel/sched/topology.c  | 10 +-------
 3 files changed, 22 insertions(+), 53 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index fa0ba0d55503..57d10ccf39b8 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -105,17 +105,9 @@ EXPORT_SYMBOL_GPL(sched_smt_present);
  */
 DEFINE_PER_CPU(int, sd_llc_id);
 
-enum {
-	LLC_LEVEL = 1,
-	NR_BEST_CPU_LEVEL
-};
-
-#define NR_BEST_CPU_MASK (1 << (NR_BEST_CPU_LEVEL - 1))
-
-static cpumask_t
-sched_best_cpu_masks[NR_CPUS][NR_BEST_CPU_MASK] ____cacheline_aligned_in_smp;
-
 #if NR_CPUS <= 64
+#define SCHED_CPUMASK_FIRST_BIT(mask)	(__ffs((mask).bits[0]))
+
 static inline unsigned int sched_cpumask_first_and(const struct cpumask *srcp,
 						   const struct cpumask *andp)
 {
@@ -126,35 +118,13 @@ static inline unsigned int sched_cpumask_first_and(const struct cpumask *srcp,
 
 	return nr_cpu_ids;
 }
-
-static inline unsigned int sched_best_cpu(const unsigned int cpu,
-					  const struct cpumask *m)
-{
-	cpumask_t *chk = sched_best_cpu_masks[cpu];
-	unsigned long t;
-
-	while ((t = chk->bits[0] & m->bits[0]) == 0UL)
-		chk++;
-
-	return __ffs(t);
-}
 #else
+#define SCHED_CPUMASK_FIRST_BIT(mask)	(cpumask_fist_bit(&(mask)))
 static inline unsigned int sched_cpumask_first_and(const struct cpumask *srcp,
 						   const struct cpumask *andp)
 {
 	return cpumask_first_and(srcp, andp);
 }
-
-static inline unsigned int sched_best_cpu(const unsigned int cpu,
-					  const struct cpumask *m)
-{
-	cpumask_t t, *chk = sched_best_cpu_masks[cpu];
-
-	while (!cpumask_and(&t, chk, m))
-		chk++;
-
-	return cpumask_any(t);
-}
 #endif
 
 #endif /* CONFIG_SMP */
@@ -852,7 +822,7 @@ int get_nohz_timer_target(void)
 		default_cpu = cpu;
 	}
 
-	for (mask = per_cpu(sched_cpu_affinity_masks, cpu);
+	for (mask = &(per_cpu(sched_cpu_affinity_masks, cpu)[0]);
 	     mask < per_cpu(sched_cpu_affinity_end_mask, cpu); mask++)
 		for_each_cpu_and(i, mask, housekeeping_cpumask(HK_FLAG_TIMER))
 			if (!idle_cpu(i))
@@ -1572,9 +1542,9 @@ static inline int select_task_rq(struct task_struct *p, struct rq *rq)
 	    cpumask_and(&tmp, &chk_mask, &sched_rq_watermark[IDLE_WM]) ||
 	    cpumask_and(&tmp, &chk_mask,
 			&sched_rq_watermark[task_sched_prio(p, rq) + 1]))
-		return sched_best_cpu(task_cpu(p), &tmp);
+		return SCHED_CPUMASK_FIRST_BIT(tmp);
 
-	return sched_best_cpu(task_cpu(p), &chk_mask);
+	return SCHED_CPUMASK_FIRST_BIT(chk_mask);
 }
 
 void sched_set_stop_task(int cpu, struct task_struct *stop)
@@ -3573,7 +3543,7 @@ static inline int take_other_rq_tasks(struct rq *rq, int cpu)
 	if (cpumask_empty(&sched_rq_pending_mask))
 		return 0;
 
-	affinity_mask = per_cpu(sched_cpu_affinity_masks, cpu);
+	affinity_mask = &(per_cpu(sched_cpu_affinity_masks, cpu)[0]);
 	end_mask = per_cpu(sched_cpu_affinity_end_mask, cpu);
 	do {
 		int i;
@@ -5924,10 +5894,6 @@ static void sched_init_topology_cpumask_early(void)
 		per_cpu(sched_cpu_affinity_end_mask, cpu) =
 			&(per_cpu(sched_cpu_affinity_masks, cpu)[1]);
 		/*per_cpu(sd_llc_id, cpu) = cpu;*/
-
-		for (level = 0; level < NR_BEST_CPU_MASK; level++)
-			cpumask_copy(&sched_best_cpu_masks[cpu][level],
-				     cpu_possible_mask);
 	}
 }
 
@@ -5963,9 +5929,6 @@ static void sched_init_topology_cpumask(void)
 		per_cpu(sched_cpu_affinity_end_mask, cpu) = chk;
 		printk(KERN_INFO "sched: cpu#%02d llc_id = %d\n",
 		       cpu, per_cpu(sd_llc_id, cpu));
-
-		cpumask_copy(sched_best_cpu_masks[cpu],
-			     cpu_coregroup_mask(cpu));
 	}
 }
 #endif
diff --git a/kernel/sched/alt_sched.h b/kernel/sched/alt_sched.h
index fee65eeb1405..03f8b8b1aa27 100644
--- a/kernel/sched/alt_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -225,6 +225,20 @@ enum {
 
 DECLARE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_CHK_LEVEL], sched_cpu_affinity_masks);
 
+static inline int __best_mask_cpu(int cpu, const cpumask_t *cpumask,
+				  const cpumask_t *mask)
+{
+	while ((cpu = cpumask_any_and(cpumask, mask)) >= nr_cpu_ids)
+		mask++;
+	return cpu;
+}
+
+static inline int best_mask_cpu(int cpu, const cpumask_t *cpumask)
+{
+	return cpumask_test_cpu(cpu, cpumask)? cpu :
+		__best_mask_cpu(cpu, cpumask, &(per_cpu(sched_cpu_affinity_masks, cpu)[0]));
+}
+
 extern void flush_smp_call_function_from_idle(void);
 
 #else  /* !CONFIG_SMP */
diff --git a/kernel/sched/topology.c b/kernel/sched/topology.c
index dfdd85ddaff1..e5a7a638f3fb 100644
--- a/kernel/sched/topology.c
+++ b/kernel/sched/topology.c
@@ -2546,15 +2546,7 @@ int __read_mostly		node_reclaim_distance = RECLAIM_DISTANCE;
 
 int sched_numa_find_closest(const struct cpumask *cpus, int cpu)
 {
-	const cpumask_t *mask;
-
-	if (cpumask_test_cpu(cpu, cpus))
-		return cpu;
-
-	mask = per_cpu(sched_cpu_affinity_masks, cpu);
-	while ((cpu = cpumask_any_and(cpus, mask)) >= nr_cpu_ids)
-		mask++;
-	return cpu;
+	return best_mask_cpu(cpu, cpus);
 }
 #endif /* CONFIG_NUMA */
 #endif
-- 
2.35.1.677.gabf474a5dd

