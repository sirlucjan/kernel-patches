From 2eb52908f4fa2e58469172dad9bfa1bb4a1f4252 Mon Sep 17 00:00:00 2001
From: Paolo Valente <paolo.valente@linaro.org>
Date: Mon, 13 May 2019 11:06:24 +0200
Subject: [PATCH 01/12] block, bfq, DEBUG: add logs and BUG_ONs to 5.17

Signed-off-by: Paolo Valente <paolo.valente@linaro.org>
---
 block/bfq-cgroup.c     |   83 ++-
 block/bfq-iosched.c    | 1330 ++++++++++++++++++++++++++++++++++++----
 block/bfq-iosched.h    |  145 ++++-
 block/bfq-wf2q.c       |  424 ++++++++++++-
 include/linux/blk-mq.h |    6 +-
 include/linux/sched.h  |    2 +
 6 files changed, 1816 insertions(+), 174 deletions(-)

diff --git a/block/bfq-cgroup.c b/block/bfq-cgroup.c
index 809bc612d..92a7a78c8 100644
--- a/block/bfq-cgroup.c
+++ b/block/bfq-cgroup.c
@@ -328,8 +328,11 @@ static void bfqg_put(struct bfq_group *bfqg)
 {
 	bfqg->ref--;
 
-	if (bfqg->ref == 0)
+	BFQ_BUG_ON(bfqg->ref < 0);
+	if (bfqg->ref == 0) {
+		BFQ_BUG_ON(bfqg->entity.on_st_or_in_serv);
 		kfree(bfqg);
+	}
 }
 
 static void bfqg_and_blkg_get(struct bfq_group *bfqg)
@@ -425,6 +428,8 @@ void bfq_init_entity(struct bfq_entity *entity, struct bfq_group *bfqg)
 {
 	struct bfq_queue *bfqq = bfq_entity_to_bfqq(entity);
 
+	BFQ_BUG_ON(!bfqq);
+
 	entity->weight = entity->new_weight;
 	entity->orig_weight = entity->new_weight;
 	if (bfqq) {
@@ -434,6 +439,9 @@ void bfq_init_entity(struct bfq_entity *entity, struct bfq_group *bfqg)
 		 * Make sure that bfqg and its associated blkg do not
 		 * disappear before entity.
 		 */
+		bfq_log_bfqq(bfqq->bfqd, bfqq, "getting bfqg %p and blkg\n",
+			     bfqg);
+
 		bfqg_and_blkg_get(bfqg);
 	}
 	entity->parent = bfqg->my_entity; /* NULL for root group */
@@ -541,11 +549,19 @@ static struct blkg_policy_data *bfq_pd_alloc(gfp_t gfp, struct request_queue *q,
 
 static void bfq_pd_init(struct blkg_policy_data *pd)
 {
-	struct blkcg_gq *blkg = pd_to_blkg(pd);
-	struct bfq_group *bfqg = blkg_to_bfqg(blkg);
-	struct bfq_data *bfqd = blkg->q->elevator->elevator_data;
-	struct bfq_entity *entity = &bfqg->entity;
-	struct bfq_group_data *d = blkcg_to_bfqgd(blkg->blkcg);
+	struct blkcg_gq *blkg;
+	struct bfq_group *bfqg;
+	struct bfq_data *bfqd;
+	struct bfq_entity *entity;
+	struct bfq_group_data *d;
+
+	blkg = pd_to_blkg(pd);
+	BFQ_BUG_ON(!blkg);
+	bfqg = blkg_to_bfqg(blkg);
+	bfqd = blkg->q->elevator->elevator_data;
+	BFQ_BUG_ON(bfqg == bfqd->root_group);
+	entity = &bfqg->entity;
+	d = blkcg_to_bfqgd(blkg->blkcg);
 
 	entity->orig_weight = entity->weight = entity->new_weight = d->weight;
 	entity->my_sched_data = &bfqg->sched_data;
@@ -580,6 +596,10 @@ static void bfq_group_set_parent(struct bfq_group *bfqg,
 {
 	struct bfq_entity *entity;
 
+	BFQ_BUG_ON(!parent);
+	BFQ_BUG_ON(!bfqg);
+	BFQ_BUG_ON(bfqg == parent);
+
 	entity = &bfqg->entity;
 	entity->parent = parent->my_entity;
 	entity->sched_data = &parent->sched_data;
@@ -616,10 +636,12 @@ struct bfq_group *bfq_find_set_group(struct bfq_data *bfqd,
 	for_each_entity(entity) {
 		struct bfq_group *curr_bfqg = container_of(entity,
 						struct bfq_group, entity);
+		BFQ_BUG_ON(!curr_bfqg);
 		if (curr_bfqg != bfqd->root_group) {
 			parent = bfqg_parent(curr_bfqg);
 			if (!parent)
 				parent = bfqd->root_group;
+			BFQ_BUG_ON(!parent);
 			bfq_group_set_parent(curr_bfqg, parent);
 		}
 	}
@@ -646,6 +668,11 @@ void bfq_bfqq_move(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 {
 	struct bfq_entity *entity = &bfqq->entity;
 
+	BFQ_BUG_ON(!bfq_bfqq_busy(bfqq) && !RB_EMPTY_ROOT(&bfqq->sort_list));
+	BFQ_BUG_ON(!RB_EMPTY_ROOT(&bfqq->sort_list) &&
+		   !entity->on_st_or_in_serv);
+	BFQ_BUG_ON(!bfq_bfqq_busy(bfqq) && bfqq == bfqd->in_service_queue);
+
 	/*
 	 * oom_bfqq is not allowed to move, oom_bfqq will hold ref to root_group
 	 * until elevator exit.
@@ -668,20 +695,27 @@ void bfq_bfqq_move(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 		bfq_bfqq_expire(bfqd, bfqd->in_service_queue,
 				false, BFQQE_PREEMPTED);
 
+	BFQ_BUG_ON(entity->on_st_or_in_serv && !bfq_bfqq_busy(bfqq)
+	    && &bfq_entity_service_tree(entity)->idle !=
+	       entity->tree);
+
 	if (bfq_bfqq_busy(bfqq))
 		bfq_deactivate_bfqq(bfqd, bfqq, false, false);
 	else if (entity->on_st_or_in_serv)
 		bfq_put_idle_entity(bfq_entity_service_tree(entity), entity);
+
 	bfqg_and_blkg_put(bfqq_group(bfqq));
 
 	if (entity->parent &&
-	    entity->parent->last_bfqq_created == bfqq)
+	    entity->parent->last_bfqq_created == bfqq) {
 		entity->parent->last_bfqq_created = NULL;
-	else if (bfqd->last_bfqq_created == bfqq)
+		BFQ_BUG_ON(bfqd->last_bfqq_created == bfqq);
+	} else if (bfqd->last_bfqq_created == bfqq)
 		bfqd->last_bfqq_created = NULL;
 
 	entity->parent = bfqg->my_entity;
 	entity->sched_data = &bfqg->sched_data;
+
 	/* pin down bfqg and its associated blkg  */
 	bfqg_and_blkg_get(bfqg);
 
@@ -693,6 +727,11 @@ void bfq_bfqq_move(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 
 	if (!bfqd->in_service_queue && !bfqd->rq_in_driver)
 		bfq_schedule_dispatch(bfqd);
+
+	BFQ_BUG_ON(entity->on_st_or_in_serv && !bfq_bfqq_busy(bfqq)
+	       && &bfq_entity_service_tree(entity)->idle !=
+	       entity->tree);
+
 	/* release extra ref taken above, bfqq may happen to be freed now */
 	bfq_put_queue(bfqq);
 }
@@ -730,6 +769,10 @@ static struct bfq_group *__bfq_bic_change_cgroup(struct bfq_data *bfqd,
 
 		if (entity->sched_data != &bfqg->sched_data) {
 			bic_set_bfqq(bic, NULL, 0);
+			bfq_log_bfqq(bfqd, async_bfqq,
+				     "%p %d",
+				     async_bfqq,
+				     async_bfqq->ref);
 			bfq_release_process_ref(bfqd, async_bfqq);
 		}
 	}
@@ -851,9 +894,12 @@ static void bfq_reparent_leaf_entity(struct bfq_data *bfqd,
 
 		if (!child_entity)
 			child_entity = child_sd->in_service_entity;
+		BFQ_BUG_ON(!child_entity);
 	}
 
 	bfqq = bfq_entity_to_bfqq(child_entity);
+	BFQ_BUG_ON(!bfqq);
+	BFQ_BUG_ON(!bfq_bfqq_busy(bfqq));
 	bfq_bfqq_move(bfqd, bfqq, bfqd->root_group);
 }
 
@@ -891,12 +937,20 @@ static void bfq_reparent_active_queues(struct bfq_data *bfqd,
 static void bfq_pd_offline(struct blkg_policy_data *pd)
 {
 	struct bfq_service_tree *st;
-	struct bfq_group *bfqg = pd_to_bfqg(pd);
-	struct bfq_data *bfqd = bfqg->bfqd;
-	struct bfq_entity *entity = bfqg->my_entity;
+	struct bfq_group *bfqg;
+	struct bfq_data *bfqd;
+	struct bfq_entity *entity;
 	unsigned long flags;
 	int i;
 
+	BFQ_BUG_ON(!pd);
+	bfqg = pd_to_bfqg(pd);
+	BFQ_BUG_ON(!bfqg);
+	bfqd = bfqg->bfqd;
+	BFQ_BUG_ON(bfqd && !bfqd->root_group);
+
+	entity = bfqg->my_entity;
+
 	spin_lock_irqsave(&bfqd->lock, flags);
 
 	if (!entity) /* root group */
@@ -935,7 +989,11 @@ static void bfq_pd_offline(struct blkg_policy_data *pd)
 		 * empty and gets expired.
 		 */
 		bfq_flush_idle_tree(st);
+		BFQ_BUG_ON(!RB_EMPTY_ROOT(&st->active));
+		BFQ_BUG_ON(!RB_EMPTY_ROOT(&st->idle));
 	}
+	BFQ_BUG_ON(bfqg->sched_data.next_in_service);
+	BFQ_BUG_ON(bfqg->sched_data.in_service_entity);
 
 	__bfq_deactivate_entity(entity, false);
 
@@ -958,6 +1016,7 @@ void bfq_end_wr_async(struct bfq_data *bfqd)
 
 	list_for_each_entry(blkg, &bfqd->queue->blkg_list, q_node) {
 		struct bfq_group *bfqg = blkg_to_bfqg(blkg);
+		BFQ_BUG_ON(!bfqg);
 
 		bfq_end_wr_async_queues(bfqd, bfqg);
 	}
@@ -1414,6 +1473,8 @@ void bfq_init_entity(struct bfq_entity *entity, struct bfq_group *bfqg)
 {
 	struct bfq_queue *bfqq = bfq_entity_to_bfqq(entity);
 
+	BFQ_BUG_ON(!bfqq);
+
 	entity->weight = entity->new_weight;
 	entity->orig_weight = entity->new_weight;
 	if (bfqq) {
diff --git a/block/bfq-iosched.c b/block/bfq-iosched.c
index 1dff82d34..dace517fb 100644
--- a/block/bfq-iosched.c
+++ b/block/bfq-iosched.c
@@ -218,6 +218,12 @@ const int bfq_timeout = HZ / 8;
  */
 static const unsigned long bfq_merge_time_limit = HZ/10;
 
+#define MAX_LENGTH_REASON_NAME 25
+
+static const char reason_name[][MAX_LENGTH_REASON_NAME] = {"TOO_IDLE",
+"BUDGET_TIMEOUT", "BUDGET_EXHAUSTED", "NO_MORE_REQUESTS",
+"PREEMPTED"};
+
 static struct kmem_cache *bfq_pool;
 
 /* Below this threshold (in ns), we consider thinktime immediate. */
@@ -386,6 +392,11 @@ static void bfq_put_stable_ref(struct bfq_queue *bfqq);
 
 void bic_set_bfqq(struct bfq_io_cq *bic, struct bfq_queue *bfqq, bool is_sync)
 {
+	if (bfqq && bfqq->bfqd)
+		bfq_log_bfqq(bfqq->bfqd, bfqq,
+			     "setting bfqq[%d] = %p for bic %p",
+			     is_sync, bfqq, bic);
+
 	/*
 	 * If bfqq != NULL, then a non-stable queue merge between
 	 * bic->bfqq and bfqq is happening here. This causes troubles
@@ -457,7 +468,7 @@ static struct bfq_io_cq *bfq_bic_lookup(struct request_queue *q)
 void bfq_schedule_dispatch(struct bfq_data *bfqd)
 {
 	if (bfqd->queued != 0) {
-		bfq_log(bfqd, "schedule dispatch");
+		bfq_log(bfqd, "");
 		blk_mq_run_hw_queues(bfqd->queue, true);
 	}
 }
@@ -684,8 +695,8 @@ static void bfq_limit_depth(unsigned int op, struct blk_mq_alloc_data *data)
 	if (bfqq && bfqq_request_over_limit(bfqq, limit))
 		depth = 1;
 
-	bfq_log(bfqd, "[%s] wr_busy %d sync %d depth %u",
-		__func__, bfqd->wr_busy_queues, op_is_sync(op), depth);
+	bfq_log(bfqd, "wr_busy %d sync %d depth %u",
+		bfqd->wr_busy_queues, op_is_sync(op), depth);
 	if (depth)
 		data->shallow_depth = depth;
 }
@@ -705,6 +716,7 @@ bfq_rq_pos_tree_lookup(struct bfq_data *bfqd, struct rb_root *root,
 
 		parent = *p;
 		bfqq = rb_entry(parent, struct bfq_queue, pos_node);
+		BFQ_BUG_ON(bfqq && !bfqq->next_rq);
 
 		/*
 		 * Sort strictly based on sector. Smallest to the left,
@@ -724,9 +736,10 @@ bfq_rq_pos_tree_lookup(struct bfq_data *bfqd, struct rb_root *root,
 	if (rb_link)
 		*rb_link = p;
 
-	bfq_log(bfqd, "rq_pos_tree_lookup %llu: returning %d",
-		(unsigned long long)sector,
-		bfqq ? bfqq->pid : 0);
+	if (bfqq != NULL)
+		bfq_log(bfqd, "%llu: returning %d",
+			(unsigned long long) sector,
+			bfqq ? bfq_get_first_task_pid(bfqq) : 0);
 
 	return bfqq;
 }
@@ -842,6 +855,22 @@ static bool bfq_asymmetric_scenario(struct bfq_data *bfqd,
 		(bfqd->busy_queues[0] && bfqd->busy_queues[2]) ||
 		(bfqd->busy_queues[1] && bfqd->busy_queues[2]);
 
+	if (bfqq) {
+		bfq_log_bfqq(bfqd, bfqq, "smallest %d varied %d mul_classes %d",
+			     smallest_weight,
+			     !RB_EMPTY_ROOT(&bfqd->queue_weights_tree.rb_root) &&
+			     (bfqd->queue_weights_tree.rb_root.rb_node->rb_left ||
+			      bfqd->queue_weights_tree.rb_root.rb_node->rb_right),
+			     multiple_classes_busy);
+	} else
+		bfq_log(bfqd, "varied_queue_weights %d mul_classes %d",
+			varied_queue_weights, multiple_classes_busy);
+
+#ifdef CONFIG_BFQ_GROUP_IOSCHED
+	bfq_log(bfqd, "num_groups_with_pending_reqs %u",
+		bfqd->num_groups_with_pending_reqs);
+#endif
+
 	return varied_queue_weights || multiple_classes_busy
 #ifdef CONFIG_BFQ_GROUP_IOSCHED
 	       || bfqd->num_groups_with_pending_reqs > 0
@@ -928,6 +957,11 @@ void bfq_weights_tree_add(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 inc_counter:
 	bfqq->weight_counter->num_active++;
 	bfqq->ref++;
+
+	bfq_log_bfqq(bfqq->bfqd, bfqq, "refs %d weight %d symmetric %d",
+				bfqq->ref,
+				entity->weight,
+				!bfq_asymmetric_scenario(bfqd, bfqq));
 }
 
 /*
@@ -940,9 +974,15 @@ void __bfq_weights_tree_remove(struct bfq_data *bfqd,
 			       struct bfq_queue *bfqq,
 			       struct rb_root_cached *root)
 {
+	struct bfq_entity *entity = &bfqq->entity;
+
 	if (!bfqq->weight_counter)
 		return;
 
+	BFQ_BUG_ON(RB_EMPTY_ROOT(&root->rb_root));
+	BFQ_BUG_ON(bfqq->weight_counter->weight != entity->weight);
+
+	BFQ_BUG_ON(!bfqq->weight_counter->num_active);
 	bfqq->weight_counter->num_active--;
 	if (bfqq->weight_counter->num_active > 0)
 		goto reset_entity_pointer;
@@ -952,6 +992,11 @@ void __bfq_weights_tree_remove(struct bfq_data *bfqd,
 
 reset_entity_pointer:
 	bfqq->weight_counter = NULL;
+	bfq_log_bfqq(bfqq->bfqd, bfqq,
+		     "refs %d weight %d symmetric %d",
+		     bfqq->ref,
+		     entity->weight,
+		     !bfq_asymmetric_scenario(bfqd, bfqq));
 	bfq_put_queue(bfqq);
 }
 
@@ -967,7 +1012,14 @@ void bfq_weights_tree_remove(struct bfq_data *bfqd,
 	for_each_entity(entity) {
 		struct bfq_sched_data *sd = entity->my_sched_data;
 
+		BFQ_BUG_ON(entity->sched_data == NULL); /*
+						     * It would mean
+						     * that this is
+						     * the root group.
+						     */
+
 		if (sd->next_in_service || sd->in_service_entity) {
+			BFQ_BUG_ON(!entity->in_groups_with_pending_reqs);
 			/*
 			 * entity is still active, because either
 			 * next_in_service or in_service_entity is not
@@ -982,6 +1034,8 @@ void bfq_weights_tree_remove(struct bfq_data *bfqd,
 			break;
 		}
 
+		BFQ_BUG_ON(!bfqd->num_groups_with_pending_reqs &&
+		       entity->in_groups_with_pending_reqs);
 		/*
 		 * The decrement of num_groups_with_pending_reqs is
 		 * not performed immediately upon the deactivation of
@@ -996,6 +1050,8 @@ void bfq_weights_tree_remove(struct bfq_data *bfqd,
 			entity->in_groups_with_pending_reqs = false;
 			bfqd->num_groups_with_pending_reqs--;
 		}
+		bfq_log_bfqq(bfqd, bfqq, "num_groups_with_pending_reqs %u",
+			     bfqd->num_groups_with_pending_reqs);
 	}
 
 	/*
@@ -1026,7 +1082,8 @@ static struct request *bfq_check_fifo(struct bfq_queue *bfqq,
 	if (rq == last || ktime_get_ns() < rq->fifo_time)
 		return NULL;
 
-	bfq_log_bfqq(bfqq->bfqd, bfqq, "check_fifo: returned %p", rq);
+	bfq_log_bfqq(bfqq->bfqd, bfqq, "returned %p", rq);
+	BFQ_BUG_ON(RB_EMPTY_NODE(&rq->rb_node));
 	return rq;
 }
 
@@ -1038,10 +1095,16 @@ static struct request *bfq_find_next_rq(struct bfq_data *bfqd,
 	struct rb_node *rbprev = rb_prev(&last->rb_node);
 	struct request *next, *prev = NULL;
 
+	BFQ_BUG_ON(list_empty(&bfqq->fifo));
+
 	/* Follow expired path, else get first next available. */
 	next = bfq_check_fifo(bfqq, last);
-	if (next)
+	if (next) {
+		BFQ_BUG_ON(next == last);
 		return next;
+	}
+
+	BFQ_BUG_ON(RB_EMPTY_NODE(&last->rb_node));
 
 	if (rbprev)
 		prev = rb_entry_rq(rbprev);
@@ -1061,6 +1124,9 @@ static struct request *bfq_find_next_rq(struct bfq_data *bfqd,
 static unsigned long bfq_serv_to_charge(struct request *rq,
 					struct bfq_queue *bfqq)
 {
+	BFQ_BUG_ON(!bfqq->bfqd);
+	BFQ_BUG_ON(!rq);
+
 	if (bfq_bfqq_sync(bfqq) || bfqq->wr_coeff > 1 ||
 	    bfq_asymmetric_scenario(bfqq->bfqd, bfqq))
 		return blk_rq_sectors(rq);
@@ -1083,6 +1149,7 @@ static void bfq_updated_next_req(struct bfq_data *bfqd,
 				 struct bfq_queue *bfqq)
 {
 	struct bfq_entity *entity = &bfqq->entity;
+	struct bfq_service_tree *st = bfq_entity_service_tree(entity);
 	struct request *next_rq = bfqq->next_rq;
 	unsigned long new_budget;
 
@@ -1096,13 +1163,16 @@ static void bfq_updated_next_req(struct bfq_data *bfqd,
 		 */
 		return;
 
+	BFQ_BUG_ON(entity->tree != &st->active);
+	BFQ_BUG_ON(entity == entity->sched_data->in_service_entity);
+
 	new_budget = max_t(unsigned long,
 			   max_t(unsigned long, bfqq->max_budget,
 				 bfq_serv_to_charge(next_rq, bfqq)),
 			   entity->service);
 	if (entity->budget != new_budget) {
 		entity->budget = new_budget;
-		bfq_log_bfqq(bfqd, bfqq, "updated next rq: new budget %lu",
+		bfq_log_bfqq(bfqd, bfqq, "new budget %lu",
 					 new_budget);
 		bfq_requeue_bfqq(bfqd, bfqq, false);
 	}
@@ -1171,6 +1241,8 @@ bfq_bfqq_resume_state(struct bfq_queue *bfqq, struct bfq_data *bfqd,
 	bfqq->inject_limit = bic->saved_inject_limit;
 	bfqq->decrease_time_jif = bic->saved_decrease_time_jif;
 
+	BFQ_BUG_ON(bic->saved_weight == 0);
+
 	bfqq->entity.new_weight = bic->saved_weight;
 	bfqq->ttime = bic->saved_ttime;
 	bfqq->io_start_time = bic->saved_io_start_time;
@@ -1184,8 +1256,15 @@ bfq_bfqq_resume_state(struct bfq_queue *bfqq, struct bfq_data *bfqd,
 	}
 	bfqq->service_from_wr = bic->saved_service_from_wr;
 	bfqq->wr_start_at_switch_to_srt = bic->saved_wr_start_at_switch_to_srt;
+	BFQ_BUG_ON(time_is_after_jiffies(bfqq->wr_start_at_switch_to_srt));
 	bfqq->last_wr_start_finish = bic->saved_last_wr_start_finish;
 	bfqq->wr_cur_max_time = bic->saved_wr_cur_max_time;
+	BFQ_BUG_ON(time_is_after_jiffies(bfqq->last_wr_start_finish));
+
+	bfq_log_bfqq(bfqq->bfqd, bfqq,
+		     "bic %p wr_coeff %d start_finish %lu max_time %lu",
+		     bic, bfqq->wr_coeff, bfqq->last_wr_start_finish,
+		     bfqq->wr_cur_max_time);
 
 	if (bfqq->wr_coeff > 1 && (bfq_bfqq_in_large_burst(bfqq) ||
 	    time_is_before_jiffies(bfqq->last_wr_start_finish +
@@ -1195,10 +1274,14 @@ bfq_bfqq_resume_state(struct bfq_queue *bfqq, struct bfq_data *bfqd,
 		    time_is_after_eq_jiffies(bfqq->wr_start_at_switch_to_srt +
 					     bfq_wr_duration(bfqd))) {
 			switch_back_to_interactive_wr(bfqq, bfqd);
+			bfq_log_bfqq(bfqq->bfqd, bfqq,
+			     "switching back to interactive");
 		} else {
 			bfqq->wr_coeff = 1;
 			bfq_log_bfqq(bfqq->bfqd, bfqq,
-				     "resume state: switching off wr");
+			     "switching off wr (%lu + %lu < %lu)",
+			     bfqq->last_wr_start_finish, bfqq->wr_cur_max_time,
+			     jiffies);
 		}
 	}
 
@@ -1208,17 +1291,37 @@ bfq_bfqq_resume_state(struct bfq_queue *bfqq, struct bfq_data *bfqd,
 	if (likely(!busy))
 		return;
 
-	if (old_wr_coeff == 1 && bfqq->wr_coeff > 1)
+	if (old_wr_coeff == 1 && bfqq->wr_coeff > 1) {
 		bfqd->wr_busy_queues++;
-	else if (old_wr_coeff > 1 && bfqq->wr_coeff == 1)
+		BFQ_BUG_ON(bfqd->wr_busy_queues > bfq_tot_busy_queues(bfqd));
+	} else if (old_wr_coeff > 1 && bfqq->wr_coeff == 1) {
 		bfqd->wr_busy_queues--;
+		BFQ_BUG_ON(bfqd->wr_busy_queues < 0);
+	}
 }
 
 static int bfqq_process_refs(struct bfq_queue *bfqq)
 {
-	return bfqq->ref - bfqq->entity.allocated -
-		bfqq->entity.on_st_or_in_serv -
+	int process_refs, io_refs;
+
+	lockdep_assert_held(&bfqq->bfqd->lock);
+
+	io_refs = bfqq->entity.allocated;
+	process_refs = bfqq->ref - io_refs - bfqq->entity.on_st_or_in_serv -
 		(bfqq->weight_counter != NULL) - bfqq->stable_ref;
+
+	if (bfqq->proc_ref > process_refs) {
+		pr_crit("ref %d proc_ref %d computed %d",
+			bfqq->ref, bfqq->proc_ref, process_refs);
+		pr_crit("allocated %d on_st %d weight_counter %d",
+			bfqq->entity.allocated, bfqq->entity.on_st_or_in_serv,
+			(bfqq->weight_counter != NULL));
+
+		BFQ_BUG_ON(true);
+	}
+
+	BFQ_BUG_ON(process_refs < 0);
+	return process_refs;
 }
 
 /* Empty burst list and add just bfqq (see comments on bfq_handle_burst) */
@@ -1250,6 +1353,10 @@ static void bfq_add_to_burst(struct bfq_data *bfqd, struct bfq_queue *bfqq)
 	/* Increment burst size to take into account also bfqq */
 	bfqd->burst_size++;
 
+	bfq_log_bfqq(bfqd, bfqq, "%d", bfqd->burst_size);
+
+	BFQ_BUG_ON(bfqd->burst_size > bfqd->bfq_large_burst_thresh);
+
 	if (bfqd->burst_size == bfqd->bfq_large_burst_thresh) {
 		struct bfq_queue *pos, *bfqq_item;
 		struct hlist_node *n;
@@ -1259,15 +1366,19 @@ static void bfq_add_to_burst(struct bfq_data *bfqd, struct bfq_queue *bfqq)
 		 * other to consider this burst as large.
 		 */
 		bfqd->large_burst = true;
+		bfq_log_bfqq(bfqd, bfqq, "large burst started");
 
 		/*
 		 * We can now mark all queues in the burst list as
 		 * belonging to a large burst.
 		 */
 		hlist_for_each_entry(bfqq_item, &bfqd->burst_list,
-				     burst_list_node)
+				     burst_list_node) {
 			bfq_mark_bfqq_in_large_burst(bfqq_item);
+			bfq_log_bfqq(bfqd, bfqq_item, "marked in large burst");
+		}
 		bfq_mark_bfqq_in_large_burst(bfqq);
+		bfq_log_bfqq(bfqd, bfqq, "marked in large burst");
 
 		/*
 		 * From now on, and until the current burst finishes, any
@@ -1432,6 +1543,8 @@ static void bfq_handle_burst(struct bfq_data *bfqd, struct bfq_queue *bfqq)
 	    bfqq->entity.parent != bfqd->burst_parent_entity) {
 		bfqd->large_burst = false;
 		bfq_reset_burst_list(bfqd, bfqq);
+		bfq_log_bfqq(bfqd, bfqq,
+			"late activation or different group");
 		goto end;
 	}
 
@@ -1441,6 +1554,7 @@ static void bfq_handle_burst(struct bfq_data *bfqd, struct bfq_queue *bfqq)
 	 * bfqq as belonging to this large burst immediately.
 	 */
 	if (bfqd->large_burst) {
+		bfq_log_bfqq(bfqd, bfqq, "marked in burst");
 		bfq_mark_bfqq_in_large_burst(bfqq);
 		goto end;
 	}
@@ -1467,6 +1581,11 @@ static int bfq_bfqq_budget_left(struct bfq_queue *bfqq)
 {
 	struct bfq_entity *entity = &bfqq->entity;
 
+	if (entity->budget < entity->service) {
+		pr_crit("budget %d service %d\n",
+			entity->budget, entity->service);
+		BUG();
+	}
 	return entity->budget - entity->service;
 }
 
@@ -1631,10 +1750,13 @@ static bool bfq_bfqq_update_budg_for_activation(struct bfq_data *bfqd,
 		 * entity->budget the remaining budget on such an
 		 * expiration.
 		 */
+		BFQ_BUG_ON(bfqq->max_budget < 0);
 		entity->budget = min_t(unsigned long,
 				       bfq_bfqq_budget_left(bfqq),
 				       bfqq->max_budget);
 
+		BFQ_BUG_ON(entity->budget < 0);
+
 		/*
 		 * At this point, we have used entity->service to get
 		 * the budget left (needed for updating
@@ -1653,8 +1775,11 @@ static bool bfq_bfqq_update_budg_for_activation(struct bfq_data *bfqd,
 	 * We can finally complete expiration, by setting service to 0.
 	 */
 	entity->service = 0;
+	BFQ_BUG_ON(bfqq->max_budget < 0);
 	entity->budget = max_t(unsigned long, bfqq->max_budget,
 			       bfq_serv_to_charge(bfqq->next_rq, bfqq));
+	BFQ_BUG_ON(entity->budget < 0);
+
 	bfq_clear_bfqq_non_blocking_wait_rq(bfqq);
 	return false;
 }
@@ -1714,13 +1839,23 @@ static void bfq_update_bfqq_wr_on_rq_arrival(struct bfq_data *bfqd,
 		bfqq->entity.budget = min_t(unsigned long,
 					    bfqq->entity.budget,
 					    2 * bfq_min_budget(bfqd));
+
+		bfq_log_bfqq(bfqd, bfqq,
+			     "wrais starting at %lu, rais_max_time %u",
+			     jiffies,
+			     jiffies_to_msecs(bfqq->wr_cur_max_time));
 	} else if (old_wr_coeff > 1) {
 		if (interactive) { /* update wr coeff and duration */
 			bfqq->wr_coeff = bfqd->bfq_wr_coeff;
 			bfqq->wr_cur_max_time = bfq_wr_duration(bfqd);
-		} else if (in_burst)
+		} else if (in_burst) {
 			bfqq->wr_coeff = 1;
-		else if (soft_rt) {
+			bfq_log_bfqq(bfqd, bfqq,
+				     "wrais ending at %lu, rais_max_time %u",
+				     jiffies,
+				     jiffies_to_msecs(bfqq->
+						      wr_cur_max_time));
+		} else if (soft_rt) {
 			/*
 			 * The application is now or still meeting the
 			 * requirements for being deemed soft rt.  We
@@ -1754,12 +1889,17 @@ static void bfq_update_bfqq_wr_on_rq_arrival(struct bfq_data *bfqd,
 				bfqd->bfq_wr_rt_max_time) {
 				bfqq->wr_start_at_switch_to_srt =
 					bfqq->last_wr_start_finish;
+		BFQ_BUG_ON(time_is_after_jiffies(bfqq->last_wr_start_finish));
 
 				bfqq->wr_cur_max_time =
 					bfqd->bfq_wr_rt_max_time;
 				bfqq->wr_coeff = bfqd->bfq_wr_coeff *
 					BFQ_SOFTRT_WEIGHT_FACTOR;
-			}
+				bfq_log_bfqq(bfqd, bfqq,
+					     "switching to soft_rt wr");
+			} else
+				bfq_log_bfqq(bfqd, bfqq,
+					"moving forward soft_rt wr duration");
 			bfqq->last_wr_start_finish = jiffies;
 		}
 	}
@@ -1824,6 +1964,22 @@ static void bfq_bfqq_handle_idle_busy_switch(struct bfq_data *bfqd,
 			bfqq->ttime.last_end_request +
 			bfqd->bfq_slice_idle * 3;
 
+	bfq_log_bfqq(bfqd, bfqq,
+		     "non-busy: "
+		     "jiffies %lu, in_time %d, idle_long %d busyw %d "
+		     "wr_coeff %u",
+		     jiffies, arrived_in_time,
+		     idle_for_long_time,
+		     bfq_bfqq_non_blocking_wait_rq(bfqq),
+		     old_wr_coeff);
+
+	bfq_log_bfqq(bfqd, bfqq,
+		     "new_weight %d weight %d",
+		     bfqq->entity.new_weight, bfqq->entity.weight);
+
+	BFQ_BUG_ON(bfqq->entity.budget < bfqq->entity.service);
+
+	BFQ_BUG_ON(bfqq == bfqd->in_service_queue);
 
 	/*
 	 * bfqq deserves to be weight-raised if:
@@ -1861,6 +2017,14 @@ static void bfq_bfqq_handle_idle_busy_switch(struct bfq_data *bfqd,
 		  (bfqq->bic || RQ_BIC(rq)->stably_merged) &&
 		   (*interactive || soft_rt)));
 
+	bfq_log_bfqq(bfqd, bfqq,
+		     "in_burst %d, "
+		     "soft_rt %d (next %lu), inter %d, bic %p",
+		     bfq_bfqq_in_large_burst(bfqq), soft_rt,
+		     bfqq->soft_rt_next_start,
+		     *interactive,
+		     bfqq->bic);
+
 	/*
 	 * Using the last flag, update budget and check whether bfqq
 	 * may want to preempt the in-service queue.
@@ -1919,6 +2083,22 @@ static void bfq_bfqq_handle_idle_busy_switch(struct bfq_data *bfqd,
 
 	bfq_add_bfqq_busy(bfqd, bfqq);
 
+	if (bfqd->in_service_queue) {
+		bfq_log_bfqq(bfqd, bfqq,
+			     "wants to preempt %d, higher %d, may preempt %d",
+			     bfqq_wants_to_preempt,
+			     bfq_bfqq_higher_class_or_weight(bfqq,
+							     bfqd->
+							     in_service_queue),
+			     next_queue_may_preempt(bfqd)
+			);
+		bfq_log_bfqq(bfqd, bfqq,
+			     "better_to_idle %d",
+			     bfq_better_to_idle(bfqd->in_service_queue));
+	} else
+		bfq_log_bfqq(bfqd, bfqq,
+			     "no queue in service");
+
 	/*
 	 * Expire in-service queue if preemption may be needed for
 	 * guarantees or throughput. As for guarantees, we care
@@ -1967,9 +2147,14 @@ static void bfq_bfqq_handle_idle_busy_switch(struct bfq_data *bfqd,
 	      bfqq->wr_coeff >= bfqd->in_service_queue->wr_coeff) ||
 	     bfq_bfqq_higher_class_or_weight(bfqq, bfqd->in_service_queue) ||
 	     !bfq_better_to_idle(bfqd->in_service_queue)) &&
-	    next_queue_may_preempt(bfqd))
+	    next_queue_may_preempt(bfqd)) {
+		struct bfq_queue *in_serv =
+			bfqd->in_service_queue;
+		BFQ_BUG_ON(in_serv == bfqq);
+
 		bfq_bfqq_expire(bfqd, bfqd->in_service_queue,
 				false, BFQQE_PREEMPTED);
+	}
 }
 
 static void bfq_reset_inject_limit(struct bfq_data *bfqd,
@@ -2036,6 +2221,8 @@ static void bfq_reset_inject_limit(struct bfq_data *bfqd,
 		bfqq->inject_limit = 1;
 
 	bfqq->decrease_time_jif = jiffies;
+
+	bfq_log_bfqq(bfqd, bfqq, "");
 }
 
 static void bfq_update_io_intensity(struct bfq_queue *bfqq, u64 now_ns)
@@ -2058,6 +2245,10 @@ static void bfq_update_io_intensity(struct bfq_queue *bfqq, u64 now_ns)
 	else
 		bfq_mark_bfqq_IO_bound(bfqq);
 
+	bfq_log_bfqq(bfqq->bfqd, bfqq, "tot %llu idle %llu bound %d",
+		     tot_io_time, bfqq->tot_idle_time,
+		     bfq_bfqq_IO_bound(bfqq));
+
 	/*
 	 * Keep an observation window of at most 200 ms in the past
 	 * from now.
@@ -2122,8 +2313,6 @@ static void bfq_update_io_intensity(struct bfq_queue *bfqq, u64 now_ns)
 static void bfq_check_waker(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 			    u64 now_ns)
 {
-	char waker_name[MAX_BFQQ_NAME_LENGTH];
-
 	if (!bfqd->last_completed_rq_bfqq ||
 	    bfqd->last_completed_rq_bfqq == bfqq ||
 	    bfq_bfqq_has_short_ttime(bfqq) ||
@@ -2136,12 +2325,15 @@ static void bfq_check_waker(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 	 * We reset waker detection logic also if too much time has passed
  	 * since the first detection. If wakeups are rare, pointless idling
 	 * doesn't hurt throughput that much. The condition below makes sure
-	 * we do not uselessly idle blocking waker in more than 1/64 cases. 
+	 * we do not uselessly idle blocking waker in more than 1/64 cases.
 	 */
 	if (bfqd->last_completed_rq_bfqq !=
 	    bfqq->tentative_waker_bfqq ||
 	    now_ns > bfqq->waker_detection_started +
 					128 * (u64)bfqd->bfq_slice_idle) {
+		BFQ_BUG_ON(bfqq->tentative_waker_bfqq &&
+			   bfqq->num_waker_detections == 0);
+
 		/*
 		 * First synchronization detected with a
 		 * candidate waker queue, or with a different
@@ -2151,18 +2343,30 @@ static void bfq_check_waker(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 			bfqd->last_completed_rq_bfqq;
 		bfqq->num_waker_detections = 1;
 		bfqq->waker_detection_started = now_ns;
-		bfq_bfqq_name(bfqq->tentative_waker_bfqq, waker_name,
-			      MAX_BFQQ_NAME_LENGTH);
-		bfq_log_bfqq(bfqd, bfqq, "set tenative waker %s", waker_name);
-	} else /* Same tentative waker queue detected again */
+
+		bfq_log_bfqq(bfqd, bfqq,
+			     "new tentative waker: %d",
+			     bfq_get_first_task_pid(
+				     bfqq->tentative_waker_bfqq));
+
+	} else { /* Same tentative waker queue detected again */
+		BFQ_BUG_ON(bfqq->num_waker_detections == 0);
 		bfqq->num_waker_detections++;
+		bfq_log_bfqq(bfqd, bfqq,
+			     "same tentative waker: %d (%d)",
+			     bfq_get_first_task_pid(
+				     bfqq->tentative_waker_bfqq),
+			     bfqq->num_waker_detections);
+	}
+
+	BFQ_BUG_ON(bfqq->num_waker_detections == 0);
+	BFQ_BUG_ON(bfqq->num_waker_detections > 3);
 
 	if (bfqq->num_waker_detections == 3) {
 		bfqq->waker_bfqq = bfqd->last_completed_rq_bfqq;
 		bfqq->tentative_waker_bfqq = NULL;
-		bfq_bfqq_name(bfqq->waker_bfqq, waker_name,
-			      MAX_BFQQ_NAME_LENGTH);
-		bfq_log_bfqq(bfqd, bfqq, "set waker %s", waker_name);
+		bfq_log_bfqq(bfqd, bfqq, "set waker bfq%d",
+			     bfq_get_first_task_pid(bfqq->waker_bfqq));
 
 		/*
 		 * If the waker queue disappears, then
@@ -2200,11 +2404,42 @@ static void bfq_add_request(struct request *rq)
 	bool interactive = false;
 	u64 now_ns = ktime_get_ns();
 
-	bfq_log_bfqq(bfqd, bfqq, "add_request %d", rq_is_sync(rq));
+	bfq_log_bfqq(bfqd, bfqq, "%p size %u %s",
+		     rq, blk_rq_sectors(rq), rq_is_sync(rq) ? "S" : "A");
+
+	if (bfqq->wr_coeff > 1) /* queue is being weight-raised */
+		bfq_log_bfqq(bfqd, bfqq,
+			"raising period dur %u/%u msec, old coeff %u, w %d(%d)",
+			jiffies_to_msecs(jiffies - bfqq->last_wr_start_finish),
+			jiffies_to_msecs(bfqq->wr_cur_max_time),
+			bfqq->wr_coeff,
+			bfqq->entity.weight, bfqq->entity.orig_weight);
+
 	bfqq->queued[rq_is_sync(rq)]++;
 	bfqd->queued++;
+	bfq_log_bfqq(bfqd, bfqq, "new in-bfqq[%d] %d, in-bfqd %d",
+		     rq_is_sync(rq), bfqq->queued[rq_is_sync(rq)],
+		     bfqd->queued);
+
+	BFQ_BUG_ON(!RQ_BFQQ(rq));
+	BFQ_BUG_ON(RQ_BFQQ(rq) != bfqq);
+	WARN_ON(blk_rq_sectors(rq) == 0);
 
 	if (RB_EMPTY_ROOT(&bfqq->sort_list) && bfq_bfqq_sync(bfqq)) {
+		bfq_log_bfqq(bfqd, bfqq,
+		"in_serv %d reset %d recalc %d",
+		bfqq == bfqd->in_service_queue,
+		time_is_before_eq_jiffies(
+			bfqq->decrease_time_jif +
+			msecs_to_jiffies(1000)),
+		time_is_before_eq_jiffies(
+			bfqq->decrease_time_jif +
+			msecs_to_jiffies(100)));
+
+		bfq_log_bfqq(bfqd, bfqq,
+		"limit %u rq_in_driver %d rqs_injected %d",
+		bfqq->inject_limit, bfqd->rq_in_driver, bfqd->rqs_injected);
+
 		bfq_check_waker(bfqd, bfqq, now_ns);
 
 		/*
@@ -2271,6 +2506,7 @@ static void bfq_add_request(struct request *rq)
 			 */
 			if (bfqd->rq_in_driver == 0)
 				bfqd->rqs_injected = false;
+			bfq_log_bfqq(bfqd, bfqq, "start limit update");
 		}
 	}
 
@@ -2284,6 +2520,9 @@ static void bfq_add_request(struct request *rq)
 	 */
 	prev = bfqq->next_rq;
 	next_rq = bfq_choose_req(bfqd, bfqq->next_rq, rq, bfqd->last_position);
+	BFQ_BUG_ON(!next_rq);
+	BFQ_BUG_ON(!RQ_BFQQ(next_rq));
+	BFQ_BUG_ON(RQ_BFQQ(next_rq) != bfqq);
 	bfqq->next_rq = next_rq;
 
 	/*
@@ -2305,7 +2544,13 @@ static void bfq_add_request(struct request *rq)
 			bfqq->wr_cur_max_time = bfq_wr_duration(bfqd);
 
 			bfqd->wr_busy_queues++;
+			BFQ_BUG_ON(bfqd->wr_busy_queues > bfq_tot_busy_queues(bfqd));
 			bfqq->entity.prio_changed = 1;
+			bfq_log_bfqq(bfqd, bfqq,
+				     "non-idle wrais starting, "
+				     "wr_max_time %u wr_busy %d",
+				     jiffies_to_msecs(bfqq->wr_cur_max_time),
+				     bfqd->wr_busy_queues);
 		}
 		if (prev != bfqq->next_rq)
 			bfq_updated_next_req(bfqd, bfqq);
@@ -2348,6 +2593,7 @@ static struct request *bfq_find_rq_fmerge(struct bfq_data *bfqd,
 {
 	struct bfq_queue *bfqq = bfqd->bio_bfqq;
 
+	BFQ_BUG_ON(!bfqd->bio_bfqq_set);
 
 	if (bfqq)
 		return elv_rb_find(&bfqq->sort_list, bio_end_sector(bio));
@@ -2375,6 +2621,7 @@ static void bfq_deactivate_request(struct request_queue *q, struct request *rq)
 {
 	struct bfq_data *bfqd = q->elevator->elevator_data;
 
+	BFQ_BUG_ON(bfqd->rq_in_driver == 0);
 	bfqd->rq_in_driver--;
 }
 #endif
@@ -2386,15 +2633,35 @@ static void bfq_remove_request(struct request_queue *q,
 	struct bfq_data *bfqd = bfqq->bfqd;
 	const int sync = rq_is_sync(rq);
 
+	BFQ_BUG_ON(bfqq->entity.service > bfqq->entity.budget);
+
 	if (bfqq->next_rq == rq) {
 		bfqq->next_rq = bfq_find_next_rq(bfqd, bfqq, rq);
+		if (bfqq->next_rq && !RQ_BFQQ(bfqq->next_rq)) {
+			pr_crit("no bfqq! for next rq %p bfqq %p\n",
+				bfqq->next_rq, bfqq);
+		}
+
+		BFQ_BUG_ON(bfqq->next_rq && !RQ_BFQQ(bfqq->next_rq));
+		if (bfqq->next_rq && RQ_BFQQ(bfqq->next_rq) != bfqq) {
+			pr_crit(
+			"wrong bfqq! for next rq %p, rq_bfqq %p bfqq %p\n",
+			bfqq->next_rq, RQ_BFQQ(bfqq->next_rq), bfqq);
+		}
+		BFQ_BUG_ON(bfqq->next_rq && RQ_BFQQ(bfqq->next_rq) != bfqq);
+
 		bfq_updated_next_req(bfqd, bfqq);
 	}
 
 	if (rq->queuelist.prev != &rq->queuelist)
 		list_del_init(&rq->queuelist);
+	BFQ_BUG_ON(bfqq->queued[sync] == 0);
+	BFQ_BUG_ON(bfqd->queued == 0);
+
 	bfqq->queued[sync]--;
 	bfqd->queued--;
+	bfq_log_bfqq(bfqd, bfqq, "%p in-bfqq[%d] %d in-bfqd %d",
+		     rq, sync, bfqq->queued[sync], bfqd->queued);
 	elv_rb_del(&bfqq->sort_list, rq);
 
 	elv_rqhash_del(q, rq);
@@ -2404,7 +2671,11 @@ static void bfq_remove_request(struct request_queue *q,
 	if (RB_EMPTY_ROOT(&bfqq->sort_list)) {
 		bfqq->next_rq = NULL;
 
+		BFQ_BUG_ON(bfqq->entity.budget < 0);
+
 		if (bfq_bfqq_busy(bfqq) && bfqq != bfqd->in_service_queue) {
+			BFQ_BUG_ON(bfqq->ref < 2); /* referred by rq
+						    * and on tree */
 			bfq_del_bfqq_busy(bfqd, bfqq, false);
 			/*
 			 * bfqq emptied. In normal operation, when
@@ -2430,14 +2701,16 @@ static void bfq_remove_request(struct request_queue *q,
 			bfqq->pos_root = NULL;
 		}
 	} else {
+		BFQ_BUG_ON(!bfqq->next_rq);
 		/* see comments on bfq_pos_tree_add_move() for the unlikely() */
 		if (unlikely(!bfqd->nonrot_with_queueing))
 			bfq_pos_tree_add_move(bfqd, bfqq);
 	}
 
-	if (rq->cmd_flags & REQ_META)
+	if (rq->cmd_flags & REQ_META) {
+		BFQ_BUG_ON(bfqq->meta_pending == 0);
 		bfqq->meta_pending--;
-
+	}
 }
 
 static bool bfq_bio_merge(struct request_queue *q, struct bio *bio,
@@ -2462,12 +2735,21 @@ static bool bfq_bio_merge(struct request_queue *q, struct bio *bio,
 	else
 		bfqd->bio_bfqq = NULL;
 	bfqd->bio_bic = bic;
+	/* Set next flag just for testing purposes */
+	bfqd->bio_bfqq_set = true;
 
 	ret = blk_mq_sched_try_merge(q, bio, nr_segs, &free);
 
-	spin_unlock_irq(&bfqd->lock);
+	/*
+	 * XXX Not yet freeing without lock held, to avoid an
+	 * inconsistency with respect to the lock-protected invocation
+	 * of blk_mq_sched_try_insert_merge in bfq_bio_merge. Waiting
+	 * for clarifications from Jens.
+	 */
 	if (free)
 		blk_mq_free_request(free);
+	bfqd->bio_bfqq_set = false;
+	spin_unlock_irq(&bfqd->lock);
 
 	return ret;
 }
@@ -2484,6 +2766,9 @@ static int bfq_request_merge(struct request_queue *q, struct request **req,
 
 		if (blk_discard_mergable(__rq))
 			return ELEVATOR_DISCARD_MERGE;
+
+		bfq_log(bfqd, "req %p", __rq);
+
 		return ELEVATOR_FRONT_MERGE;
 	}
 
@@ -2495,6 +2780,8 @@ static struct bfq_queue *bfq_init_rq(struct request *rq);
 static void bfq_request_merged(struct request_queue *q, struct request *req,
 			       enum elv_merge type)
 {
+	BFQ_BUG_ON(req->rq_flags & RQF_DISP_LIST);
+
 	if (type == ELEVATOR_FRONT_MERGE &&
 	    rb_prev(&req->rb_node) &&
 	    blk_rq_pos(req) <
@@ -2511,13 +2798,22 @@ static void bfq_request_merged(struct request_queue *q, struct request *req,
 
 		/* Reposition request in its sort_list */
 		elv_rb_del(&bfqq->sort_list, req);
+		BFQ_BUG_ON(!RQ_BFQQ(req));
+		BFQ_BUG_ON(RQ_BFQQ(req) != bfqq);
 		elv_rb_add(&bfqq->sort_list, req);
 
 		/* Choose next request to be served for bfqq */
 		prev = bfqq->next_rq;
 		next_rq = bfq_choose_req(bfqd, bfqq->next_rq, req,
 					 bfqd->last_position);
+		BFQ_BUG_ON(!next_rq);
+
 		bfqq->next_rq = next_rq;
+
+		bfq_log_bfqq(bfqd, bfqq,
+			"req %p prev %p next_rq %p bfqq %p",
+			     req, prev, next_rq, bfqq);
+
 		/*
 		 * If next_rq changes, update both the queue's budget to
 		 * fit the new request and the queue's position in its
@@ -2558,6 +2854,17 @@ static void bfq_requests_merged(struct request_queue *q, struct request *rq,
 	if (!bfqq)
 		goto remove;
 
+	BFQ_BUG_ON(!RQ_BFQQ(rq));
+	BFQ_BUG_ON(!RQ_BFQQ(next)); /* this does not imply next is in a bfqq */
+	BFQ_BUG_ON(rq->rq_flags & RQF_DISP_LIST);
+	BFQ_BUG_ON(next->rq_flags & RQF_DISP_LIST);
+
+	lockdep_assert_held(&bfqq->bfqd->lock);
+
+	bfq_log_bfqq(bfqq->bfqd, bfqq,
+		     "rq %p next %p bfqq %p next_bfqq %p",
+		     rq, next, bfqq, next_bfqq);
+
 	/*
 	 * If next and rq belong to the same bfq_queue and next is older
 	 * than rq, then reposition rq in the fifo (by substituting next
@@ -2592,6 +2899,8 @@ static void bfq_requests_merged(struct request_queue *q, struct request *rq,
 /* Must be called with bfqq != NULL */
 static void bfq_bfqq_end_wr(struct bfq_queue *bfqq)
 {
+	BFQ_BUG_ON(!bfqq);
+
 	/*
 	 * If bfqq has been enjoying interactive weight-raising, then
 	 * reset soft_rt_next_start. We do it for the following
@@ -2610,8 +2919,10 @@ static void bfq_bfqq_end_wr(struct bfq_queue *bfqq)
 	    bfqq->bfqd->bfq_wr_rt_max_time)
 		bfqq->soft_rt_next_start = jiffies;
 
-	if (bfq_bfqq_busy(bfqq))
+	if (bfq_bfqq_busy(bfqq)) {
 		bfqq->bfqd->wr_busy_queues--;
+		BFQ_BUG_ON(bfqq->bfqd->wr_busy_queues < 0);
+	}
 	bfqq->wr_coeff = 1;
 	bfqq->wr_cur_max_time = 0;
 	bfqq->last_wr_start_finish = jiffies;
@@ -2620,6 +2931,12 @@ static void bfq_bfqq_end_wr(struct bfq_queue *bfqq)
 	 * __bfq_entity_update_weight_prio.
 	 */
 	bfqq->entity.prio_changed = 1;
+	bfq_log_bfqq(bfqq->bfqd, bfqq,
+		     "wrais ending at %lu, rais_max_time %u",
+		     bfqq->last_wr_start_finish,
+		     jiffies_to_msecs(bfqq->wr_cur_max_time));
+	bfq_log_bfqq(bfqq->bfqd, bfqq, "wr_busy %d",
+		     bfqq->bfqd->wr_busy_queues);
 }
 
 void bfq_end_wr_async_queues(struct bfq_data *bfqd,
@@ -2690,6 +3007,7 @@ static struct bfq_queue *bfqq_find_close(struct bfq_data *bfqd,
 	 * next_request position).
 	 */
 	__bfqq = rb_entry(parent, struct bfq_queue, pos_node);
+	BFQ_BUG_ON(!__bfqq->next_rq);
 	if (bfq_rq_close_to_sector(__bfqq->next_rq, true, sector))
 		return __bfqq;
 
@@ -2701,6 +3019,7 @@ static struct bfq_queue *bfqq_find_close(struct bfq_data *bfqd,
 		return NULL;
 
 	__bfqq = rb_entry(node, struct bfq_queue, pos_node);
+	BFQ_BUG_ON(!__bfqq->next_rq);
 	if (bfq_rq_close_to_sector(__bfqq->next_rq, true, sector))
 		return __bfqq;
 
@@ -2733,6 +3052,8 @@ bfq_setup_merge(struct bfq_queue *bfqq, struct bfq_queue *new_bfqq)
 	int process_refs, new_process_refs;
 	struct bfq_queue *__bfqq;
 
+	BFQ_BUG_ON(new_bfqq == bfqq);
+
 	/*
 	 * If there are no process references on the new_bfqq, then it is
 	 * unsafe to follow the ->new_bfqq chain as other bfqq's in the chain
@@ -2759,7 +3080,7 @@ bfq_setup_merge(struct bfq_queue *bfqq, struct bfq_queue *new_bfqq)
 		return NULL;
 
 	bfq_log_bfqq(bfqq->bfqd, bfqq, "scheduling merge with queue %d",
-		new_bfqq->pid);
+		bfq_get_first_task_pid(new_bfqq));
 
 	/*
 	 * Merging is just a redirection: the requests of the process
@@ -2798,8 +3119,12 @@ bfq_setup_merge(struct bfq_queue *bfqq, struct bfq_queue *new_bfqq)
 static bool bfq_may_be_close_cooperator(struct bfq_queue *bfqq,
 					struct bfq_queue *new_bfqq)
 {
-	if (bfq_too_late_for_merging(new_bfqq))
+	if (bfq_too_late_for_merging(new_bfqq)) {
+		bfq_log_bfqq(bfqq->bfqd, bfqq,
+			     "too late for bfq%d to be merged",
+				bfq_get_first_task_pid(new_bfqq));
 		return false;
+	}
 
 	if (bfq_class_idle(bfqq) || bfq_class_idle(new_bfqq) ||
 	    (bfqq->ioprio_class != new_bfqq->ioprio_class))
@@ -2853,6 +3178,8 @@ bfq_setup_cooperator(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 {
 	struct bfq_queue *in_service_bfqq, *new_bfqq;
 
+	BFQ_BUG_ON(!bic);
+
 	/* if a merge has already been setup, then proceed with that first */
 	if (bfqq->new_bfqq)
 		return bfqq->new_bfqq;
@@ -2884,9 +3211,16 @@ bfq_setup_cooperator(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 			int proc_ref = min(bfqq_process_refs(bfqq),
 					   bfqq_process_refs(stable_merge_bfqq));
 
+			bfq_log_bfqq(bfqd, bfqq,
+				     "checking late stable merging");
+
+			BFQ_BUG_ON(stable_merge_bfqq->ref < 1 + proc_ref);
 			/* deschedule stable merge, because done or aborted here */
 			bfq_put_stable_ref(stable_merge_bfqq);
 
+			BFQ_BUG_ON(proc_ref > 0 &&
+				   bfqq_process_refs(stable_merge_bfqq) < proc_ref);
+
 			bic->stable_merge_bfqq = NULL;
 
 			if (!idling_boosts_thr_without_issues(bfqd, bfqq) &&
@@ -2895,12 +3229,27 @@ bfq_setup_cooperator(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 				struct bfq_queue *new_bfqq =
 					bfq_setup_merge(bfqq, stable_merge_bfqq);
 
+				BFQ_BUG_ON(bic_to_bfqq(bic, 1) != bfqq);
+
+				if (!new_bfqq)
+					bfq_log_bfqq(bfqd, bfqq,
+					     "merging already occurred");
+				else {
+					BFQ_BUG_ON(stable_merge_bfqq == bfqq);
+					bfq_log_bfqq(bfqd, bfqq,
+						     "late stable merging with %p",
+						     new_bfqq);
+				}
+
 				bic->stably_merged = true;
 				if (new_bfqq && new_bfqq->bic)
 					new_bfqq->bic->stably_merged = true;
 				return new_bfqq;
-			} else
+			} else {
+				bfq_log_bfqq(bfqd, bfqq,
+					     "no late stable merging");
 				return NULL;
+			}
 		}
 	}
 
@@ -2955,8 +3304,11 @@ bfq_setup_cooperator(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 	 * happen to do close I/O for some short time interval, have
 	 * their queues merged by mistake.
 	 */
-	if (bfq_too_late_for_merging(bfqq))
+	if (bfq_too_late_for_merging(bfqq)) {
+		bfq_log_bfqq(bfqd, bfqq,
+			     "would have looked for coop, but too late");
 		return NULL;
+	}
 
 	if (!io_struct || unlikely(bfqq == &bfqd->oom_bfqq))
 		return NULL;
@@ -2985,6 +3337,8 @@ bfq_setup_cooperator(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 	new_bfqq = bfq_find_close_cooperator(bfqd, bfqq,
 			bfq_io_struct_pos(io_struct, request));
 
+	BFQ_BUG_ON(new_bfqq && bfqq->entity.parent != new_bfqq->entity.parent);
+
 	if (new_bfqq && likely(new_bfqq != &bfqd->oom_bfqq) &&
 	    bfq_may_be_close_cooperator(bfqq, new_bfqq))
 		return bfq_setup_merge(bfqq, new_bfqq);
@@ -3009,6 +3363,7 @@ static void bfq_bfqq_save_state(struct bfq_queue *bfqq)
 	bic->saved_decrease_time_jif = bfqq->decrease_time_jif;
 
 	bic->saved_weight = bfqq->entity.orig_weight;
+	BFQ_BUG_ON(bic->saved_weight == 0);
 	bic->saved_ttime = bfqq->ttime;
 	bic->saved_has_short_ttime = bfq_bfqq_has_short_ttime(bfqq);
 	bic->saved_IO_bound = bfq_bfqq_IO_bound(bfqq);
@@ -3040,6 +3395,12 @@ static void bfq_bfqq_save_state(struct bfq_queue *bfqq)
 		bic->saved_last_wr_start_finish = bfqq->last_wr_start_finish;
 		bic->saved_wr_cur_max_time = bfqq->wr_cur_max_time;
 	}
+
+	BFQ_BUG_ON(time_is_after_jiffies(bfqq->last_wr_start_finish));
+	bfq_log_bfqq(bfqq->bfqd, bfqq,
+		     "bic %p wr_coeff %d start_finish %lu max_time %lu",
+		     bic, bfqq->wr_coeff, bfqq->last_wr_start_finish,
+		     bfqq->wr_cur_max_time);
 }
 
 
@@ -3047,10 +3408,20 @@ static void
 bfq_reassign_last_bfqq(struct bfq_queue *cur_bfqq, struct bfq_queue *new_bfqq)
 {
 	if (cur_bfqq->entity.parent &&
-	    cur_bfqq->entity.parent->last_bfqq_created == cur_bfqq)
+	    cur_bfqq->entity.parent->last_bfqq_created == cur_bfqq) {
 		cur_bfqq->entity.parent->last_bfqq_created = new_bfqq;
-	else if (cur_bfqq->bfqd && cur_bfqq->bfqd->last_bfqq_created == cur_bfqq)
+		if (new_bfqq)
+			bfq_log_bfqq(cur_bfqq->bfqd, new_bfqq,
+				     "new last bfqq created %lu",
+				     new_bfqq->creation_time);
+	} else if (cur_bfqq->bfqd && cur_bfqq->bfqd->last_bfqq_created ==
+		   cur_bfqq) {
 		cur_bfqq->bfqd->last_bfqq_created = new_bfqq;
+		if (new_bfqq)
+			bfq_log_bfqq(cur_bfqq->bfqd, new_bfqq,
+				     "new last bfqq created %lu",
+				     new_bfqq->creation_time);
+	}
 }
 
 void bfq_release_process_ref(struct bfq_data *bfqd, struct bfq_queue *bfqq)
@@ -3072,6 +3443,7 @@ void bfq_release_process_ref(struct bfq_data *bfqd, struct bfq_queue *bfqq)
 
 	bfq_reassign_last_bfqq(bfqq, NULL);
 
+	bfqq->proc_ref--;
 	bfq_put_queue(bfqq);
 }
 
@@ -3079,8 +3451,15 @@ static void
 bfq_merge_bfqqs(struct bfq_data *bfqd, struct bfq_io_cq *bic,
 		struct bfq_queue *bfqq, struct bfq_queue *new_bfqq)
 {
+	struct task_struct *item;
+	struct hlist_node *n;
+
 	bfq_log_bfqq(bfqd, bfqq, "merging with queue %lu",
-		(unsigned long)new_bfqq->pid);
+		(unsigned long)bfq_get_first_task_pid(new_bfqq));
+
+	BFQ_BUG_ON(new_bfqq == &bfqd->oom_bfqq);
+
+	BFQ_BUG_ON(bfqq->bic && bfqq->bic == new_bfqq->bic);
 	/* Save weight raising and idle window of the merged queues */
 	bfq_bfqq_save_state(bfqq);
 	bfq_bfqq_save_state(new_bfqq);
@@ -3100,6 +3479,11 @@ bfq_merge_bfqqs(struct bfq_data *bfqd, struct bfq_io_cq *bic,
 		new_bfqq->waker_bfqq = bfqq->waker_bfqq;
 		new_bfqq->tentative_waker_bfqq = NULL;
 
+		bfq_log_bfqq(bfqd, new_bfqq, "inherits waker %d from bfq%d",
+			     bfq_get_first_task_pid(
+				     bfqq->waker_bfqq),
+			     bfq_get_first_task_pid(bfqq));
+
 		/*
 		 * If the waker queue disappears, then
 		 * new_bfqq->waker_bfqq must be reset. So insert
@@ -3126,19 +3510,30 @@ bfq_merge_bfqqs(struct bfq_data *bfqd, struct bfq_io_cq *bic,
 		new_bfqq->last_wr_start_finish = bfqq->last_wr_start_finish;
 		new_bfqq->wr_start_at_switch_to_srt =
 			bfqq->wr_start_at_switch_to_srt;
-		if (bfq_bfqq_busy(new_bfqq))
+		if (bfq_bfqq_busy(new_bfqq)) {
 			bfqd->wr_busy_queues++;
+			BFQ_BUG_ON(bfqd->wr_busy_queues >
+			       bfq_tot_busy_queues(bfqd));
+		}
+
 		new_bfqq->entity.prio_changed = 1;
+		bfq_log_bfqq(bfqd, new_bfqq,
+			     "wr start after merge with %d, rais_max_time %u",
+			     bfq_get_first_task_pid(bfqq),
+			     jiffies_to_msecs(bfqq->wr_cur_max_time));
 	}
 
 	if (bfqq->wr_coeff > 1) { /* bfqq has given its wr to new_bfqq */
 		bfqq->wr_coeff = 1;
 		bfqq->entity.prio_changed = 1;
-		if (bfq_bfqq_busy(bfqq))
+		if (bfq_bfqq_busy(bfqq)) {
 			bfqd->wr_busy_queues--;
+			BFQ_BUG_ON(bfqd->wr_busy_queues < 0);
+		}
+
 	}
 
-	bfq_log_bfqq(bfqd, new_bfqq, "merge_bfqqs: wr_busy %d",
+	bfq_log_bfqq(bfqd, new_bfqq, "wr_busy %d",
 		     bfqd->wr_busy_queues);
 
 	/*
@@ -3166,11 +3561,18 @@ bfq_merge_bfqqs(struct bfq_data *bfqd, struct bfq_io_cq *bic,
 	 * We mark such a queue with a pid -1, and then print SHARED instead of
 	 * a pid in logging messages.
 	 */
-	new_bfqq->pid = -1;
 	bfqq->bic = NULL;
 
 	bfq_reassign_last_bfqq(bfqq, new_bfqq);
 
+	/*
+	 * move task_list_node from its current list to that of new_bfqq
+	 */
+	hlist_for_each_entry_safe(item, n, &bfqq->task_list, task_list_node) {
+		hlist_del_init(&item->task_list_node);
+		hlist_add_head(&item->task_list_node, &new_bfqq->task_list);
+	}
+
 	bfq_release_process_ref(bfqd, bfqq);
 }
 
@@ -3181,6 +3583,7 @@ static bool bfq_allow_bio_merge(struct request_queue *q, struct request *rq,
 	bool is_sync = op_is_sync(bio->bi_opf);
 	struct bfq_queue *bfqq = bfqd->bio_bfqq, *new_bfqq;
 
+	assert_spin_locked(&bfqd->lock);
 	/*
 	 * Disallow merge of a sync bio into an async request.
 	 */
@@ -3191,6 +3594,7 @@ static bool bfq_allow_bio_merge(struct request_queue *q, struct request *rq,
 	 * Lookup the bfqq that this bio will be queued with. Allow
 	 * merge only if rq is queued there.
 	 */
+	BFQ_BUG_ON(!bfqd->bio_bfqq_set);
 	if (!bfqq)
 		return false;
 
@@ -3199,6 +3603,9 @@ static bool bfq_allow_bio_merge(struct request_queue *q, struct request *rq,
 	 * of the queues of possible cooperating processes.
 	 */
 	new_bfqq = bfq_setup_cooperator(bfqd, bfqq, bio, false, bfqd->bio_bic);
+	BFQ_BUG_ON(new_bfqq == bfqq);
+	BFQ_BUG_ON(new_bfqq == &bfqd->oom_bfqq);
+
 	if (new_bfqq) {
 		/*
 		 * bic still points to bfqq, then it has not yet been
@@ -3248,6 +3655,9 @@ static void bfq_set_budget_timeout(struct bfq_data *bfqd,
 
 	bfqq->budget_timeout = jiffies +
 		bfqd->bfq_timeout * timeout_coeff;
+
+	bfq_log_bfqq(bfqd, bfqq, "%u",
+		jiffies_to_msecs(bfqd->bfq_timeout * timeout_coeff));
 }
 
 static void __bfq_set_in_service_queue(struct bfq_data *bfqd,
@@ -3258,6 +3668,8 @@ static void __bfq_set_in_service_queue(struct bfq_data *bfqd,
 
 		bfqd->budgets_assigned = (bfqd->budgets_assigned * 7 + 256) / 8;
 
+		BFQ_BUG_ON(bfqq == bfqd->in_service_queue);
+
 		if (time_is_before_jiffies(bfqq->last_wr_start_finish) &&
 		    bfqq->wr_coeff > 1 &&
 		    bfqq->wr_cur_max_time == bfqd->bfq_wr_rt_max_time &&
@@ -3292,13 +3704,27 @@ static void __bfq_set_in_service_queue(struct bfq_data *bfqd,
 					jiffies - bfqq->budget_timeout;
 			else
 				bfqq->last_wr_start_finish = jiffies;
+
+			if (time_is_after_jiffies(bfqq->last_wr_start_finish)) {
+			       pr_crit(
+			       "BFQ WARNING:last %lu budget %lu jiffies %lu",
+			       bfqq->last_wr_start_finish,
+			       bfqq->budget_timeout,
+			       jiffies);
+			       pr_crit("diff %lu", jiffies -
+				       max_t(unsigned long,
+					     bfqq->last_wr_start_finish,
+					     bfqq->budget_timeout));
+			       bfqq->last_wr_start_finish = jiffies;
+			}
 		}
 
 		bfq_set_budget_timeout(bfqd, bfqq);
 		bfq_log_bfqq(bfqd, bfqq,
-			     "set_in_service_queue, cur-budget = %d",
-			     bfqq->entity.budget);
-	}
+			     "cur-budget = %d prio_class %d",
+			     bfqq->entity.budget, bfqq->ioprio_class);
+	} else
+		bfq_log(bfqd, "NULL");
 
 	bfqd->in_service_queue = bfqq;
 	bfqd->in_serv_last_pos = 0;
@@ -3312,6 +3738,7 @@ static struct bfq_queue *bfq_set_in_service_queue(struct bfq_data *bfqd)
 	struct bfq_queue *bfqq = bfq_get_next_queue(bfqd);
 
 	__bfq_set_in_service_queue(bfqd, bfqq);
+	BFQ_BUG_ON(bfqq && !bfqq->entity.on_st_or_in_serv);
 	return bfqq;
 }
 
@@ -3320,6 +3747,8 @@ static void bfq_arm_slice_timer(struct bfq_data *bfqd)
 	struct bfq_queue *bfqq = bfqd->in_service_queue;
 	u32 sl;
 
+	BFQ_BUG_ON(!RB_EMPTY_ROOT(&bfqq->sort_list));
+
 	bfq_mark_bfqq_wait_request(bfqq);
 
 	/*
@@ -3350,6 +3779,8 @@ static void bfq_arm_slice_timer(struct bfq_data *bfqd)
 	hrtimer_start(&bfqd->idle_slice_timer, ns_to_ktime(sl),
 		      HRTIMER_MODE_REL);
 	bfqg_stats_set_start_idle_time(bfqq_group(bfqq));
+	bfq_log_bfqq(bfqd, bfqq, "arm idle: %ld/%ld ms",
+		sl / NSEC_PER_MSEC, bfqd->bfq_slice_idle / NSEC_PER_MSEC);
 }
 
 /*
@@ -3375,7 +3806,9 @@ static void update_thr_responsiveness_params(struct bfq_data *bfqd)
 	if (bfqd->bfq_user_max_budget == 0) {
 		bfqd->bfq_max_budget =
 			bfq_calc_max_budget(bfqd);
-		bfq_log(bfqd, "new max_budget = %d", bfqd->bfq_max_budget);
+		BFQ_BUG_ON(bfqd->bfq_max_budget < 0);
+		bfq_log(bfqd, "new max_budget = %d",
+			bfqd->bfq_max_budget);
 	}
 }
 
@@ -3392,7 +3825,7 @@ static void bfq_reset_rate_computation(struct bfq_data *bfqd,
 		bfqd->peak_rate_samples = 0; /* full re-init on next disp. */
 
 	bfq_log(bfqd,
-		"reset_rate_computation at end, sample %u/%u tot_sects %llu",
+		"at end, sample %u/%u tot_sects %llu",
 		bfqd->peak_rate_samples, bfqd->sequential_samples,
 		bfqd->tot_sectors_dispatched);
 }
@@ -3410,8 +3843,12 @@ static void bfq_update_rate_reset(struct bfq_data *bfqd, struct request *rq)
 	 * for a new evaluation attempt.
 	 */
 	if (bfqd->peak_rate_samples < BFQ_RATE_MIN_SAMPLES ||
-	    bfqd->delta_from_first < BFQ_RATE_MIN_INTERVAL)
+	    bfqd->delta_from_first < BFQ_RATE_MIN_INTERVAL) {
+		bfq_log(bfqd,
+	"only resetting, delta_first %lluus samples %d",
+			bfqd->delta_from_first>>10, bfqd->peak_rate_samples);
 		goto reset_computation;
+	}
 
 	/*
 	 * If a new request completion has occurred after last
@@ -3423,6 +3860,7 @@ static void bfq_update_rate_reset(struct bfq_data *bfqd, struct request *rq)
 		max_t(u64, bfqd->delta_from_first,
 		      bfqd->last_completion - bfqd->first_dispatch);
 
+	BFQ_BUG_ON(bfqd->delta_from_first == 0);
 	/*
 	 * Rate computed in sects/usec, and not sects/nsec, for
 	 * precision issues.
@@ -3430,6 +3868,12 @@ static void bfq_update_rate_reset(struct bfq_data *bfqd, struct request *rq)
 	rate = div64_ul(bfqd->tot_sectors_dispatched<<BFQ_RATE_SHIFT,
 			div_u64(bfqd->delta_from_first, NSEC_PER_USEC));
 
+	bfq_log(bfqd,
+"tot_sects %llu delta_first %lluus rate %llu sects/s (%d)",
+		bfqd->tot_sectors_dispatched, bfqd->delta_from_first>>10,
+		((USEC_PER_SEC*(u64)rate)>>BFQ_RATE_SHIFT),
+		rate > 20<<BFQ_RATE_SHIFT);
+
 	/*
 	 * Peak rate not updated if:
 	 * - the percentage of sequential dispatches is below 3/4 of the
@@ -3438,8 +3882,20 @@ static void bfq_update_rate_reset(struct bfq_data *bfqd, struct request *rq)
 	 */
 	if ((bfqd->sequential_samples < (3 * bfqd->peak_rate_samples)>>2 &&
 	     rate <= bfqd->peak_rate) ||
-		rate > 20<<BFQ_RATE_SHIFT)
+		rate > 20<<BFQ_RATE_SHIFT) {
+		bfq_log(bfqd,
+		"goto reset, samples %u/%u rate/peak %llu/%llu",
+		bfqd->peak_rate_samples, bfqd->sequential_samples,
+		((USEC_PER_SEC*(u64)rate)>>BFQ_RATE_SHIFT),
+		((USEC_PER_SEC*(u64)bfqd->peak_rate)>>BFQ_RATE_SHIFT));
 		goto reset_computation;
+	} else {
+		bfq_log(bfqd,
+		"do update, samples %u/%u rate/peak %llu/%llu",
+		bfqd->peak_rate_samples, bfqd->sequential_samples,
+		((USEC_PER_SEC*(u64)rate)>>BFQ_RATE_SHIFT),
+		((USEC_PER_SEC*(u64)bfqd->peak_rate)>>BFQ_RATE_SHIFT));
+	}
 
 	/*
 	 * We have to update the peak rate, at last! To this purpose,
@@ -3479,6 +3935,7 @@ static void bfq_update_rate_reset(struct bfq_data *bfqd, struct request *rq)
 	 * maximum weight.
 	 */
 	divisor = 10 - weight;
+	BFQ_BUG_ON(divisor == 0);
 
 	/*
 	 * Finally, update peak rate:
@@ -3489,6 +3946,15 @@ static void bfq_update_rate_reset(struct bfq_data *bfqd, struct request *rq)
 	bfqd->peak_rate /= divisor;
 	rate /= divisor; /* smoothing constant alpha = 1/divisor */
 
+	bfq_log(bfqd,
+		"divisor %d tmp_peak_rate %llu tmp_rate %u",
+		divisor,
+		((USEC_PER_SEC*(u64)bfqd->peak_rate)>>BFQ_RATE_SHIFT),
+		(u32)((USEC_PER_SEC*(u64)rate)>>BFQ_RATE_SHIFT));
+
+	BFQ_BUG_ON(bfqd->peak_rate == 0);
+	BFQ_BUG_ON(bfqd->peak_rate > 20<<BFQ_RATE_SHIFT);
+
 	bfqd->peak_rate += rate;
 
 	/*
@@ -3501,6 +3967,7 @@ static void bfq_update_rate_reset(struct bfq_data *bfqd, struct request *rq)
 	bfqd->peak_rate = max_t(u32, 1, bfqd->peak_rate);
 
 	update_thr_responsiveness_params(bfqd);
+	BFQ_BUG_ON(bfqd->peak_rate > 20<<BFQ_RATE_SHIFT);
 
 reset_computation:
 	bfq_reset_rate_computation(bfqd, rq);
@@ -3543,8 +4010,9 @@ static void bfq_update_peak_rate(struct bfq_data *bfqd, struct request *rq)
 	u64 now_ns = ktime_get_ns();
 
 	if (bfqd->peak_rate_samples == 0) { /* first dispatch */
-		bfq_log(bfqd, "update_peak_rate: goto reset, samples %d",
-			bfqd->peak_rate_samples);
+		bfq_log(bfqd,
+		"goto reset, samples %d",
+				bfqd->peak_rate_samples) ;
 		bfq_reset_rate_computation(bfqd, rq);
 		goto update_last_values; /* will add one sample */
 	}
@@ -3562,8 +4030,13 @@ static void bfq_update_peak_rate(struct bfq_data *bfqd, struct request *rq)
 	 * - start a new observation interval with this dispatch
 	 */
 	if (now_ns - bfqd->last_dispatch > 100*NSEC_PER_MSEC &&
-	    bfqd->rq_in_driver == 0)
+	    bfqd->rq_in_driver == 0) {
+		bfq_log(bfqd,
+"jumping to updating&resetting delta_last %lluus samples %d",
+			(now_ns - bfqd->last_dispatch)>>10,
+			bfqd->peak_rate_samples) ;
 		goto update_rate_and_reset;
+	}
 
 	/* Update sampling information */
 	bfqd->peak_rate_samples++;
@@ -3584,6 +4057,12 @@ static void bfq_update_peak_rate(struct bfq_data *bfqd, struct request *rq)
 
 	bfqd->delta_from_first = now_ns - bfqd->first_dispatch;
 
+	bfq_log(bfqd,
+	"added samples %u/%u tot_sects %llu delta_first %lluus",
+		bfqd->peak_rate_samples, bfqd->sequential_samples,
+		bfqd->tot_sectors_dispatched,
+		bfqd->delta_from_first>>10);
+
 	/* Target observation interval not yet reached, go on sampling */
 	if (bfqd->delta_from_first < BFQ_RATE_REF_INTERVAL)
 		goto update_last_values;
@@ -3595,6 +4074,14 @@ static void bfq_update_peak_rate(struct bfq_data *bfqd, struct request *rq)
 	if (RQ_BFQQ(rq) == bfqd->in_service_queue)
 		bfqd->in_serv_last_pos = bfqd->last_position;
 	bfqd->last_dispatch = now_ns;
+
+	bfq_log(bfqd,
+	"delta_first %lluus last_pos %llu peak_rate %llu",
+		(now_ns - bfqd->first_dispatch)>>10,
+		(unsigned long long) bfqd->last_position,
+		((USEC_PER_SEC*(u64)bfqd->peak_rate)>>BFQ_RATE_SHIFT));
+	bfq_log(bfqd,
+	"samples at end %d", bfqd->peak_rate_samples);
 }
 
 /*
@@ -3827,23 +4314,39 @@ static bool idling_needed_for_service_guarantees(struct bfq_data *bfqd,
 						 struct bfq_queue *bfqq)
 {
 	int tot_busy_queues = bfq_tot_busy_queues(bfqd);
+	bool asymmetric_scenario;
+
+	BFQ_BUG_ON(tot_busy_queues < 1);
 
 	/* No point in idling for bfqq if it won't get requests any longer */
 	if (unlikely(!bfqq_process_refs(bfqq)))
 		return false;
 
-	return (bfqq->wr_coeff > 1 &&
+	asymmetric_scenario = (bfqq->wr_coeff > 1 &&
 		(bfqd->wr_busy_queues <
 		 tot_busy_queues ||
 		 bfqd->rq_in_driver >=
 		 bfqq->dispatched + 4)) ||
 		bfq_asymmetric_scenario(bfqd, bfqq) ||
 		tot_busy_queues == 1;
+
+	bfq_log_bfqq(bfqd, bfqq,
+		     "wr_coeff %d wr_busy %d busy %d asymmetric %d",
+		     bfqq->wr_coeff,
+		     bfqd->wr_busy_queues,
+		     tot_busy_queues,
+		     asymmetric_scenario);
+
+	return asymmetric_scenario;
 }
 
 static bool __bfq_bfqq_expire(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 			      enum bfqq_expiration reason)
 {
+	BFQ_BUG_ON(bfqq != bfqd->in_service_queue);
+	BFQ_BUG_ON(!bfqq->entity.on_st_or_in_serv);
+	bfqq_process_refs(bfqq); // DEBUG: check process refs consistency
+
 	/*
 	 * If this bfqq is shared between multiple processes, check
 	 * to make sure that those processes are still issuing I/Os
@@ -3880,6 +4383,8 @@ static bool __bfq_bfqq_expire(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 
 		bfq_del_bfqq_busy(bfqd, bfqq, true);
 	} else {
+		BFQ_BUG_ON(RB_EMPTY_ROOT(&bfqq->sort_list) &&
+			   !bfqq_process_refs(bfqq));
 		bfq_requeue_bfqq(bfqd, bfqq, true);
 		/*
 		 * Resort priority tree of potential close cooperators.
@@ -3916,6 +4421,8 @@ static void __bfq_bfqq_recalc_budget(struct bfq_data *bfqd,
 	struct request *next_rq;
 	int budget, min_budget;
 
+	BFQ_BUG_ON(bfqq != bfqd->in_service_queue);
+
 	min_budget = bfq_min_budget(bfqd);
 
 	if (bfqq->wr_coeff == 1)
@@ -3928,11 +4435,11 @@ static void __bfq_bfqq_recalc_budget(struct bfq_data *bfqd,
 	      */
 		budget = 2 * min_budget;
 
-	bfq_log_bfqq(bfqd, bfqq, "recalc_budg: last budg %d, budg left %d",
+	bfq_log_bfqq(bfqd, bfqq, "last budg %d, budg left %d",
 		bfqq->entity.budget, bfq_bfqq_budget_left(bfqq));
-	bfq_log_bfqq(bfqd, bfqq, "recalc_budg: last max_budg %d, min budg %d",
+	bfq_log_bfqq(bfqd, bfqq, "last max_budg %d, min budg %d",
 		budget, bfq_min_budget(bfqd));
-	bfq_log_bfqq(bfqd, bfqq, "recalc_budg: sync %d, seeky %d",
+	bfq_log_bfqq(bfqd, bfqq, "sync %d, seeky %d",
 		bfq_bfqq_sync(bfqq), BFQQ_SEEKY(bfqd->in_service_queue));
 
 	if (bfq_bfqq_sync(bfqq) && bfqq->wr_coeff == 1) {
@@ -4061,9 +4568,14 @@ static void __bfq_bfqq_recalc_budget(struct bfq_data *bfqd,
 	 * it will be updated on the arrival of a new request.
 	 */
 	next_rq = bfqq->next_rq;
-	if (next_rq)
+	if (next_rq) {
+		BFQ_BUG_ON(reason == BFQQE_TOO_IDLE ||
+		       reason == BFQQE_NO_MORE_REQUESTS);
 		bfqq->entity.budget = max_t(unsigned long, bfqq->max_budget,
 					    bfq_serv_to_charge(next_rq, bfqq));
+		BFQ_BUG_ON(!bfq_bfqq_busy(bfqq));
+		BFQ_BUG_ON(RB_EMPTY_ROOT(&bfqq->sort_list));
+	}
 
 	bfq_log_bfqq(bfqd, bfqq, "head sect: %u, new budget %d",
 			next_rq ? blk_rq_sectors(next_rq) : 0,
@@ -4130,6 +4642,8 @@ static bool bfq_bfqq_is_slow(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 		else /* charge at least one seek */
 			*delta_ms = bfq_slice_idle / NSEC_PER_MSEC;
 
+		bfq_log(bfqd, "too short %u", delta_usecs);
+
 		return slow;
 	}
 
@@ -4151,9 +4665,11 @@ static bool bfq_bfqq_is_slow(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 		 * peak rate.
 		 */
 		slow = bfqq->entity.service < bfqd->bfq_max_budget / 2;
+		bfq_log(bfqd, "relative rate %d/%d",
+			bfqq->entity.service, bfqd->bfq_max_budget);
 	}
 
-	bfq_log_bfqq(bfqd, bfqq, "bfq_bfqq_is_slow: slow %d", slow);
+	bfq_log_bfqq(bfqd, bfqq, "slow %d", slow);
 
 	return slow;
 }
@@ -4254,6 +4770,13 @@ static bool bfq_bfqq_is_slow(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 static unsigned long bfq_bfqq_softrt_next_start(struct bfq_data *bfqd,
 						struct bfq_queue *bfqq)
 {
+	bfq_log_bfqq(bfqd, bfqq,
+"service_blkg %lu soft_rate %u sects/sec interval %u",
+		     bfqq->service_from_backlogged,
+		     bfqd->bfq_wr_max_softrt_rate,
+		     jiffies_to_msecs(HZ * bfqq->service_from_backlogged /
+				      bfqd->bfq_wr_max_softrt_rate));
+
 	return max3(bfqq->soft_rt_next_start,
 		    bfqq->last_idle_bklogged +
 		    HZ * bfqq->service_from_backlogged /
@@ -4296,6 +4819,8 @@ void bfq_bfqq_expire(struct bfq_data *bfqd,
 	unsigned long delta = 0;
 	struct bfq_entity *entity = &bfqq->entity;
 
+	BFQ_BUG_ON(bfqq != bfqd->in_service_queue);
+
 	/*
 	 * Check whether the process is slow (see bfq_bfqq_is_slow).
 	 */
@@ -4322,6 +4847,8 @@ void bfq_bfqq_expire(struct bfq_data *bfqd,
 	      bfq_bfqq_budget_left(bfqq) >=  entity->budget / 3)))
 		bfq_bfqq_charge_time(bfqd, bfqq, delta);
 
+	BFQ_BUG_ON(bfqq->entity.budget < bfqq->entity.service);
+
 	if (bfqd->low_latency && bfqq->wr_coeff == 1)
 		bfqq->last_wr_start_finish = jiffies;
 
@@ -4339,10 +4866,13 @@ void bfq_bfqq_expire(struct bfq_data *bfqd,
 		 * of all the outstanding requests to discover whether
 		 * the request pattern is actually isochronous.
 		 */
-		if (bfqq->dispatched == 0)
+		BFQ_BUG_ON(bfq_tot_busy_queues(bfqd) < 1);
+		if (bfqq->dispatched == 0) {
 			bfqq->soft_rt_next_start =
 				bfq_bfqq_softrt_next_start(bfqd, bfqq);
-		else if (bfqq->dispatched > 0) {
+			bfq_log_bfqq(bfqd, bfqq, "new soft_rt_next %lu",
+				     bfqq->soft_rt_next_start);
+		} else if (bfqq->dispatched > 0) {
 			/*
 			 * Schedule an update of soft_rt_next_start to when
 			 * the task may be discovered to be isochronous.
@@ -4352,8 +4882,10 @@ void bfq_bfqq_expire(struct bfq_data *bfqd,
 	}
 
 	bfq_log_bfqq(bfqd, bfqq,
-		"expire (%d, slow %d, num_disp %d, short_ttime %d)", reason,
-		slow, bfqq->dispatched, bfq_bfqq_has_short_ttime(bfqq));
+	"expire (%s, slow %d, num_disp %d, short %d, weight %d, serv %d/%d)",
+		     reason_name[reason], slow, bfqq->dispatched,
+		     bfq_bfqq_has_short_ttime(bfqq), entity->weight,
+		     entity->service, entity->budget);
 
 	/*
 	 * bfqq expired, so no total service time needs to be computed
@@ -4367,7 +4899,10 @@ void bfq_bfqq_expire(struct bfq_data *bfqd,
 	 * Increase, decrease or leave budget unchanged according to
 	 * reason.
 	 */
+	BFQ_BUG_ON(bfqq->entity.budget < bfqq->entity.service);
 	__bfq_bfqq_recalc_budget(bfqd, bfqq, reason);
+	BFQ_BUG_ON(bfqq->next_rq == NULL &&
+	       bfqq->entity.budget < bfqq->entity.service);
 	if (__bfq_bfqq_expire(bfqd, bfqq, reason))
 		/* bfqq is gone, no more actions on it */
 		return;
@@ -4376,14 +4911,18 @@ void bfq_bfqq_expire(struct bfq_data *bfqd,
 	if (!bfq_bfqq_busy(bfqq) &&
 	    reason != BFQQE_BUDGET_TIMEOUT &&
 	    reason != BFQQE_BUDGET_EXHAUSTED) {
+		BFQ_BUG_ON(!RB_EMPTY_ROOT(&bfqq->sort_list));
+		BFQ_BUG_ON(bfqq->next_rq);
 		bfq_mark_bfqq_non_blocking_wait_rq(bfqq);
 		/*
 		 * Not setting service to 0, because, if the next rq
 		 * arrives in time, the queue will go on receiving
 		 * service with this same budget (as if it never expired)
 		 */
-	} else
+	} else {
 		entity->service = 0;
+		bfq_log_bfqq(bfqd, bfqq, "resetting service");
+	}
 
 	/*
 	 * Reset the received-service counter for every parent entity.
@@ -4428,7 +4967,7 @@ static bool bfq_bfqq_budget_timeout(struct bfq_queue *bfqq)
 static bool bfq_may_expire_for_budg_timeout(struct bfq_queue *bfqq)
 {
 	bfq_log_bfqq(bfqq->bfqd, bfqq,
-		"may_budget_timeout: wait_request %d left %d timeout %d",
+		"wait_request %d left %d timeout %d",
 		bfq_bfqq_wait_request(bfqq),
 			bfq_bfqq_budget_left(bfqq) >=  bfqq->entity.budget / 3,
 		bfq_bfqq_budget_timeout(bfqq));
@@ -4480,6 +5019,11 @@ static bool idling_boosts_thr_without_issues(struct bfq_data *bfqd,
 		((!blk_queue_nonrot(bfqd->queue) || !bfqd->hw_tag) &&
 		 bfqq_sequential_and_IO_bound);
 
+	bfq_log_bfqq(bfqd, bfqq, "rot_no_q %d q %d seq %d boost %d",
+		     rot_without_queueing, bfqd->hw_tag,
+		     bfqq_sequential_and_IO_bound,
+		     idling_boosts_thr);
+
 	/*
 	 * The return value of this function is equal to that of
 	 * idling_boosts_thr, unless a special case holds. In this
@@ -4575,6 +5119,13 @@ static bool bfq_better_to_idle(struct bfq_queue *bfqq)
 	 * either boosts the throughput (without issues), or is
 	 * necessary to preserve service guarantees.
 	 */
+	bfq_log_bfqq(bfqd, bfqq,
+		     "wr_busy %d boosts %d IO-bound %d guar %d",
+		     bfqd->wr_busy_queues,
+		     idling_boosts_thr_with_no_issue,
+		     bfq_bfqq_IO_bound(bfqq),
+		     idling_needed_for_service_guar);
+
 	return idling_boosts_thr_with_no_issue ||
 		idling_needed_for_service_guar;
 }
@@ -4639,7 +5190,7 @@ bfq_choose_bfqq_for_injection(struct bfq_data *bfqd)
 		limit = 1;
 
 	if (bfqd->rq_in_driver >= limit)
-		return NULL;
+		goto no_queue;
 
 	/*
 	 * Linear search of the source queue for injection; but, with
@@ -4658,6 +5209,8 @@ bfq_choose_bfqq_for_injection(struct bfq_data *bfqd)
 		    (in_serv_always_inject || bfqq->wr_coeff > 1) &&
 		    bfq_serv_to_charge(bfqq->next_rq, bfqq) <=
 		    bfq_bfqq_budget_left(bfqq)) {
+			bfq_log_bfqq(bfqd, bfqq, "found this queue");
+
 			/*
 			 * Allow for only one large in-flight request
 			 * on non-rotational devices, for the
@@ -4682,12 +5235,22 @@ bfq_choose_bfqq_for_injection(struct bfq_data *bfqd)
 			else
 				limit = in_serv_bfqq->inject_limit;
 
+			bfq_log_bfqq(bfqd, bfqq,
+				     "rq_sect %u in_driver %d limit %u",
+				     blk_rq_sectors(bfqq->next_rq),
+				     bfqd->rq_in_driver, limit);
+
 			if (bfqd->rq_in_driver < limit) {
+				bfq_log_bfqq(bfqd, bfqq,
+					     "returned this queue, rqs_inj set");
 				bfqd->rqs_injected = true;
 				return bfqq;
 			}
 		}
 
+no_queue:
+	bfq_log(bfqd, "no queue found: in_driver %d limit %u",
+		bfqd->rq_in_driver, limit);
 	return NULL;
 }
 
@@ -4705,7 +5268,7 @@ static struct bfq_queue *bfq_select_queue(struct bfq_data *bfqd)
 	if (!bfqq)
 		goto new_queue;
 
-	bfq_log_bfqq(bfqd, bfqq, "select_queue: already in-service queue");
+	bfq_log_bfqq(bfqd, bfqq, "already in-service queue");
 
 	/*
 	 * Do not expire bfqq for budget timeout if bfqq may be about
@@ -4731,6 +5294,8 @@ static struct bfq_queue *bfq_select_queue(struct bfq_data *bfqd)
 	 * serve them, keep the queue, otherwise expire it.
 	 */
 	if (next_rq) {
+		BFQ_BUG_ON(RB_EMPTY_ROOT(&bfqq->sort_list));
+
 		if (bfq_serv_to_charge(next_rq, bfqq) >
 			bfq_bfqq_budget_left(bfqq)) {
 			/*
@@ -4790,6 +5355,26 @@ static struct bfq_queue *bfq_select_queue(struct bfq_data *bfqd)
 				     woken_list_node)
 			: NULL;
 
+		bfq_log_bfqq(bfqd, bfqq,
+			     "bic %p bfqq[0] %p busy %d",
+			     bfqq->bic,
+			     bfqq->bic ? bfqq->bic->bfqq[0] : NULL,
+			     (bfqq->bic && bfqq->bic->bfqq[0]) ?
+			     bfq_bfqq_busy(bfqq->bic->bfqq[0]) : false);
+
+		BFQ_BUG_ON(async_bfqq && !bfq_bfqq_sync(bfqq));
+
+		if (async_bfqq)
+			bfq_log_bfqq(bfqd, bfqq,
+				"bic ok %d serv_to_charge %lu, budg_left %d",
+				     icq_to_bic(async_bfqq->next_rq->elv.icq)
+				     == bfqq->bic,
+				     bfq_serv_to_charge(async_bfqq->next_rq,
+							async_bfqq),
+				     bfq_bfqq_budget_left(async_bfqq)
+				);
+		BFQ_BUG_ON(bfqq->waker_bfqq == bfqq);
+
 		/*
 		 * The next four mutually-exclusive ifs decide
 		 * whether to try injection, and choose the queue to
@@ -4873,31 +5458,70 @@ static struct bfq_queue *bfq_select_queue(struct bfq_data *bfqd)
 		if (async_bfqq &&
 		    icq_to_bic(async_bfqq->next_rq->elv.icq) == bfqq->bic &&
 		    bfq_serv_to_charge(async_bfqq->next_rq, async_bfqq) <=
-		    bfq_bfqq_budget_left(async_bfqq))
+		    bfq_bfqq_budget_left(async_bfqq)) {
+			bfq_log_bfqq(bfqd, bfqq,
+				     "choosing directly the async queue %d",
+				     bfq_get_first_task_pid(
+					     bfqq->bic->bfqq[0]));
+			BUG_ON(bfqq->bic->bfqq[0] == bfqq);
 			bfqq = bfqq->bic->bfqq[0];
-		else if (bfqq->waker_bfqq &&
+			bfq_log_bfqq(bfqd, bfqq,
+				     "chosen directly this async queue");
+		} else if (bfqq->waker_bfqq &&
 			   bfq_bfqq_busy(bfqq->waker_bfqq) &&
 			   bfqq->waker_bfqq->next_rq &&
 			   bfq_serv_to_charge(bfqq->waker_bfqq->next_rq,
 					      bfqq->waker_bfqq) <=
 			   bfq_bfqq_budget_left(bfqq->waker_bfqq)
-			)
+			) {
+			bfq_log_bfqq(bfqd, bfqq,
+				     "choosing directly the waker queue %d",
+				     bfq_get_first_task_pid(bfqq->waker_bfqq));
+			BUG_ON(bfqq->waker_bfqq == bfqq);
 			bfqq = bfqq->waker_bfqq;
-		else if (blocked_bfqq &&
+			bfq_log_bfqq(bfqd, bfqq,
+				     "chosen directly this waker queue");
+		} else if (blocked_bfqq &&
 			   bfq_bfqq_busy(blocked_bfqq) &&
 			   blocked_bfqq->next_rq &&
 			   bfq_serv_to_charge(blocked_bfqq->next_rq,
 					      blocked_bfqq) <=
 			   bfq_bfqq_budget_left(blocked_bfqq)
-			)
+			) {
+			bfq_log_bfqq(bfqd, bfqq,
+				     "choosing directly the blocked queue %d",
+				     bfq_get_first_task_pid(blocked_bfqq));
+			BUG_ON(blocked_bfqq == bfqq);
 			bfqq = blocked_bfqq;
-		else if (!idling_boosts_thr_without_issues(bfqd, bfqq) &&
+			bfq_log_bfqq(bfqd, bfqq,
+				     "chosen directly this blocked queue");
+		} else if (!idling_boosts_thr_without_issues(bfqd, bfqq) &&
 			 (bfqq->wr_coeff == 1 || bfqd->wr_busy_queues > 1 ||
-			  !bfq_bfqq_has_short_ttime(bfqq)))
-			bfqq = bfq_choose_bfqq_for_injection(bfqd);
-		else
-			bfqq = NULL;
+			  !bfq_bfqq_has_short_ttime(bfqq))) {
+			struct bfq_queue *new_bfqq;
+
+			bfq_log_bfqq(bfqd, bfqq,
+				     "looking inject wr_busy %d long_tt %d",
+				     bfqd->wr_busy_queues,
+				     !bfq_bfqq_has_short_ttime(bfqq));
+			new_bfqq = bfq_choose_bfqq_for_injection(bfqd);
+			BUG_ON(new_bfqq == bfqq);
+			if (new_bfqq)
+				bfq_log_bfqq(bfqd, bfqq,
+					"chosen the queue %d for injection",
+					bfq_get_first_task_pid(new_bfqq));
+			else
+				bfq_log_bfqq(bfqd, bfqq,
+			"idling with %d busy queues, as no injector found",
+					     bfq_tot_busy_queues(bfqd));
 
+			bfqq = new_bfqq;
+		} else {
+			bfq_log_bfqq(bfqd, bfqq,
+			"idling with %d busy queues, intentionally",
+				     bfq_tot_busy_queues(bfqd));
+			bfqq = NULL;
+		}
 		goto keep_queue;
 	}
 
@@ -4907,14 +5531,14 @@ static struct bfq_queue *bfq_select_queue(struct bfq_data *bfqd)
 new_queue:
 	bfqq = bfq_set_in_service_queue(bfqd);
 	if (bfqq) {
-		bfq_log_bfqq(bfqd, bfqq, "select_queue: checking new queue");
+		bfq_log_bfqq(bfqd, bfqq, "checking new queue");
 		goto check_queue;
 	}
 keep_queue:
 	if (bfqq)
-		bfq_log_bfqq(bfqd, bfqq, "select_queue: returned this queue");
+		bfq_log_bfqq(bfqd, bfqq, "returned this queue");
 	else
-		bfq_log(bfqd, "select_queue: no queue returned");
+		bfq_log(bfqd, "no queue returned");
 
 	return bfqq;
 }
@@ -4924,6 +5548,9 @@ static void bfq_update_wr_data(struct bfq_data *bfqd, struct bfq_queue *bfqq)
 	struct bfq_entity *entity = &bfqq->entity;
 
 	if (bfqq->wr_coeff > 1) { /* queue is being weight-raised */
+		BFQ_BUG_ON(bfqq->wr_cur_max_time == bfqd->bfq_wr_rt_max_time &&
+		       time_is_after_jiffies(bfqq->last_wr_start_finish));
+
 		bfq_log_bfqq(bfqd, bfqq,
 			"raising period dur %u/%u msec, old coeff %u, w %d(%d)",
 			jiffies_to_msecs(jiffies - bfqq->last_wr_start_finish),
@@ -4931,6 +5558,8 @@ static void bfq_update_wr_data(struct bfq_data *bfqd, struct bfq_queue *bfqq)
 			bfqq->wr_coeff,
 			bfqq->entity.weight, bfqq->entity.orig_weight);
 
+		BFQ_BUG_ON(bfqq != bfqd->in_service_queue && entity->weight !=
+		       entity->orig_weight * bfqq->wr_coeff);
 		if (entity->prio_changed)
 			bfq_log_bfqq(bfqd, bfqq, "WARN: pending prio change");
 
@@ -4961,7 +5590,11 @@ static void bfq_update_wr_data(struct bfq_data *bfqd, struct bfq_queue *bfqq)
 				  * interactive weight raising
 				  */
 				switch_back_to_interactive_wr(bfqq, bfqd);
+				BFQ_BUG_ON(time_is_after_jiffies(
+					       bfqq->last_wr_start_finish));
 				bfqq->entity.prio_changed = 1;
+				bfq_log_bfqq(bfqd, bfqq,
+					"back to interactive wr");
 			}
 		}
 		if (bfqq->wr_coeff > 1 &&
@@ -4969,6 +5602,10 @@ static void bfq_update_wr_data(struct bfq_data *bfqd, struct bfq_queue *bfqq)
 		    bfqq->service_from_wr > max_service_from_wr) {
 			/* see comments on max_service_from_wr */
 			bfq_bfqq_end_wr(bfqq);
+			bfq_log_bfqq(bfqd, bfqq,
+				     "too much service %lu > %lu",
+				     bfqq->service_from_wr,
+				     max_service_from_wr);
 		}
 	}
 	/*
@@ -4993,17 +5630,32 @@ static struct request *bfq_dispatch_rq_from_bfqq(struct bfq_data *bfqd,
 	struct request *rq = bfqq->next_rq;
 	unsigned long service_to_charge;
 
+	BFQ_BUG_ON(RB_EMPTY_ROOT(&bfqq->sort_list));
+	BFQ_BUG_ON(!rq);
 	service_to_charge = bfq_serv_to_charge(rq, bfqq);
 
+	BFQ_BUG_ON(service_to_charge > bfq_bfqq_budget_left(bfqq));
+
+	BFQ_BUG_ON(bfqq->entity.budget < bfqq->entity.service);
+
 	bfq_bfqq_served(bfqq, service_to_charge);
 
+	BFQ_BUG_ON(bfqq->entity.budget < bfqq->entity.service);
+
 	if (bfqq == bfqd->in_service_queue && bfqd->wait_dispatch) {
+		bfq_log_bfqq(bfqd, bfqq, "set waited_rq to %p", rq);
 		bfqd->wait_dispatch = false;
 		bfqd->waited_rq = rq;
 	}
-
 	bfq_dispatch_remove(bfqd->queue, rq);
 
+	bfq_log_bfqq(bfqd, bfqq,
+	     "dispatched %u sec req (%llu), budg left %d, new disp_nr %d",
+			blk_rq_sectors(rq),
+			(unsigned long long) blk_rq_pos(rq),
+		     bfq_bfqq_budget_left(bfqq),
+		     bfqq->dispatched);
+
 	if (bfqq != bfqd->in_service_queue)
 		goto return_rq;
 
@@ -5038,6 +5690,10 @@ static bool bfq_has_work(struct blk_mq_hw_ctx *hctx)
 {
 	struct bfq_data *bfqd = hctx->queue->elevator->elevator_data;
 
+	bfq_log(bfqd, "dispatch_non_empty %d busy_queues %d",
+		!list_empty_careful(&bfqd->dispatch),
+		bfq_tot_busy_queues(bfqd) > 0);
+
 	/*
 	 * Avoiding lock: a race on bfqd->busy_queues should cause at
 	 * most a call to dispatch for nothing
@@ -5056,7 +5712,10 @@ static struct request *__bfq_dispatch_request(struct blk_mq_hw_ctx *hctx)
 		rq = list_first_entry(&bfqd->dispatch, struct request,
 				      queuelist);
 		list_del_init(&rq->queuelist);
+		rq->rq_flags &= ~RQF_DISP_LIST;
 
+		bfq_log(bfqd,
+			"picked %p from dispatch list", rq);
 		bfqq = RQ_BFQQ(rq);
 
 		if (bfqq) {
@@ -5068,6 +5727,17 @@ static struct request *__bfq_dispatch_request(struct blk_mq_hw_ctx *hctx)
 			 */
 			bfqq->dispatched++;
 
+			/*
+			 * TESTING: reset DISP_LIST flag, because: 1)
+			 * this rq this request has passed through
+			 * bfq_prepare_request, 2) then it will have
+			 * bfq_finish_requeue_request invoked on it, and 3) in
+			 * bfq_finish_requeue_request we use this flag to check
+			 * that bfq_finish_requeue_request is not invoked on
+			 * requests for which bfq_prepare_request has
+			 * been invoked.
+			 */
+			rq->rq_flags &= ~RQF_DISP_LIST;
 			goto inc_in_driver_start_rq;
 		}
 
@@ -5097,8 +5767,7 @@ static struct request *__bfq_dispatch_request(struct blk_mq_hw_ctx *hctx)
 		goto start_rq;
 	}
 
-	bfq_log(bfqd, "dispatch requests: %d busy queues",
-		bfq_tot_busy_queues(bfqd));
+	bfq_log(bfqd, "%d busy queues", bfq_tot_busy_queues(bfqd));
 
 	if (bfq_tot_busy_queues(bfqd) == 0)
 		goto exit;
@@ -5122,14 +5791,36 @@ static struct request *__bfq_dispatch_request(struct blk_mq_hw_ctx *hctx)
 	if (!bfqq)
 		goto exit;
 
+	BFQ_BUG_ON(bfqq == bfqd->in_service_queue &&
+	       bfqq->entity.budget < bfqq->entity.service);
+
+	BFQ_BUG_ON(bfqq == bfqd->in_service_queue &&
+	       bfq_bfqq_wait_request(bfqq));
+
 	rq = bfq_dispatch_rq_from_bfqq(bfqd, bfqq);
 
+	BFQ_BUG_ON(bfqq->entity.budget < bfqq->entity.service);
+
 	if (rq) {
 inc_in_driver_start_rq:
 		bfqd->rq_in_driver++;
 start_rq:
 		rq->rq_flags |= RQF_STARTED;
-	}
+		if (bfqq)
+			bfq_log_bfqq(bfqd, bfqq,
+				"%s request %p (%u), rq_in_driver %d",
+				     bfq_bfqq_sync(bfqq) ? "sync" : "async",
+				     rq, blk_rq_sectors(rq),
+				     bfqd->rq_in_driver);
+		else
+			bfq_log(bfqd,
+		"request %p from dispatch list, rq_in_driver %d",
+				rq, bfqd->rq_in_driver);
+	} else
+		bfq_log(bfqd,
+		"returned NULL request, rq_in_driver %d",
+			bfqd->rq_in_driver);
+
 exit:
 	return rq;
 }
@@ -5204,6 +5895,8 @@ static struct request *bfq_dispatch_request(struct blk_mq_hw_ctx *hctx)
 			waiting_rq && !bfq_bfqq_wait_request(in_serv_queue);
 	}
 
+	BFQ_BUG_ON(bfq_tot_busy_queues(bfqd) <= 0 && bfqd->queued > 0);
+
 	spin_unlock_irq(&bfqd->lock);
 	bfq_update_dispatch_stats(hctx->queue, rq,
 			idle_timer_disabled ? in_serv_queue : NULL,
@@ -5219,20 +5912,30 @@ static struct request *bfq_dispatch_request(struct blk_mq_hw_ctx *hctx)
  * Scheduler lock must be held here. Recall not to use bfqq after calling
  * this function on it.
  */
-void bfq_put_queue(struct bfq_queue *bfqq)
+void __bfq_put_queue(struct bfq_queue *bfqq)
 {
 	struct bfq_queue *item;
 	struct hlist_node *n;
 	struct bfq_group *bfqg = bfqq_group(bfqq);
 
+	BFQ_BUG_ON(bfqq->ref <= 0);
+
 	if (bfqq->bfqd)
-		bfq_log_bfqq(bfqq->bfqd, bfqq, "put_queue: %p %d",
-			     bfqq, bfqq->ref);
+		bfq_log_bfqq(bfqq->bfqd, bfqq, "%p %d", bfqq, bfqq->ref);
 
 	bfqq->ref--;
+	bfqq_process_refs(bfqq); // DEBUG: check process ref consistency
+
 	if (bfqq->ref)
 		return;
 
+	BFQ_BUG_ON(rb_first(&bfqq->sort_list));
+	BFQ_BUG_ON(bfqq->entity.allocated != 0);
+	BFQ_BUG_ON(bfqq->entity.tree);
+	BFQ_BUG_ON(bfq_bfqq_busy(bfqq));
+	BFQ_BUG_ON(bfqq->entity.on_st_or_in_serv);
+	BFQ_BUG_ON(bfqq->weight_counter != NULL);
+
 	if (!hlist_unhashed(&bfqq->burst_list_node)) {
 		hlist_del_init(&bfqq->burst_list_node);
 		/*
@@ -5294,14 +5997,35 @@ void bfq_put_queue(struct bfq_queue *bfqq)
 	if (bfqq->bfqd && bfqq->bfqd->last_completed_rq_bfqq == bfqq)
 		bfqq->bfqd->last_completed_rq_bfqq = NULL;
 
-	kmem_cache_free(bfq_pool, bfqq);
+#ifdef CONFIG_BFQ_GROUP_IOSCHED
+	bfq_log_bfqq(bfqq->bfqd, bfqq, "putting blkg and bfqg %p\n", bfqg);
+#endif
 	bfqg_and_blkg_put(bfqg);
+	if (bfqq->bfqd)
+		bfq_log_bfqq(bfqq->bfqd, bfqq, "%p freed", bfqq);
+
+	kmem_cache_free(bfq_pool, bfqq);
 }
 
 static void bfq_put_stable_ref(struct bfq_queue *bfqq)
 {
+	BFQ_BUG_ON(bfqq->stable_ref <= 0);
 	bfqq->stable_ref--;
-	bfq_put_queue(bfqq);
+	__bfq_put_queue(bfqq);
+}
+
+/*
+ * Task holds one reference to the queue, dropped when task exits.  Each rq
+ * in-flight on this queue also holds a reference, dropped when rq is freed.
+ *
+ * Scheduler lock must be held here. Recall not to use bfqq after calling
+ * this function on it.
+ */
+void bfq_put_queue(struct bfq_queue *bfqq)
+{
+	assert_spin_locked(&bfqq->bfqd->lock);
+
+	__bfq_put_queue(bfqq);
 }
 
 static void bfq_put_cooperator(struct bfq_queue *bfqq)
@@ -5330,10 +6054,12 @@ static void bfq_exit_bfqq(struct bfq_data *bfqd, struct bfq_queue *bfqq)
 		bfq_schedule_dispatch(bfqd);
 	}
 
-	bfq_log_bfqq(bfqd, bfqq, "exit_bfqq: %p, %d", bfqq, bfqq->ref);
+	bfq_log_bfqq(bfqd, bfqq, "%p, %d", bfqq, bfqq->ref);
 
 	bfq_put_cooperator(bfqq);
 
+	if (!hlist_unhashed(&current->task_list_node))
+		hlist_del_init(&current->task_list_node);
 	bfq_release_process_ref(bfqd, bfqq);
 }
 
@@ -5352,6 +6078,7 @@ static void bfq_exit_icq_bfqq(struct bfq_io_cq *bic, bool is_sync)
 		bfqq->bic = NULL;
 		bfq_exit_bfqq(bfqd, bfqq);
 		bic_set_bfqq(bic, NULL, is_sync);
+		BFQ_BUG_ON(bfq_tot_busy_queues(bfqd) <= 0 && bfqd->queued > 0);
 		spin_unlock_irqrestore(&bfqd->lock, flags);
 	}
 }
@@ -5360,9 +6087,12 @@ static void bfq_exit_icq(struct io_cq *icq)
 {
 	struct bfq_io_cq *bic = icq_to_bic(icq);
 
+	BFQ_BUG_ON(!bic);
 	if (bic->stable_merge_bfqq) {
 		struct bfq_data *bfqd = bic->stable_merge_bfqq->bfqd;
 
+		BFQ_BUG_ON(bic->stable_merge_bfqq == bic_to_bfqq(bic, false));
+
 		/*
 		 * bfqd is NULL if scheduler already exited, and in
 		 * that case this is the last time bfqq is accessed.
@@ -5371,6 +6101,9 @@ static void bfq_exit_icq(struct io_cq *icq)
 			unsigned long flags;
 
 			spin_lock_irqsave(&bfqd->lock, flags);
+			BFQ_BUG_ON(bic->stable_merge_bfqq ==
+				   bic_to_bfqq(bic, true) &&
+				   bic->stable_merge_bfqq->ref == 1);
 			bfq_put_stable_ref(bic->stable_merge_bfqq);
 			spin_unlock_irqrestore(&bfqd->lock, flags);
 		} else {
@@ -5386,13 +6119,14 @@ static void bfq_exit_icq(struct io_cq *icq)
  * Update the entity prio values; note that the new values will not
  * be used until the next (re)activation.
  */
-static void
-bfq_set_next_ioprio_data(struct bfq_queue *bfqq, struct bfq_io_cq *bic)
+static void bfq_set_next_ioprio_data(struct bfq_queue *bfqq,
+				     struct bfq_io_cq *bic)
 {
 	struct task_struct *tsk = current;
 	int ioprio_class;
 	struct bfq_data *bfqd = bfqq->bfqd;
 
+	WARN_ON(!bfqd);
 	if (!bfqd)
 		return;
 
@@ -5458,9 +6192,13 @@ static void bfq_check_ioprio_change(struct bfq_io_cq *bic, struct bio *bio)
 
 	bfqq = bic_to_bfqq(bic, false);
 	if (bfqq) {
+		hlist_del_init(&current->task_list_node);
 		bfq_release_process_ref(bfqd, bfqq);
 		bfqq = bfq_get_queue(bfqd, bio, BLK_RW_ASYNC, bic, true);
 		bic_set_bfqq(bic, bfqq, false);
+		bfq_log_bfqq(bfqd, bfqq,
+			     "bfqq %p %d",
+			     bfqq, bfqq->ref);
 	}
 
 	bfqq = bic_to_bfqq(bic, true);
@@ -5478,6 +6216,9 @@ static void bfq_init_bfqq(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 	INIT_HLIST_NODE(&bfqq->burst_list_node);
 	INIT_HLIST_NODE(&bfqq->woken_list_node);
 	INIT_HLIST_HEAD(&bfqq->woken_list);
+	INIT_HLIST_HEAD(&bfqq->task_list);
+	INIT_HLIST_NODE(&current->task_list_node);
+	BFQ_BUG_ON(!hlist_unhashed(&bfqq->burst_list_node));
 
 	bfqq->ref = 0;
 	bfqq->bfqd = bfqd;
@@ -5508,7 +6249,8 @@ static void bfq_init_bfqq(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 
 	bfq_mark_bfqq_IO_bound(bfqq);
 
-	bfqq->pid = pid;
+	/* add current task to task_list in bfqq */
+	hlist_add_head(&current->task_list_node, &bfqq->task_list);
 
 	/* Tentative initial value to trade off between thr and lat */
 	bfqq->max_budget = (2 * bfq_max_budget(bfqd)) / 3;
@@ -5549,7 +6291,7 @@ static struct bfq_queue **bfq_async_queue_prio(struct bfq_data *bfqd,
 	case IOPRIO_CLASS_IDLE:
 		return &bfqg->async_idle_bfqq;
 	default:
-		return NULL;
+		BUG();
 	}
 }
 
@@ -5561,13 +6303,28 @@ bfq_do_early_stable_merge(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 	struct bfq_queue *new_bfqq =
 		bfq_setup_merge(bfqq, last_bfqq_created);
 
-	if (!new_bfqq)
+	BFQ_BUG_ON(last_bfqq_created == bfqq);
+	if (!new_bfqq) {
+		bfq_log_bfqq(bfqd, bfqq, "no new_bfqq found for stable merging");
 		return bfqq;
+	}
 
 	if (new_bfqq->bic)
 		new_bfqq->bic->stably_merged = true;
 	bic->stably_merged = true;
+	BFQ_BUG_ON(new_bfqq == bfqq);
+
+	bfq_log_bfqq(bfqd, bfqq,
+		     "stably merging %lu + %lu <= %lu",
+		     last_bfqq_created->creation_time,
+		     msecs_to_jiffies(bfq_activation_stable_merging),
+		     bfqq->creation_time);
+	BFQ_BUG_ON(bfqq->entity.weight == 0);
+	BFQ_BUG_ON(new_bfqq->entity.weight == 0);
+	BFQ_BUG_ON(bfqq->entity.new_weight == 0);
+	BFQ_BUG_ON(new_bfqq->entity.new_weight == 0);
 
+	BFQ_BUG_ON(bfqq->ref != 1);
 	/*
 	 * Reusing merge functions. This implies that
 	 * bfqq->bic must be set too, for
@@ -5577,6 +6334,12 @@ bfq_do_early_stable_merge(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 	bfqq->bic = bic;
 	bfq_merge_bfqqs(bfqd, bic, bfqq, new_bfqq);
 
+	BFQ_BUG_ON(bic->saved_weight == 0);
+	BFQ_BUG_ON(new_bfqq->entity.weight == 0);
+	BFQ_BUG_ON(new_bfqq->entity.new_weight == 0);
+	bfq_log_bfqq(bfqd, new_bfqq,
+		     "stably merged with new queue");
+
 	return new_bfqq;
 }
 
@@ -5636,6 +6399,8 @@ static struct bfq_queue *bfq_do_or_sched_stable_merge(struct bfq_data *bfqd,
 
 	struct bfq_queue *last_bfqq_created = *source_bfqq;
 
+	BFQ_BUG_ON(last_bfqq_created == bfqq);
+
 	/*
 	 * If last_bfqq_created has not been set yet, then init it. If
 	 * it has been set already, but too long ago, then move it
@@ -5661,11 +6426,29 @@ static struct bfq_queue *bfq_do_or_sched_stable_merge(struct bfq_data *bfqd,
 			bfqq->creation_time) ||
 		bfqq->entity.parent != last_bfqq_created->entity.parent ||
 		bfqq->ioprio != last_bfqq_created->ioprio ||
-		bfqq->ioprio_class != last_bfqq_created->ioprio_class)
+		bfqq->ioprio_class != last_bfqq_created->ioprio_class) {
+		if (!last_bfqq_created)
+			bfq_log_bfqq(bfqd, bfqq, "no last bfqq created");
+
+		if (last_bfqq_created &&
+		    !time_after_eq(last_bfqq_created->creation_time +
+				   msecs_to_jiffies(
+					   bfq_activation_stable_merging),
+				   bfqq->creation_time))
+			bfq_log_bfqq(bfqd, bfqq,
+				     "not merging stably %d > %d",
+				     jiffies_to_msecs(bfqq->creation_time -
+						      last_bfqq_created->creation_time),
+				     jiffies_to_msecs(bfqd->bfq_burst_interval));
+
 		*source_bfqq = bfqq;
-	else if (time_after_eq(last_bfqq_created->creation_time +
+		bfq_log_bfqq(bfqd, bfqq, "set as last bfqq created %lu",
+			     bfqq->creation_time);
+	} else if (time_after_eq(last_bfqq_created->creation_time +
 				 bfqd->bfq_burst_interval,
 				 bfqq->creation_time)) {
+		BFQ_BUG_ON(last_bfqq_created == bic_to_bfqq(bic, false));
+
 		if (likely(bfqd->nonrot_with_queueing))
 			/*
 			 * With this type of drive, leaving
@@ -5677,6 +6460,11 @@ static struct bfq_queue *bfq_do_or_sched_stable_merge(struct bfq_data *bfqd,
 							 bic,
 							 last_bfqq_created);
 		else { /* schedule tentative stable merge */
+			BFQ_BUG_ON(last_bfqq_created == bfqq);
+			BFQ_BUG_ON(last_bfqq_created ==
+				   bic_to_bfqq(bic, true));
+			BFQ_BUG_ON(last_bfqq_created ==
+				   bic_to_bfqq(bic, false));
 			/*
 			 * get reference on last_bfqq_created,
 			 * to prevent it from being freed,
@@ -5692,6 +6480,8 @@ static struct bfq_queue *bfq_do_or_sched_stable_merge(struct bfq_data *bfqd,
 			 * Record the bfqq to merge to.
 			 */
 			bic->stable_merge_bfqq = last_bfqq_created;
+			bfq_log_bfqq(bfqd, bfqq, "scheduled stable merge with bfq%d",
+				     bfq_get_first_task_pid(last_bfqq_created));
 		}
 	}
 
@@ -5734,13 +6524,16 @@ static struct bfq_queue *bfq_get_queue(struct bfq_data *bfqd,
 		bfq_init_bfqq(bfqd, bfqq, bic, current->pid,
 			      is_sync);
 		bfq_init_entity(&bfqq->entity, bfqg);
-		bfq_log_bfqq(bfqd, bfqq, "allocated");
+		bfq_log_bfqq(bfqd, bfqq, "allocated at time %lu",
+			     bfqq->creation_time);
 	} else {
 		bfqq = &bfqd->oom_bfqq;
 		bfq_log_bfqq(bfqd, bfqq, "using oom bfqq");
 		goto out;
 	}
 
+	BFQ_BUG_ON(!bfqq);
+
 	/*
 	 * Pin the queue now that it's allocated, scheduler exit will
 	 * prune it.
@@ -5753,16 +6546,27 @@ static struct bfq_queue *bfq_get_queue(struct bfq_data *bfqd,
 			      * guarantee that this queue is not freed
 			      * until its group goes away.
 			      */
-		bfq_log_bfqq(bfqd, bfqq, "get_queue, bfqq not in async: %p, %d",
+		bfq_log_bfqq(bfqd, bfqq, "bfqq not in async: %p, %d",
 			     bfqq, bfqq->ref);
 		*async_bfqq = bfqq;
 	}
 
 out:
+	bfqq->proc_ref++; /* get a process reference to this queue */
 	bfqq->ref++; /* get a process reference to this queue */
+	bfq_log_bfqq(bfqd, bfqq, "initial refs: %p, %d", bfqq, bfqq->ref);
+
+	if (bfqq != &bfqd->oom_bfqq && is_sync && !respawn) {
+		struct bfq_queue *last_bfqq_created = bfqq->entity.parent ?
+			bfqq->entity.parent->last_bfqq_created :
+			bfqd->last_bfqq_created;
+
+		BFQ_BUG_ON(last_bfqq_created == bfqq);
+
+		BFQ_BUG_ON(bfqq == bic_to_bfqq(bic, false));
 
-	if (bfqq != &bfqd->oom_bfqq && is_sync && !respawn)
 		bfqq = bfq_do_or_sched_stable_merge(bfqd, bfqq, bic);
+	}
 
 	rcu_read_unlock();
 	return bfqq;
@@ -5796,6 +6600,13 @@ bfq_update_io_seektime(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 {
 	bfqq->seek_history <<= 1;
 	bfqq->seek_history |= BFQ_RQ_SEEKY(bfqd, bfqq->last_request_pos, rq);
+	bfq_log_bfqq(bfqd, bfqq,
+		     "rq %p, distant %d, small %d, hist %x (%u), tot_seeky %d",
+		     rq, get_sdist(bfqq->last_request_pos, rq) > BFQQ_SEEK_THR,
+		     blk_rq_sectors(rq) < BFQQ_SECT_THR_NONROT,
+		     bfqq->seek_history,
+		     hweight32(bfqq->seek_history),
+		     BFQQ_TOTALLY_SEEKY(bfqq));
 
 	if (bfqq->wr_coeff > 1 &&
 	    bfqq->wr_cur_max_time == bfqd->bfq_wr_rt_max_time &&
@@ -5809,6 +6620,8 @@ bfq_update_io_seektime(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 			 * interactive weight raising).
 			 */
 			bfq_bfqq_end_wr(bfqq);
+			bfq_log_bfqq(bfqd, bfqq,
+				     "stopping soft_rt wr");
 		} else { /*
 			  * stopping soft_rt weight raising
 			  * while still in interactive period,
@@ -5817,6 +6630,8 @@ bfq_update_io_seektime(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 			  */
 			switch_back_to_interactive_wr(bfqq, bfqd);
 			bfqq->entity.prio_changed = 1;
+			bfq_log_bfqq(bfqd, bfqq,
+				     "back to interactive wr");
 		}
 	}
 }
@@ -5853,6 +6668,9 @@ static void bfq_update_has_short_ttime(struct bfq_data *bfqd,
 
 	state_changed = has_short_ttime != bfq_bfqq_has_short_ttime(bfqq);
 
+	bfq_log_bfqq(bfqd, bfqq, "has_short_ttime %d, changed %d",
+		     has_short_ttime, state_changed);
+
 	if (has_short_ttime)
 		bfq_mark_bfqq_has_short_ttime(bfqq);
 	else
@@ -6028,18 +6846,37 @@ static void bfqq_request_freed(struct bfq_queue *bfqq)
 /* returns true if it causes the idle timer to be disabled */
 static bool __bfq_insert_request(struct bfq_data *bfqd, struct request *rq)
 {
-	struct bfq_queue *bfqq = RQ_BFQQ(rq),
-		*new_bfqq = bfq_setup_cooperator(bfqd, bfqq, rq, true,
-						 RQ_BIC(rq));
+	struct bfq_queue *bfqq, *new_bfqq;
 	bool waiting, idle_timer_disabled = false;
 
+	bfqq = RQ_BFQQ(rq);
+	BFQ_BUG_ON(!bfqq);
+	BFQ_BUG_ON(bfqq->entity.weight == 0);
+	BFQ_BUG_ON(bfqq->entity.new_weight == 0);
+	BFQ_BUG_ON(bic_to_bfqq(RQ_BIC(rq), rq_is_sync(rq)) != bfqq);
+
+	new_bfqq = bfq_setup_cooperator(bfqd, bfqq, rq, true,
+						 RQ_BIC(rq));
+	BFQ_BUG_ON(new_bfqq == &bfqd->oom_bfqq);
+	BFQ_BUG_ON(bic_to_bfqq(RQ_BIC(rq), rq_is_sync(rq)) != bfqq);
+
+	assert_spin_locked(&bfqd->lock);
+	bfq_log_bfqq(bfqd, bfqq, "rq %p bfqq %p", rq, bfqq);
+
 	if (new_bfqq) {
+		BFQ_BUG_ON(bic_to_bfqq(RQ_BIC(rq), 1) != bfqq);
 		/*
 		 * Release the request's reference to the old bfqq
 		 * and make sure one is taken to the shared queue.
 		 */
 		bfqq_request_allocated(new_bfqq);
 		bfqq_request_freed(bfqq);
+		bfq_log_bfqq(bfqd, bfqq,
+		     "new allocated %d", bfqq->entity.allocated);
+			bfq_log_bfqq(bfqd, new_bfqq,
+		     "new_bfqq new allocated %d",
+				     new_bfqq->entity.allocated);
+
 		new_bfqq->ref++;
 		/*
 		 * If the bic associated with the process
@@ -6067,6 +6904,10 @@ static bool __bfq_insert_request(struct bfq_data *bfqd, struct request *rq)
 	bfq_update_has_short_ttime(bfqd, bfqq, RQ_BIC(rq));
 	bfq_update_io_seektime(bfqd, bfqq, rq);
 
+	bfq_log_bfqq(bfqd, bfqq,
+		     "has_short_ttime=%d (seeky %d)",
+		     bfq_bfqq_has_short_ttime(bfqq), BFQQ_SEEKY(bfqq));
+
 	waiting = bfqq && bfq_bfqq_wait_request(bfqq);
 	bfq_add_request(rq);
 	idle_timer_disabled = waiting && !bfq_bfqq_wait_request(bfqq);
@@ -6126,6 +6967,8 @@ static void bfq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 		bfqg_stats_update_legacy_io(q, rq);
 #endif
 	spin_lock_irq(&bfqd->lock);
+	BFQ_BUG_ON(bfq_tot_busy_queues(bfqd) <= 0 && bfqd->queued > 0);
+
 	if (blk_mq_sched_try_insert_merge(q, rq, &free)) {
 		spin_unlock_irq(&bfqd->lock);
 		blk_mq_free_requests(&free);
@@ -6137,13 +6980,33 @@ static void bfq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 	trace_block_rq_insert(rq);
 
 	spin_lock_irq(&bfqd->lock);
+	// XXX next function also takes a lock on bfqq
 	bfqq = bfq_init_rq(rq);
+
+	BFQ_BUG_ON(!bfqq && !(at_head || blk_rq_is_passthrough(rq)));
+	BFQ_BUG_ON(bfqq && bic_to_bfqq(RQ_BIC(rq), rq_is_sync(rq)) != bfqq);
+	BFQ_BUG_ON(bfqq->entity.weight == 0);
+
 	if (!bfqq || at_head) {
 		if (at_head)
 			list_add(&rq->queuelist, &bfqd->dispatch);
 		else
 			list_add_tail(&rq->queuelist, &bfqd->dispatch);
+
+		rq->rq_flags |= RQF_DISP_LIST;
+		if (bfqq)
+			bfq_log_bfqq(bfqd, bfqq,
+				     "%p in disp: at_head %d",
+				     rq, at_head);
+		else
+			bfq_log(bfqd,
+				"%p in disp: at_head %d",
+				rq, at_head);
 	} else {
+		BFQ_BUG_ON(!bfqq);
+		BFQ_BUG_ON(!(rq->rq_flags & RQF_GOT));
+		rq->rq_flags &= ~RQF_GOT;
+
 		idle_timer_disabled = __bfq_insert_request(bfqd, rq);
 		/*
 		 * Update bfqq, because, if a queue merge has occurred
@@ -6165,6 +7028,9 @@ static void bfq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 	 * merge).
 	 */
 	cmd_flags = rq->cmd_flags;
+
+	BFQ_BUG_ON(bfq_tot_busy_queues(bfqd) <= 0 && bfqd->queued > 0);
+
 	spin_unlock_irq(&bfqd->lock);
 
 	bfq_update_insert_stats(q, bfqq, idle_timer_disabled,
@@ -6231,10 +7097,19 @@ static void bfq_completed_request(struct bfq_queue *bfqq, struct bfq_data *bfqd)
 
 	bfq_update_hw_tag(bfqd);
 
+	BFQ_BUG_ON(!bfqd->rq_in_driver);
+	BFQ_BUG_ON(!bfqq->dispatched);
 	bfqd->rq_in_driver--;
+
 	bfqq->dispatched--;
 
+	bfq_log_bfqq(bfqd, bfqq,
+		     "in_serv %d, new disp %d, new rq_in_driver %d",
+		     bfqq == bfqd->in_service_queue,
+		     bfqq->dispatched, bfqd->rq_in_driver);
+
 	if (!bfqq->dispatched && !bfq_bfqq_busy(bfqq)) {
+		BFQ_BUG_ON(!RB_EMPTY_ROOT(&bfqq->sort_list));
 		/*
 		 * Set budget_timeout (which we overload to store the
 		 * time at which the queue remains with no backlog and
@@ -6248,6 +7123,9 @@ static void bfq_completed_request(struct bfq_queue *bfqq, struct bfq_data *bfqd)
 
 	now_ns = ktime_get_ns();
 
+	bfq_log_bfqq(bfqd, bfqq, "rq completion time: %llu us",
+		     div_u64(now_ns - bfqd->last_dispatch, NSEC_PER_USEC));
+
 	bfqq->ttime.last_end_request = now_ns;
 
 	/*
@@ -6256,6 +7134,17 @@ static void bfq_completed_request(struct bfq_queue *bfqq, struct bfq_data *bfqd)
 	 */
 	delta_us = div_u64(now_ns - bfqd->last_completion, NSEC_PER_USEC);
 
+	bfq_log_bfqq(bfqd, bfqq,
+		"delta %uus/%luus max_size %u rate %llu/%llu",
+		delta_us, BFQ_MIN_TT/NSEC_PER_USEC, bfqd->last_rq_max_size,
+		delta_us > 0 ?
+		(USEC_PER_SEC*
+		(u64)((bfqd->last_rq_max_size<<BFQ_RATE_SHIFT)/delta_us))
+			>>BFQ_RATE_SHIFT :
+		(USEC_PER_SEC*
+		(u64)(bfqd->last_rq_max_size<<BFQ_RATE_SHIFT))>>BFQ_RATE_SHIFT,
+		(USEC_PER_SEC*(u64)(1UL<<(BFQ_RATE_SHIFT-10)))>>BFQ_RATE_SHIFT);
+
 	/*
 	 * If the request took rather long to complete, and, according
 	 * to the maximum request size recorded, this completion latency
@@ -6356,6 +7245,9 @@ static void bfq_completed_request(struct bfq_queue *bfqq, struct bfq_data *bfqd)
 
 static void bfq_finish_requeue_request_body(struct bfq_queue *bfqq)
 {
+	bfq_log_bfqq(bfqq->bfqd, bfqq,
+		     "allocated %d", bfqq->entity.allocated);
+	BFQ_BUG_ON(!bfqq->entity.allocated);
 	bfqq_request_freed(bfqq);
 	bfq_put_queue(bfqq);
 }
@@ -6470,6 +7362,12 @@ static void bfq_update_inject_limit(struct bfq_data *bfqd,
 	u64 tot_time_ns = ktime_get_ns() - bfqd->last_empty_occupied_ns;
 	unsigned int old_limit = bfqq->inject_limit;
 
+	bfq_log_bfqq(bfqd, bfqq,
+		     "tot_time_ns %llu last_serv_time_ns %llu old limit %u",
+		     tot_time_ns, bfqq->last_serv_time_ns, old_limit);
+
+	bfq_log_bfqq(bfqd, bfqq, "rq_in_driver %d", bfqd->rq_in_driver);
+
 	if (bfqq->last_serv_time_ns > 0 && bfqd->rqs_injected) {
 		u64 threshold = (bfqq->last_serv_time_ns * 3)>>1;
 
@@ -6479,7 +7377,15 @@ static void bfq_update_inject_limit(struct bfq_data *bfqd,
 		} else if (tot_time_ns < threshold &&
 			   old_limit <= bfqd->max_rq_in_driver)
 			bfqq->inject_limit++;
+
+		bfq_log_bfqq(bfqd, bfqq,
+			"tot_time_ns %llu last_serv_time_ns %llu new limit %u",
+			tot_time_ns, bfqq->last_serv_time_ns,
+			bfqq->inject_limit);
 	}
+	BUG_ON(bfqq->last_serv_time_ns == 0 && old_limit > 1);
+
+	BUG_ON(bfqd->rq_in_driver < 1);
 
 	/*
 	 * Either we still have to compute the base value for the
@@ -6529,19 +7435,44 @@ static void bfq_update_inject_limit(struct bfq_data *bfqd,
  */
 static void bfq_finish_requeue_request(struct request *rq)
 {
-	struct bfq_queue *bfqq = RQ_BFQQ(rq);
+	struct bfq_queue *bfqq;
 	struct bfq_data *bfqd;
 	unsigned long flags;
+	struct bfq_io_cq *bic;
+
+	BFQ_BUG_ON(!rq);
+
+	bfqq = RQ_BFQQ(rq);
 
 	/*
 	 * rq either is not associated with any icq, or is an already
 	 * requeued request that has not (yet) been re-inserted into
 	 * a bfq_queue.
 	 */
-	if (!rq->elv.icq || !bfqq)
+	if (!rq->elv.icq || !bfqq) {
+#ifndef CONFIG_BFQ_MQ_NOLOG_BUG_ON
+		trace_printk("bfq_finish_requeue_request exiting %p %p %p",
+			     rq, rq->elv.icq, bfqq);
+#endif
 		return;
+	}
+
+	bic = RQ_BIC(rq);
+	BFQ_BUG_ON(!bic);
 
 	bfqd = bfqq->bfqd;
+	BFQ_BUG_ON(!bfqd);
+
+	if (rq->rq_flags & RQF_DISP_LIST) {
+		pr_crit("putting disp rq %p for %d", rq,
+			bfq_get_first_task_pid(bfqq));
+		BUG();
+	}
+
+	bfq_log_bfqq(bfqd, bfqq,
+		     "putting rq %p with %u sects left, STARTED %d",
+		     rq, blk_rq_sectors(rq),
+		     rq->rq_flags & RQF_STARTED);
 
 	if (rq->rq_flags & RQF_STARTED)
 		bfqg_stats_update_completion(bfqq_group(bfqq),
@@ -6550,12 +7481,17 @@ static void bfq_finish_requeue_request(struct request *rq)
 					     rq->cmd_flags);
 
 	spin_lock_irqsave(&bfqd->lock, flags);
+	WARN_ON(blk_rq_sectors(rq) == 0 && !(rq->rq_flags & RQF_STARTED));
+
 	if (likely(rq->rq_flags & RQF_STARTED)) {
 		if (rq == bfqd->waited_rq)
 			bfq_update_inject_limit(bfqd, bfqq);
 
 		bfq_completed_request(bfqq, bfqd);
 	}
+
+	BFQ_BUG_ON(bfq_tot_busy_queues(bfqd) <= 0 && bfqd->queued > 0);
+
 	bfq_finish_requeue_request_body(bfqq);
 	spin_unlock_irqrestore(&bfqd->lock, flags);
 
@@ -6602,12 +7538,14 @@ bfq_split_bfqq(struct bfq_io_cq *bic, struct bfq_queue *bfqq)
 	bfq_log_bfqq(bfqq->bfqd, bfqq, "splitting queue");
 
 	if (bfqq_process_refs(bfqq) == 1) {
-		bfqq->pid = current->pid;
 		bfq_clear_bfqq_coop(bfqq);
 		bfq_clear_bfqq_split_coop(bfqq);
 		return bfqq;
 	}
 
+	/* delete current task from queue iterating on task_list */
+	hlist_del_init(&current->task_list_node);
+
 	bic_set_bfqq(bic, NULL, 1);
 
 	bfq_put_cooperator(bfqq);
@@ -6633,13 +7571,28 @@ static struct bfq_queue *bfq_get_bfqq_handle_split(struct bfq_data *bfqd,
 	if (bfqq)
 		bfq_put_queue(bfqq);
 	bfqq = bfq_get_queue(bfqd, bio, is_sync, bic, split);
+	BFQ_BUG_ON(split && !hlist_unhashed(&bfqq->burst_list_node));
 
 	bic_set_bfqq(bic, bfqq, is_sync);
 	if (split && is_sync) {
+		bfq_log_bfqq(bfqd, bfqq,
+			     "get_request: was_in_list %d "
+			     "was_in_large_burst %d "
+			     "large burst in progress %d",
+			     bic->was_in_burst_list,
+			     bic->saved_in_large_burst,
+			     bfqd->large_burst);
+
 		if ((bic->was_in_burst_list && bfqd->large_burst) ||
-		    bic->saved_in_large_burst)
+		    bic->saved_in_large_burst) {
+			bfq_log_bfqq(bfqd, bfqq,
+				     "get_request: marking in "
+				     "large burst");
 			bfq_mark_bfqq_in_large_burst(bfqq);
-		else {
+		} else {
+			bfq_log_bfqq(bfqd, bfqq,
+				     "get_request: clearing in "
+				     "large burst");
 			bfq_clear_bfqq_in_large_burst(bfqq);
 			if (bic->was_in_burst_list)
 				/*
@@ -6741,8 +7694,10 @@ static struct bfq_queue *bfq_init_rq(struct request *rq)
 	 * events, a request cannot be manipulated any longer before
 	 * being removed from bfq.
 	 */
-	if (rq->elv.priv[1])
+	if (rq->elv.priv[1]) {
+		BFQ_BUG_ON(!(rq->rq_flags & RQF_ELVPRIV));
 		return rq->elv.priv[1];
+	}
 
 	bic = icq_to_bic(rq->elv.icq);
 
@@ -6753,12 +7708,19 @@ static struct bfq_queue *bfq_init_rq(struct request *rq)
 	bfqq = bfq_get_bfqq_handle_split(bfqd, bic, bio, false, is_sync,
 					 &new_queue);
 
+	BFQ_BUG_ON(bfqq->entity.weight == 0);
+	BFQ_BUG_ON(bfqq->entity.new_weight == 0);
+
 	if (likely(!new_queue)) {
 		/* If the queue was seeky for too long, break it apart. */
 		if (bfq_bfqq_coop(bfqq) && bfq_bfqq_split_coop(bfqq) &&
 			!bic->stably_merged) {
 			struct bfq_queue *old_bfqq = bfqq;
 
+			BFQ_BUG_ON(bfqq == &bfqd->oom_bfqq);
+			BFQ_BUG_ON(!is_sync);
+			bfq_log_bfqq(bfqd, bfqq, "breaking apart bfqq");
+
 			/* Update bic before losing reference to bfqq */
 			if (bfq_bfqq_in_large_burst(bfqq))
 				bic->saved_in_large_burst = true;
@@ -6770,7 +7732,11 @@ static struct bfq_queue *bfq_init_rq(struct request *rq)
 				bfqq = bfq_get_bfqq_handle_split(bfqd, bic, bio,
 								 true, is_sync,
 								 NULL);
+				BFQ_BUG_ON(bfqq->entity.weight == 0);
+				BFQ_BUG_ON(bfqq->entity.new_weight == 0);
+
 				bfqq->waker_bfqq = old_bfqq->waker_bfqq;
+				BFQ_BUG_ON(bfqq->waker_bfqq == bfqq);
 				bfqq->tentative_waker_bfqq = NULL;
 
 				/*
@@ -6780,21 +7746,35 @@ static struct bfq_queue *bfq_init_rq(struct request *rq)
 				 * woken_list of the waker. See
 				 * bfq_check_waker for details.
 				 */
-				if (bfqq->waker_bfqq)
+				if (bfqq->waker_bfqq) {
 					hlist_add_head(&bfqq->woken_list_node,
 						       &bfqq->waker_bfqq->woken_list);
+					bfq_log_bfqq(bfqd, bfqq,
+						     "gets waker %d back from bfq%d",
+						     bfq_get_first_task_pid(
+							     bfqq->waker_bfqq),
+						     bfq_get_first_task_pid(old_bfqq));
+
+				}
 			} else
 				bfqq_already_existing = true;
+
+			BFQ_BUG_ON(!bfqq);
+			BFQ_BUG_ON(bfqq->entity.weight == 0);
+			BFQ_BUG_ON(bfqq->entity.new_weight == 0);
 		}
 	}
 
 	bfqq_request_allocated(bfqq);
+	bfq_log_bfqq(bfqq->bfqd, bfqq,
+		     "new allocated %d", bfqq->entity.allocated);
+
 	bfqq->ref++;
-	bfq_log_bfqq(bfqd, bfqq, "get_request %p: bfqq %p, %d",
-		     rq, bfqq, bfqq->ref);
+	bfq_log_bfqq(bfqd, bfqq, "%p: bfqq %p, %d", rq, bfqq, bfqq->ref);
 
 	rq->elv.priv[0] = bic;
 	rq->elv.priv[1] = bfqq;
+	rq->rq_flags &= ~RQF_DISP_LIST;
 
 	/*
 	 * If a bfq_queue has only one process reference, it is owned
@@ -6812,6 +7792,8 @@ static struct bfq_queue *bfq_init_rq(struct request *rq)
 			 */
 			bfq_bfqq_resume_state(bfqq, bfqd, bic,
 					      bfqq_already_existing);
+			BFQ_BUG_ON(bfqq->entity.weight == 0);
+			BFQ_BUG_ON(bfqq->entity.new_weight == 0);
 		}
 	}
 
@@ -6840,6 +7822,8 @@ static struct bfq_queue *bfq_init_rq(struct request *rq)
 		      bfq_tot_busy_queues(bfqd) == 0)))
 		bfq_handle_burst(bfqd, bfqq);
 
+	rq->rq_flags |= RQF_GOT;
+
 	return bfqq;
 }
 
@@ -6849,8 +7833,11 @@ bfq_idle_slice_timer_body(struct bfq_data *bfqd, struct bfq_queue *bfqq)
 	enum bfqq_expiration reason;
 	unsigned long flags;
 
+	BFQ_BUG_ON(!bfqd);
 	spin_lock_irqsave(&bfqd->lock, flags);
 
+	bfq_log_bfqq(bfqd, bfqq, "handling slice_timer expiration");
+
 	/*
 	 * Considering that bfqq may be in race, we should firstly check
 	 * whether bfqq is in service before doing something on it. If
@@ -6900,6 +7887,8 @@ static enum hrtimer_restart bfq_idle_slice_timer(struct hrtimer *timer)
 					     idle_slice_timer);
 	struct bfq_queue *bfqq = bfqd->in_service_queue;
 
+	bfq_log(bfqd, "expired");
+
 	/*
 	 * Theoretical race here: the in-service queue can be NULL or
 	 * different from the queue that was idling if a new request
@@ -6919,11 +7908,11 @@ static void __bfq_put_async_bfqq(struct bfq_data *bfqd,
 {
 	struct bfq_queue *bfqq = *bfqq_ptr;
 
-	bfq_log(bfqd, "put_async_bfqq: %p", bfqq);
+	bfq_log(bfqd, "%p", bfqq);
 	if (bfqq) {
 		bfq_bfqq_move(bfqd, bfqq, bfqd->root_group);
 
-		bfq_log_bfqq(bfqd, bfqq, "put_async_bfqq: putting %p, %d",
+		bfq_log_bfqq(bfqd, bfqq, "putting %p, %d",
 			     bfqq, bfqq->ref);
 		bfq_put_queue(bfqq);
 		*bfqq_ptr = NULL;
@@ -7008,8 +7997,13 @@ static void bfq_exit_queue(struct elevator_queue *e)
 	struct bfq_data *bfqd = e->elevator_data;
 	struct bfq_queue *bfqq, *n;
 
+	bfq_log(bfqd, "starting ...");
+
 	hrtimer_cancel(&bfqd->idle_slice_timer);
 
+	BFQ_BUG_ON(bfqd->in_service_queue);
+	BFQ_BUG_ON(!list_empty(&bfqd->active_list));
+
 	spin_lock_irq(&bfqd->lock);
 	list_for_each_entry_safe(bfqq, n, &bfqd->idle_list, bfqq_list)
 		bfq_deactivate_bfqq(bfqd, bfqq, false, false);
@@ -7017,6 +8011,8 @@ static void bfq_exit_queue(struct elevator_queue *e)
 
 	hrtimer_cancel(&bfqd->idle_slice_timer);
 
+	BFQ_BUG_ON(hrtimer_active(&bfqd->idle_slice_timer));
+
 	/* release oom-queue reference to root group */
 	bfqg_and_blkg_put(bfqd->root_group);
 
@@ -7031,6 +8027,7 @@ static void bfq_exit_queue(struct elevator_queue *e)
 
 	wbt_enable_default(bfqd->queue);
 
+	bfq_log(bfqd, "finished ...");
 	kfree(bfqd);
 }
 
@@ -7076,6 +8073,7 @@ static int bfq_init_queue(struct request_queue *q, struct elevator_type *e)
 	 * will not attempt to free it.
 	 */
 	bfq_init_bfqq(bfqd, &bfqd->oom_bfqq, NULL, 1, 0);
+	bfqd->oom_bfqq.proc_ref++;
 	bfqd->oom_bfqq.ref++;
 	bfqd->oom_bfqq.new_ioprio = BFQ_DEFAULT_QUEUE_IOPRIO;
 	bfqd->oom_bfqq.new_ioprio_class = IOPRIO_CLASS_BE;
@@ -7209,6 +8207,58 @@ static int bfq_var_store(unsigned long *var, const char *page)
 	return 0;
 }
 
+static ssize_t bfq_wr_max_time_show(struct elevator_queue *e, char *page)
+{
+	struct bfq_data *bfqd = e->elevator_data;
+
+	return sprintf(page, "%d\n", bfqd->bfq_wr_max_time > 0 ?
+		       jiffies_to_msecs(bfqd->bfq_wr_max_time) :
+		       jiffies_to_msecs(bfq_wr_duration(bfqd)));
+}
+
+static ssize_t bfq_weights_show(struct elevator_queue *e, char *page)
+{
+	struct bfq_queue *bfqq;
+	struct bfq_data *bfqd = e->elevator_data;
+	ssize_t num_char = 0;
+
+	num_char += sprintf(page + num_char, "Tot reqs queued %d\n\n",
+			    bfqd->queued);
+
+	spin_lock_irq(&bfqd->lock);
+
+	num_char += sprintf(page + num_char, "Active:\n");
+	list_for_each_entry(bfqq, &bfqd->active_list, bfqq_list) {
+		num_char += sprintf(page + num_char,
+				    "pid%d: weight %hu, nr_queued %d %d, ",
+				    bfq_get_first_task_pid(bfqq),
+				    bfqq->entity.weight,
+				    bfqq->queued[0],
+				    bfqq->queued[1]);
+		num_char += sprintf(page + num_char,
+				    "dur %d/%u\n",
+				    jiffies_to_msecs(
+					    jiffies -
+					    bfqq->last_wr_start_finish),
+				    jiffies_to_msecs(bfqq->wr_cur_max_time));
+	}
+
+	num_char += sprintf(page + num_char, "Idle:\n");
+	list_for_each_entry(bfqq, &bfqd->idle_list, bfqq_list) {
+		num_char += sprintf(page + num_char,
+				    "pid%d: weight %hu, dur %d/%u\n",
+				    bfq_get_first_task_pid(bfqq),
+				    bfqq->entity.weight,
+				    jiffies_to_msecs(jiffies -
+						     bfqq->last_wr_start_finish),
+				    jiffies_to_msecs(bfqq->wr_cur_max_time));
+	}
+
+	spin_unlock_irq(&bfqd->lock);
+
+	return num_char;
+}
+
 #define SHOW_FUNCTION(__FUNC, __VAR, __CONV)				\
 static ssize_t __FUNC(struct elevator_queue *e, char *page)		\
 {									\
@@ -7229,6 +8279,12 @@ SHOW_FUNCTION(bfq_max_budget_show, bfqd->bfq_user_max_budget, 0);
 SHOW_FUNCTION(bfq_timeout_sync_show, bfqd->bfq_timeout, 1);
 SHOW_FUNCTION(bfq_strict_guarantees_show, bfqd->strict_guarantees, 0);
 SHOW_FUNCTION(bfq_low_latency_show, bfqd->low_latency, 0);
+SHOW_FUNCTION(bfq_wr_coeff_show, bfqd->bfq_wr_coeff, 0);
+SHOW_FUNCTION(bfq_wr_rt_max_time_show, bfqd->bfq_wr_rt_max_time, 1);
+SHOW_FUNCTION(bfq_wr_min_idle_time_show, bfqd->bfq_wr_min_idle_time, 1);
+SHOW_FUNCTION(bfq_wr_min_inter_arr_async_show, bfqd->bfq_wr_min_inter_arr_async,
+	1);
+SHOW_FUNCTION(bfq_wr_max_softrt_rate_show, bfqd->bfq_wr_max_softrt_rate, 0);
 #undef SHOW_FUNCTION
 
 #define USEC_SHOW_FUNCTION(__FUNC, __VAR)				\
@@ -7273,6 +8329,16 @@ STORE_FUNCTION(bfq_back_seek_max_store, &bfqd->bfq_back_max, 0, INT_MAX, 0);
 STORE_FUNCTION(bfq_back_seek_penalty_store, &bfqd->bfq_back_penalty, 1,
 		INT_MAX, 0);
 STORE_FUNCTION(bfq_slice_idle_store, &bfqd->bfq_slice_idle, 0, INT_MAX, 2);
+STORE_FUNCTION(bfq_wr_coeff_store, &bfqd->bfq_wr_coeff, 1, INT_MAX, 0);
+STORE_FUNCTION(bfq_wr_max_time_store, &bfqd->bfq_wr_max_time, 0, INT_MAX, 1);
+STORE_FUNCTION(bfq_wr_rt_max_time_store, &bfqd->bfq_wr_rt_max_time, 0, INT_MAX,
+		1);
+STORE_FUNCTION(bfq_wr_min_idle_time_store, &bfqd->bfq_wr_min_idle_time, 0,
+		INT_MAX, 1);
+STORE_FUNCTION(bfq_wr_min_inter_arr_async_store,
+		&bfqd->bfq_wr_min_inter_arr_async, 0, INT_MAX, 1);
+STORE_FUNCTION(bfq_wr_max_softrt_rate_store, &bfqd->bfq_wr_max_softrt_rate, 0,
+		INT_MAX, 0);
 #undef STORE_FUNCTION
 
 #define USEC_STORE_FUNCTION(__FUNC, __PTR, MIN, MAX)			\
@@ -7296,6 +8362,13 @@ USEC_STORE_FUNCTION(bfq_slice_idle_us_store, &bfqd->bfq_slice_idle, 0,
 		    UINT_MAX);
 #undef USEC_STORE_FUNCTION
 
+/* do nothing for the moment */
+static ssize_t bfq_weights_store(struct elevator_queue *e,
+				    const char *page, size_t count)
+{
+	return count;
+}
+
 static ssize_t bfq_max_budget_store(struct elevator_queue *e,
 				    const char *page, size_t count)
 {
@@ -7403,6 +8476,13 @@ static struct elv_fs_entry bfq_attrs[] = {
 	BFQ_ATTR(timeout_sync),
 	BFQ_ATTR(strict_guarantees),
 	BFQ_ATTR(low_latency),
+	BFQ_ATTR(wr_coeff),
+	BFQ_ATTR(wr_max_time),
+	BFQ_ATTR(wr_rt_max_time),
+	BFQ_ATTR(wr_min_idle_time),
+	BFQ_ATTR(wr_min_inter_arr_async),
+	BFQ_ATTR(wr_max_softrt_rate),
+	BFQ_ATTR(weights),
 	__ATTR_NULL
 };
 
@@ -7440,6 +8520,7 @@ MODULE_ALIAS("bfq-iosched");
 static int __init bfq_init(void)
 {
 	int ret;
+	char msg[60] = "BFQ I/O-scheduler: v11";
 
 #ifdef CONFIG_BFQ_GROUP_IOSCHED
 	ret = blkcg_policy_register(&blkcg_policy_bfq);
@@ -7471,6 +8552,11 @@ static int __init bfq_init(void)
 	if (ret)
 		goto slab_kill;
 
+#ifdef CONFIG_BFQ_GROUP_IOSCHED
+	strcat(msg, " (with cgroups support)");
+#endif
+	pr_info("%s", msg);
+
 	return 0;
 
 slab_kill:
@@ -7491,6 +8577,18 @@ static void __exit bfq_exit(void)
 	bfq_slab_kill();
 }
 
+pid_t bfq_get_first_task_pid(struct bfq_queue *bfqq)
+{
+	struct task_struct *item;
+
+	if ((&bfqq->task_list)->first != NULL)
+		return (hlist_entry_safe( (&bfqq->task_list)->first,
+					  typeof(*(item)), task_list_node))
+			->pid;
+
+	return -1;
+}
+
 module_init(bfq_init);
 module_exit(bfq_exit);
 
diff --git a/block/bfq-iosched.h b/block/bfq-iosched.h
index 07288b9da..48e113f2c 100644
--- a/block/bfq-iosched.h
+++ b/block/bfq-iosched.h
@@ -25,7 +25,7 @@
 #define BFQ_DEFAULT_GRP_IOPRIO	0
 #define BFQ_DEFAULT_GRP_CLASS	IOPRIO_CLASS_BE
 
-#define MAX_BFQQ_NAME_LENGTH 16
+#define MAX_PID_STR_LENGTH 256
 
 /*
  * Soft real-time applications are extremely more latency sensitive
@@ -238,6 +238,8 @@ struct bfq_queue {
 	int ref;
 	/* counter of references from other queues for delayed stable merge */
 	int stable_ref;
+	/* DEBUG: process ref counter */
+	int proc_ref;
 	/* parent bfq_data */
 	struct bfq_data *bfqd;
 
@@ -319,6 +321,9 @@ struct bfq_queue {
 	 */
 	unsigned int requests_within_timer;
 
+	/* list of task in queue */
+	struct hlist_head task_list;
+
 	/* pid of the process owning the queue, used for logging purposes */
 	pid_t pid;
 
@@ -765,6 +770,8 @@ struct bfq_data {
 	struct bfq_io_cq *bio_bic;
 	/* bfqq associated with the task issuing current bio for merging */
 	struct bfq_queue *bio_bfqq;
+	/* Extra flag used only for TESTING */
+	bool bio_bfqq_set;
 
 	/*
 	 * Depth limits used in bfq_limit_depth (see comments on the
@@ -984,6 +991,7 @@ void bfq_end_wr_async_queues(struct bfq_data *bfqd, struct bfq_group *bfqg);
 void bfq_release_process_ref(struct bfq_data *bfqd, struct bfq_queue *bfqq);
 void bfq_schedule_dispatch(struct bfq_data *bfqd);
 void bfq_put_async_queues(struct bfq_data *bfqd, struct bfq_group *bfqg);
+pid_t bfq_get_first_task_pid(struct bfq_queue *queue);
 
 /* ------------ end of main algorithm interface -------------- */
 
@@ -1083,48 +1091,135 @@ void bfq_add_bfqq_busy(struct bfq_data *bfqd, struct bfq_queue *bfqq);
 /* --------------- end of interface of B-WF2Q+ ---------------- */
 
 /* Logging facilities. */
-static inline void bfq_bfqq_name(struct bfq_queue *bfqq, char *str, int len)
+static inline void bfq_pid_to_str(int pid, char *str, int len, struct bfq_queue *bfqq)
+{
+	struct task_struct *item;
+	ssize_t num_char = 0;
+
+	hlist_for_each_entry(item, &bfqq->task_list, task_list_node) {
+		num_char += sprintf(str + num_char,
+					"%d-",
+					item->pid);
+	}
+}
+
+#ifdef CONFIG_BFQ_REDIRECT_TO_CONSOLE
+
+static const char *checked_dev_name(const struct device *dev)
 {
-	char type = bfq_bfqq_sync(bfqq) ? 'S' : 'A';
+	static const char nodev[] = "nodev";
 
-	if (bfqq->pid != -1)
-		snprintf(str, len, "bfq%d%c", bfqq->pid, type);
-	else
-		snprintf(str, len, "bfqSHARED-%c", type);
+	if (dev)
+		return dev_name(dev);
+
+	return nodev;
 }
 
 #ifdef CONFIG_BFQ_GROUP_IOSCHED
-struct bfq_group *bfqq_group(struct bfq_queue *bfqq);
 
-#define bfq_log_bfqq(bfqd, bfqq, fmt, args...)	do {			\
-	char pid_str[MAX_BFQQ_NAME_LENGTH];				\
+#define bfq_log_bfqq(bfqd, bfqq, fmt, args...)  do {		\
+	char pid_str[MAX_PID_STR_LENGTH];			\
 	if (likely(!blk_trace_note_message_enabled((bfqd)->queue)))	\
 		break;							\
-	bfq_bfqq_name((bfqq), pid_str, MAX_BFQQ_NAME_LENGTH);		\
-	blk_add_cgroup_trace_msg((bfqd)->queue,				\
-			bfqg_to_blkg(bfqq_group(bfqq))->blkcg,		\
-			"%s " fmt, pid_str, ##args);			\
+	bfq_pid_to_str((bfqq)->pid, pid_str, MAX_PID_STR_LENGTH, bfqq); \
+	pr_crit("%s bfq%s%c %s [%s] " fmt "\n",			\
+		checked_dev_name((bfqd)->queue->backing_dev_info->dev), \
+		pid_str,					\
+		bfq_bfqq_sync((bfqq)) ? 'S' : 'A',		\
+		bfqq_group(bfqq)->blkg_path, __func__, ##args);	\
 } while (0)
 
-#define bfq_log_bfqg(bfqd, bfqg, fmt, args...)	do {			\
-	blk_add_cgroup_trace_msg((bfqd)->queue,				\
-		bfqg_to_blkg(bfqg)->blkcg, fmt, ##args);		\
+#define bfq_log_bfqg(bfqd, bfqg, fmt, args...)  do {                    \
+	pr_crit("%s %s [%s] " fmt "\n",				\
+		checked_dev_name((bfqd)->queue->backing_dev_info->dev),	\
+		bfqg->blkg_path, __func__, ##args);		\
 } while (0)
 
 #else /* CONFIG_BFQ_GROUP_IOSCHED */
 
-#define bfq_log_bfqq(bfqd, bfqq, fmt, args...) do {	\
-	char pid_str[MAX_BFQQ_NAME_LENGTH];				\
+#define bfq_log_bfqq(bfqd, bfqq, fmt, args...) do {			\
+	char pid_str[MAX_PID_STR_LENGTH];			\
 	if (likely(!blk_trace_note_message_enabled((bfqd)->queue)))	\
 		break;							\
-	bfq_bfqq_name((bfqq), pid_str, MAX_BFQQ_NAME_LENGTH);		\
-	blk_add_trace_msg((bfqd)->queue, "%s " fmt, pid_str, ##args);	\
-} while (0)
-#define bfq_log_bfqg(bfqd, bfqg, fmt, args...)		do {} while (0)
+	bfq_pid_to_str((bfqq)->pid, pid_str, MAX_PID_STR_LENGTH, bfqq); \
+	pr_crit("%s bfq%s%c %s [%s] " fmt "\n",			\
+		checked_dev_name((bfqd)->queue->backing_dev_info->dev), \
+		pid_str, bfq_bfqq_sync((bfqq)) ? 'S' : 'A',	\
+		__func__, ##args);				\
+	} while (0)
+#define bfq_log_bfqg(bfqd, bfqg, fmt, args...)          do {} while (0)
 
 #endif /* CONFIG_BFQ_GROUP_IOSCHED */
+#define bfq_log(bfqd, fmt, args...)					\
+	pr_crit("%s bfq [%s] " fmt "\n",				\
+		checked_dev_name((bfqd)->queue->backing_dev_info->dev), \
+		__func__, ##args)
+
+#else /* CONFIG_BFQ_REDIRECT_TO_CONSOLE */
+
+#if defined(CONFIG_BFQ_MQ_NOLOG_BUG_ON) || !defined(CONFIG_BLK_DEV_IO_TRACE)
+
+/* Avoid possible "unused-variable" warning. See commit message. */
+
+#define bfq_log_bfqq(bfqd, bfqq, fmt, args...)  ((void) (bfqq))
+
+#define bfq_log_bfqg(bfqd, bfqg, fmt, args...)  ((void) (bfqg))
+
+#define bfq_log(bfqd, fmt, args...)             do {} while (0)
+
+#else /* CONFIG_BLK_DEV_IO_TRACE */
+
+#include <linux/blktrace_api.h>
+
+#ifdef CONFIG_BFQ_GROUP_IOSCHED
 
-#define bfq_log(bfqd, fmt, args...) \
-	blk_add_trace_msg((bfqd)->queue, "bfq " fmt, ##args)
+#define bfq_log_bfqq(bfqd, bfqq, fmt, args...)  do {		\
+	char pid_str[MAX_PID_STR_LENGTH];			\
+	if (likely(!blk_trace_note_message_enabled((bfqd)->queue)))	\
+		break;							\
+	bfq_pid_to_str((bfqq)->pid, pid_str, MAX_PID_STR_LENGTH, bfqq); \
+	blk_add_trace_msg((bfqd)->queue, "bfq%s%c %s [%s] " fmt, \
+			  pid_str,				\
+			  bfq_bfqq_sync((bfqq)) ? 'S' : 'A',    \
+			  bfqq_group(bfqq)->blkg_path, __func__, ##args); \
+	} while (0)
+
+#define bfq_log_bfqg(bfqd, bfqg, fmt, args...)  do {                    \
+	if (likely(!blk_trace_note_message_enabled((bfqd)->queue)))	\
+		break;							\
+	blk_add_trace_msg((bfqd)->queue, "%s [%s] " fmt, bfqg->blkg_path, \
+				  __func__, ##args);			\
+	} while (0)
+
+#else /* CONFIG_BFQ_GROUP_IOSCHED */
+
+#define bfq_log_bfqq(bfqd, bfqq, fmt, args...) do {			\
+	char pid_str[MAX_PID_STR_LENGTH];			\
+	if (likely(!blk_trace_note_message_enabled((bfqd)->queue)))	\
+		break;							\
+	bfq_pid_to_str((bfqq)->pid, pid_str, MAX_PID_STR_LENGTH, bfqq); \
+	blk_add_trace_msg((bfqd)->queue, "bfq%s%c [%s] " fmt, pid_str, \
+			  bfq_bfqq_sync((bfqq)) ? 'S' : 'A',	\
+			  __func__, ##args);			\
+	} while (0)
+#define bfq_log_bfqg(bfqd, bfqg, fmt, args...)          do {} while (0)
+
+#endif /* CONFIG_BFQ_GROUP_IOSCHED */
+
+#define bfq_log(bfqd, fmt, args...)  do {				\
+	if (likely(!blk_trace_note_message_enabled((bfqd)->queue)))	\
+		break;							\
+	blk_add_trace_msg((bfqd)->queue, "bfq [%s] " fmt, __func__, ##args); \
+	} while (0)
+
+#endif /* CONFIG_BLK_DEV_IO_TRACE */
+#endif /* CONFIG_BFQ_REDIRECT_TO_CONSOLE */
+
+#if defined(CONFIG_BFQ_MQ_NOLOG_BUG_ON)
+/* Avoid possible "unused-variable" warning. */
+#define BFQ_BUG_ON(cond)        ((void) (cond))
+#else
+#define BFQ_BUG_ON(cond)        BUG_ON(cond)
+#endif
 
 #endif /* _BFQ_H */
diff --git a/block/bfq-wf2q.c b/block/bfq-wf2q.c
index 709b901de..38507140e 100644
--- a/block/bfq-wf2q.c
+++ b/block/bfq-wf2q.c
@@ -76,6 +76,7 @@ static bool bfq_update_next_in_service(struct bfq_sched_data *sd,
 				       bool expiration)
 {
 	struct bfq_entity *next_in_service = sd->next_in_service;
+	struct bfq_queue *bfqq;
 	bool parent_sched_may_change = false;
 	bool change_without_lookup = false;
 
@@ -120,8 +121,24 @@ static bool bfq_update_next_in_service(struct bfq_sched_data *sd,
 					new_entity->finish));
 		}
 
-		if (change_without_lookup)
+		if (change_without_lookup) {
 			next_in_service = new_entity;
+			bfqq = bfq_entity_to_bfqq(next_in_service);
+
+			if (bfqq)
+				bfq_log_bfqq(bfqq->bfqd, bfqq,
+				"chose without lookup");
+#ifdef CONFIG_BFQ_GROUP_IOSCHED
+			else {
+				struct bfq_group *bfqg =
+					container_of(next_in_service,
+						     struct bfq_group, entity);
+
+				bfq_log_bfqg((struct bfq_data*)bfqg->bfqd, bfqg,
+				"chose without lookup");
+			}
+#endif
+		}
 	}
 
 	if (!change_without_lookup) /* lookup needed */
@@ -137,6 +154,23 @@ static bool bfq_update_next_in_service(struct bfq_sched_data *sd,
 
 	sd->next_in_service = next_in_service;
 
+	if (!next_in_service)
+		return parent_sched_may_change;
+
+	bfqq = bfq_entity_to_bfqq(next_in_service);
+	if (bfqq)
+		bfq_log_bfqq(bfqq->bfqd, bfqq,
+			     "chosen this queue");
+#ifdef CONFIG_BFQ_GROUP_IOSCHED
+	else {
+		struct bfq_group *bfqg =
+			container_of(next_in_service,
+				     struct bfq_group, entity);
+
+		bfq_log_bfqg((struct bfq_data *)bfqg->bfqd, bfqg,
+			     "chosen this entity");
+	}
+#endif
 	return parent_sched_may_change;
 }
 
@@ -163,6 +197,8 @@ static bool bfq_update_parent_budget(struct bfq_entity *next_in_service)
 	struct bfq_sched_data *group_sd;
 	bool ret = false;
 
+	BFQ_BUG_ON(!next_in_service);
+
 	group_sd = next_in_service->sched_data;
 
 	bfqg = container_of(group_sd, struct bfq_group, sched_data);
@@ -175,6 +211,9 @@ static bool bfq_update_parent_budget(struct bfq_entity *next_in_service)
 	if (bfqg_entity) {
 		if (bfqg_entity->budget > next_in_service->budget)
 			ret = true;
+		bfq_log_bfqg((struct bfq_data *)bfqg->bfqd, bfqg,
+			"old budg: %d, new budg: %d",
+			bfqg_entity->budget, next_in_service->budget);
 		bfqg_entity->budget = next_in_service->budget;
 	}
 
@@ -210,6 +249,8 @@ static bool bfq_no_longer_next_in_service(struct bfq_entity *entity)
 
 	bfqg = container_of(entity, struct bfq_group, entity);
 
+	BFQ_BUG_ON(bfqg == ((struct bfq_data *)(bfqg->bfqd))->root_group);
+	BFQ_BUG_ON(bfqg->active_entities == 0);
 	/*
 	 * The field active_entities does not always contain the
 	 * actual number of active children entities: it happens to
@@ -260,6 +301,8 @@ struct bfq_queue *bfq_entity_to_bfqq(struct bfq_entity *entity)
 {
 	struct bfq_queue *bfqq = NULL;
 
+	BFQ_BUG_ON(!entity);
+
 	if (!entity->my_sched_data)
 		bfqq = container_of(entity, struct bfq_queue, entity);
 
@@ -285,18 +328,36 @@ static u64 bfq_delta(unsigned long service, unsigned long weight)
 static void bfq_calc_finish(struct bfq_entity *entity, unsigned long service)
 {
 	struct bfq_queue *bfqq = bfq_entity_to_bfqq(entity);
+	unsigned long long start, finish, delta;
+
+	BFQ_BUG_ON(entity->weight == 0);
 
 	entity->finish = entity->start +
 		bfq_delta(service, entity->weight);
 
+	start = ((entity->start>>10)*1000)>>12;
+	finish = ((entity->finish>>10)*1000)>>12;
+	delta = ((bfq_delta(service, entity->weight)>>10)*1000)>>12;
+
 	if (bfqq) {
 		bfq_log_bfqq(bfqq->bfqd, bfqq,
-			"calc_finish: serv %lu, w %d",
+			"serv %lu, w %d",
 			service, entity->weight);
 		bfq_log_bfqq(bfqq->bfqd, bfqq,
-			"calc_finish: start %llu, finish %llu, delta %llu",
-			entity->start, entity->finish,
-			bfq_delta(service, entity->weight));
+			"start %llu, finish %llu, delta %llu",
+			start, finish, delta);
+#ifdef CONFIG_BFQ_GROUP_IOSCHED
+	} else {
+		struct bfq_group *bfqg =
+			container_of(entity, struct bfq_group, entity);
+
+		bfq_log_bfqg((struct bfq_data *)bfqg->bfqd, bfqg,
+			"group: serv %lu, w %d",
+			     service, entity->weight);
+		bfq_log_bfqg((struct bfq_data *)bfqg->bfqd, bfqg,
+			"group: start %llu, finish %llu, delta %llu",
+			start, finish, delta);
+#endif
 	}
 }
 
@@ -326,6 +387,8 @@ struct bfq_entity *bfq_entity_of(struct rb_node *node)
  */
 static void bfq_extract(struct rb_root *root, struct bfq_entity *entity)
 {
+	BFQ_BUG_ON(entity->tree != root);
+
 	entity->tree = NULL;
 	rb_erase(&entity->rb_node, root);
 }
@@ -341,6 +404,8 @@ static void bfq_idle_extract(struct bfq_service_tree *st,
 	struct bfq_queue *bfqq = bfq_entity_to_bfqq(entity);
 	struct rb_node *next;
 
+	BFQ_BUG_ON(entity->tree != &st->idle);
+
 	if (entity == st->first_idle) {
 		next = rb_next(&entity->rb_node);
 		st->first_idle = bfq_entity_of(next);
@@ -371,6 +436,8 @@ static void bfq_insert(struct rb_root *root, struct bfq_entity *entity)
 	struct rb_node **node = &root->rb_node;
 	struct rb_node *parent = NULL;
 
+	BFQ_BUG_ON(entity->tree);
+
 	while (*node) {
 		parent = *node;
 		entry = rb_entry(parent, struct bfq_entity, rb_node);
@@ -419,10 +486,26 @@ static void bfq_update_min(struct bfq_entity *entity, struct rb_node *node)
 static void bfq_update_active_node(struct rb_node *node)
 {
 	struct bfq_entity *entity = rb_entry(node, struct bfq_entity, rb_node);
+	struct bfq_queue *bfqq = bfq_entity_to_bfqq(entity);
 
 	entity->min_start = entity->start;
 	bfq_update_min(entity, node->rb_right);
 	bfq_update_min(entity, node->rb_left);
+
+	if (bfqq) {
+		bfq_log_bfqq(bfqq->bfqd, bfqq,
+			     "new min_start %llu",
+			     ((entity->min_start>>10)*1000)>>12);
+#ifdef CONFIG_BFQ_GROUP_IOSCHED
+	} else {
+		struct bfq_group *bfqg =
+			container_of(entity, struct bfq_group, entity);
+
+		bfq_log_bfqg((struct bfq_data *)bfqg->bfqd, bfqg,
+			     "new min_start %llu",
+			     ((entity->min_start>>10)*1000)>>12);
+#endif
+	}
 }
 
 /**
@@ -489,13 +572,17 @@ static void bfq_active_insert(struct bfq_service_tree *st,
 #ifdef CONFIG_BFQ_GROUP_IOSCHED
 	sd = entity->sched_data;
 	bfqg = container_of(sd, struct bfq_group, sched_data);
+	BFQ_BUG_ON(!bfqg);
 	bfqd = (struct bfq_data *)bfqg->bfqd;
 #endif
 	if (bfqq)
 		list_add(&bfqq->bfqq_list, &bfqq->bfqd->active_list);
 #ifdef CONFIG_BFQ_GROUP_IOSCHED
-	if (bfqg != bfqd->root_group)
+	if (bfqg != bfqd->root_group) {
+		BFQ_BUG_ON(!bfqg);
+		BFQ_BUG_ON(!bfqd);
 		bfqg->active_entities++;
+	}
 #endif
 }
 
@@ -505,6 +592,7 @@ static void bfq_active_insert(struct bfq_service_tree *st,
  */
 unsigned short bfq_ioprio_to_weight(int ioprio)
 {
+	BFQ_BUG_ON(ioprio < 0 || ioprio >= IOPRIO_NR_LEVELS);
 	return (IOPRIO_NR_LEVELS - ioprio) * BFQ_WEIGHT_CONVERSION_COEFF;
 }
 
@@ -518,6 +606,7 @@ unsigned short bfq_ioprio_to_weight(int ioprio)
  */
 static unsigned short bfq_weight_to_ioprio(int weight)
 {
+	BFQ_BUG_ON(weight < BFQ_MIN_WEIGHT || weight > BFQ_MAX_WEIGHT);
 	return max_t(int, 0,
 		     IOPRIO_NR_LEVELS - weight / BFQ_WEIGHT_CONVERSION_COEFF);
 }
@@ -528,7 +617,7 @@ static void bfq_get_entity(struct bfq_entity *entity)
 
 	if (bfqq) {
 		bfqq->ref++;
-		bfq_log_bfqq(bfqq->bfqd, bfqq, "get_entity: %p %d",
+		bfq_log_bfqq(bfqq->bfqd, bfqq, "%p %d",
 			     bfqq, bfqq->ref);
 	}
 }
@@ -588,13 +677,18 @@ static void bfq_active_extract(struct bfq_service_tree *st,
 #ifdef CONFIG_BFQ_GROUP_IOSCHED
 	sd = entity->sched_data;
 	bfqg = container_of(sd, struct bfq_group, sched_data);
+	BFQ_BUG_ON(!bfqg);
 	bfqd = (struct bfq_data *)bfqg->bfqd;
 #endif
 	if (bfqq)
 		list_del(&bfqq->bfqq_list);
 #ifdef CONFIG_BFQ_GROUP_IOSCHED
-	if (bfqg != bfqd->root_group)
+	if (bfqg != bfqd->root_group) {
+		BFQ_BUG_ON(!bfqg);
+		BFQ_BUG_ON(!bfqd);
+		BFQ_BUG_ON(!bfqg->active_entities);
 		bfqg->active_entities--;
+	}
 #endif
 }
 
@@ -641,11 +735,16 @@ static void bfq_forget_entity(struct bfq_service_tree *st,
 			      bool is_in_service)
 {
 	struct bfq_queue *bfqq = bfq_entity_to_bfqq(entity);
+	BFQ_BUG_ON(!entity->on_st_or_in_serv);
 
 	entity->on_st_or_in_serv = false;
 	st->wsum -= entity->weight;
-	if (bfqq && !is_in_service)
+
+	if (bfqq && !is_in_service) {
+		bfq_log_bfqq(bfqq->bfqd, bfqq, "(before): %p %d",
+			     bfqq, bfqq->ref);
 		bfq_put_queue(bfqq);
+	}
 }
 
 /**
@@ -688,8 +787,26 @@ static void bfq_forget_idle(struct bfq_service_tree *st)
 struct bfq_service_tree *bfq_entity_service_tree(struct bfq_entity *entity)
 {
 	struct bfq_sched_data *sched_data = entity->sched_data;
+	struct bfq_queue *bfqq = bfq_entity_to_bfqq(entity);
 	unsigned int idx = bfq_class_idx(entity);
 
+	BFQ_BUG_ON(idx >= BFQ_IOPRIO_CLASSES);
+	BFQ_BUG_ON(sched_data == NULL);
+
+	if (bfqq)
+		bfq_log_bfqq(bfqq->bfqd, bfqq,
+			     "%p %d",
+			     sched_data->service_tree + idx, idx);
+#ifdef CONFIG_BFQ_GROUP_IOSCHED
+	else {
+		struct bfq_group *bfqg =
+			container_of(entity, struct bfq_group, entity);
+
+		bfq_log_bfqg((struct bfq_data *)bfqg->bfqd, bfqg,
+			     "%p %d",
+			     sched_data->service_tree + idx, idx);
+	}
+#endif
 	return sched_data->service_tree + idx;
 }
 
@@ -734,12 +851,17 @@ __bfq_entity_update_weight_prio(struct bfq_service_tree *old_st,
 		else {
 			sd = entity->my_sched_data;
 			bfqg = container_of(sd, struct bfq_group, sched_data);
+			BFQ_BUG_ON(!bfqg);
 			bfqd = (struct bfq_data *)bfqg->bfqd;
+			BFQ_BUG_ON(!bfqd);
 		}
 #endif
 
 		/* Matches the smp_wmb() in bfq_group_set_weight. */
 		smp_rmb();
+
+		BFQ_BUG_ON(entity->tree && update_class_too);
+		BFQ_BUG_ON(old_st->wsum < entity->weight);
 		old_st->wsum -= entity->weight;
 
 		if (entity->new_weight != entity->orig_weight) {
@@ -801,8 +923,10 @@ __bfq_entity_update_weight_prio(struct bfq_service_tree *old_st,
 
 		new_st->wsum += entity->weight;
 
-		if (new_st != old_st)
+		if (new_st != old_st) {
+			BFQ_BUG_ON(!update_class_too);
 			entity->start = new_st->vtime;
+		}
 	}
 
 	return new_st;
@@ -826,8 +950,11 @@ void bfq_bfqq_served(struct bfq_queue *bfqq, int served)
 	if (!bfqq->service_from_backlogged)
 		bfqq->first_IO_time = jiffies;
 
-	if (bfqq->wr_coeff > 1)
+	if (bfqq->wr_coeff > 1) {
 		bfqq->service_from_wr += served;
+		bfq_log_bfqq(bfqq->bfqd, bfqq, "serv_from_wr: %lu",
+			     bfqq->service_from_wr);
+	}
 
 	bfqq->service_from_backlogged += served;
 	for_each_entity(entity) {
@@ -835,10 +962,14 @@ void bfq_bfqq_served(struct bfq_queue *bfqq, int served)
 
 		entity->service += served;
 
+		BFQ_BUG_ON(st->wsum == 0);
+
 		st->vtime += bfq_delta(served, st->wsum);
 		bfq_forget_idle(st);
 	}
-	bfq_log_bfqq(bfqq->bfqd, bfqq, "bfqq_served %d secs", served);
+	st = bfq_entity_service_tree(&bfqq->entity);
+	bfq_log_bfqq(bfqq->bfqd, bfqq, "bfqq_served %d secs, vtime %llu on %p",
+		     served,  ((st->vtime>>10)*1000)>>12, st);
 }
 
 /**
@@ -878,6 +1009,14 @@ void bfq_bfqq_charge_time(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 		(bfqd->bfq_max_budget * bounded_time_ms) / timeout_ms;
 	int tot_serv_to_charge = max(serv_to_charge_for_time, entity->service);
 
+	bfq_log_bfqq(bfqq->bfqd, bfqq,
+		     "%lu/%lu ms, %d/%d/%d/%d sectors",
+		     time_ms, timeout_ms,
+		     entity->service,
+		     tot_serv_to_charge,
+		     bfqd->bfq_max_budget,
+		     entity->budget);
+
 	/* Increase budget to avoid inconsistencies */
 	if (tot_serv_to_charge > entity->budget)
 		entity->budget = tot_serv_to_charge;
@@ -891,12 +1030,14 @@ static void bfq_update_fin_time_enqueue(struct bfq_entity *entity,
 					bool backshifted)
 {
 	struct bfq_queue *bfqq = bfq_entity_to_bfqq(entity);
+	struct bfq_sched_data *sd = entity->sched_data;
 
 	/*
 	 * When this function is invoked, entity is not in any service
 	 * tree, then it is safe to invoke next function with the last
 	 * parameter set (see the comments on the function).
 	 */
+	BFQ_BUG_ON(entity->tree);
 	st = __bfq_entity_update_weight_prio(st, entity, true);
 	bfq_calc_finish(entity, entity->budget);
 
@@ -941,9 +1082,43 @@ static void bfq_update_fin_time_enqueue(struct bfq_entity *entity,
 
 		entity->start += delta;
 		entity->finish += delta;
+
+		if (bfqq) {
+			bfq_log_bfqq(bfqq->bfqd, bfqq,
+				     "new queue finish %llu",
+				     ((entity->finish>>10)*1000)>>12);
+#ifdef CONFIG_BFQ_GROUP_IOSCHED
+		} else {
+			struct bfq_group *bfqg =
+				container_of(entity, struct bfq_group, entity);
+
+			bfq_log_bfqg((struct bfq_data *)bfqg->bfqd, bfqg,
+				     "new group finish %llu",
+				     ((entity->finish>>10)*1000)>>12);
+#endif
+		}
 	}
 
 	bfq_active_insert(st, entity);
+
+	if (bfqq) {
+		bfq_log_bfqq(bfqq->bfqd, bfqq,
+			"queue %seligible in st %p",
+			     entity->start <= st->vtime ? "" : "non ", st);
+#ifdef CONFIG_BFQ_GROUP_IOSCHED
+	} else {
+		struct bfq_group *bfqg =
+			container_of(entity, struct bfq_group, entity);
+
+		bfq_log_bfqg((struct bfq_data *)bfqg->bfqd, bfqg,
+			"group %seligible in st %p",
+			     entity->start <= st->vtime ? "" : "non ", st);
+#endif
+	}
+	BFQ_BUG_ON(RB_EMPTY_ROOT(&st->active));
+	BFQ_BUG_ON(&st->active != &sd->service_tree->active &&
+	       &st->active != &(sd->service_tree+1)->active &&
+	       &st->active != &(sd->service_tree+2)->active);
 }
 
 /**
@@ -961,10 +1136,15 @@ static void bfq_update_fin_time_enqueue(struct bfq_entity *entity,
 static void __bfq_activate_entity(struct bfq_entity *entity,
 				  bool non_blocking_wait_rq)
 {
+	struct bfq_sched_data *sd = entity->sched_data;
 	struct bfq_service_tree *st = bfq_entity_service_tree(entity);
+	struct bfq_queue *bfqq = bfq_entity_to_bfqq(entity);
 	bool backshifted = false;
 	unsigned long long min_vstart;
 
+	BFQ_BUG_ON(!sd);
+	BFQ_BUG_ON(!st);
+
 	/* See comments on bfq_fqq_update_budg_for_activation */
 	if (non_blocking_wait_rq && bfq_gt(st->vtime, entity->finish)) {
 		backshifted = true;
@@ -978,9 +1158,11 @@ static void __bfq_activate_entity(struct bfq_entity *entity,
 		 * check for that.
 		 */
 		bfq_idle_extract(st, entity);
+		BFQ_BUG_ON(entity->tree);
 		entity->start = bfq_gt(min_vstart, entity->finish) ?
 			min_vstart : entity->finish;
 	} else {
+		BFQ_BUG_ON(entity->tree);
 		/*
 		 * The finish time of the entity may be invalid, and
 		 * it is in the past for sure, otherwise the queue
@@ -996,6 +1178,21 @@ static void __bfq_activate_entity(struct bfq_entity *entity,
 		 */
 		bfq_get_entity(entity);
 
+		BFQ_BUG_ON(entity->on_st_or_in_serv && bfqq);
+
+#ifdef CONFIG_BFQ_GROUP_IOSCHED
+		if (entity->on_st_or_in_serv && !bfqq) {
+			struct bfq_group *bfqg =
+				container_of(entity, struct bfq_group,
+					     entity);
+
+			bfq_log_bfqg((struct bfq_data *)bfqg->bfqd,
+				     bfqg,
+				     "activate bug, class %d in_service %p",
+				     bfq_class_idx(entity), sd->in_service_entity);
+		}
+#endif
+		BFQ_BUG_ON(entity->on_st_or_in_serv && !bfqq);
 		entity->on_st_or_in_serv = true;
 	}
 
@@ -1005,10 +1202,13 @@ static void __bfq_activate_entity(struct bfq_entity *entity,
 			container_of(entity, struct bfq_group, entity);
 		struct bfq_data *bfqd = bfqg->bfqd;
 
+		BFQ_BUG_ON(!bfqd);
 		if (!entity->in_groups_with_pending_reqs) {
 			entity->in_groups_with_pending_reqs = true;
 			bfqd->num_groups_with_pending_reqs++;
 		}
+		bfq_log_bfqg(bfqd, bfqg, "num_groups_with_pending_reqs %u",
+			     bfqd->num_groups_with_pending_reqs);
 	}
 #endif
 
@@ -1035,6 +1235,12 @@ static void __bfq_requeue_entity(struct bfq_entity *entity)
 	struct bfq_sched_data *sd = entity->sched_data;
 	struct bfq_service_tree *st = bfq_entity_service_tree(entity);
 
+	BFQ_BUG_ON(!sd);
+	BFQ_BUG_ON(!st);
+
+	BFQ_BUG_ON(entity != sd->in_service_entity &&
+	       entity->tree != &st->active);
+
 	if (entity == sd->in_service_entity) {
 		/*
 		 * We are requeueing the current in-service entity,
@@ -1060,6 +1266,8 @@ static void __bfq_requeue_entity(struct bfq_entity *entity)
 		 */
 		bfq_calc_finish(entity, entity->service);
 		entity->start = entity->finish;
+		BFQ_BUG_ON(entity->tree && entity->tree == &st->idle);
+		BFQ_BUG_ON(entity->tree && entity->tree != &st->active);
 		/*
 		 * In addition, if the entity had more than one child
 		 * when set in service, then it was not extracted from
@@ -1137,12 +1345,20 @@ static void bfq_activate_requeue_entity(struct bfq_entity *entity,
 	struct bfq_sched_data *sd;
 
 	for_each_entity(entity) {
+		BFQ_BUG_ON(!entity);
 		sd = entity->sched_data;
 		__bfq_activate_requeue_entity(entity, sd, non_blocking_wait_rq);
 
+		BFQ_BUG_ON(RB_EMPTY_ROOT(&sd->service_tree->active) &&
+		       RB_EMPTY_ROOT(&(sd->service_tree+1)->active) &&
+		       RB_EMPTY_ROOT(&(sd->service_tree+2)->active));
+
 		if (!bfq_update_next_in_service(sd, entity, expiration) &&
-		    !requeue)
+		    !requeue) {
+			BFQ_BUG_ON(!sd->next_in_service);
 			break;
+		}
+		BFQ_BUG_ON(!sd->next_in_service);
 	}
 }
 
@@ -1162,11 +1378,11 @@ bool __bfq_deactivate_entity(struct bfq_entity *entity, bool ins_into_idle_tree)
 	struct bfq_service_tree *st;
 	bool is_in_service;
 
-	if (!entity->on_st_or_in_serv) /*
-					* entity never activated, or
-					* already inactive
-					*/
+	if (!entity->on_st_or_in_serv) {
+		/* entity never activated, or already inactive */
+		BFQ_BUG_ON(sd && entity == sd->in_service_entity);
 		return false;
+	}
 
 	/*
 	 * If we get here, then entity is active, which implies that
@@ -1177,6 +1393,8 @@ bool __bfq_deactivate_entity(struct bfq_entity *entity, bool ins_into_idle_tree)
 	st = bfq_entity_service_tree(entity);
 	is_in_service = entity == sd->in_service_entity;
 
+	BFQ_BUG_ON(is_in_service && entity->tree && entity->tree != &st->active);
+
 	bfq_calc_finish(entity, entity->service);
 
 	if (is_in_service)
@@ -1193,6 +1411,8 @@ bool __bfq_deactivate_entity(struct bfq_entity *entity, bool ins_into_idle_tree)
 		bfq_active_extract(st, entity);
 	else if (!is_in_service && entity->tree == &st->idle)
 		bfq_idle_extract(st, entity);
+	else if (entity->tree)
+		BUG();
 
 	if (!ins_into_idle_tree || !bfq_gt(entity->finish, st->vtime))
 		bfq_forget_entity(st, entity, is_in_service);
@@ -1219,6 +1439,18 @@ static void bfq_deactivate_entity(struct bfq_entity *entity,
 	for_each_entity_safe(entity, parent) {
 		sd = entity->sched_data;
 
+		BFQ_BUG_ON(sd == NULL); /*
+				     * It would mean that this is the
+				     * root group.
+				     */
+
+		BFQ_BUG_ON(expiration && entity != sd->in_service_entity);
+
+		BFQ_BUG_ON(entity != sd->in_service_entity &&
+		       entity->tree ==
+		       &bfq_entity_service_tree(entity)->active &&
+		       !sd->next_in_service);
+
 		if (!__bfq_deactivate_entity(entity, ins_into_idle_tree)) {
 			/*
 			 * entity is not in any tree any more, so
@@ -1227,6 +1459,10 @@ static void bfq_deactivate_entity(struct bfq_entity *entity,
 			 * (in case of expiration, this can never
 			 * happen).
 			 */
+			BFQ_BUG_ON(expiration); /*
+					     * entity cannot be already out of
+					     * any tree
+					     */
 			return;
 		}
 
@@ -1259,6 +1495,8 @@ static void bfq_deactivate_entity(struct bfq_entity *entity,
 			 * next_in_service. See the comments on the
 			 * definition of next_in_service for details.
 			 */
+			BFQ_BUG_ON(sd->next_in_service == entity);
+			BFQ_BUG_ON(sd->in_service_entity == entity);
 			break;
 		}
 
@@ -1286,6 +1524,8 @@ static void bfq_deactivate_entity(struct bfq_entity *entity,
 	 */
 	entity = parent;
 	for_each_entity(entity) {
+		struct bfq_queue *bfqq = bfq_entity_to_bfqq(entity);
+
 		/*
 		 * Invoke __bfq_requeue_entity on entity, even if
 		 * already active, to requeue/reposition it in the
@@ -1295,6 +1535,21 @@ static void bfq_deactivate_entity(struct bfq_entity *entity,
 		__bfq_requeue_entity(entity);
 
 		sd = entity->sched_data;
+		BFQ_BUG_ON(expiration && sd->in_service_entity != entity);
+
+		if (bfqq)
+			bfq_log_bfqq(bfqq->bfqd, bfqq,
+				     "invoking udpdate_next for this queue");
+#ifdef CONFIG_BFQ_GROUP_IOSCHED
+		else {
+			struct bfq_group *bfqg =
+				container_of(entity,
+					     struct bfq_group, entity);
+
+			bfq_log_bfqg((struct bfq_data *)bfqg->bfqd, bfqg,
+				     "invoking udpdate_next for this entity");
+		}
+#endif
 		if (!bfq_update_next_in_service(sd, entity, expiration) &&
 		    !expiration)
 			/*
@@ -1318,9 +1573,26 @@ static u64 bfq_calc_vtime_jump(struct bfq_service_tree *st)
 {
 	struct bfq_entity *root_entity = bfq_root_active_entity(&st->active);
 
-	if (bfq_gt(root_entity->min_start, st->vtime))
-		return root_entity->min_start;
+	if (bfq_gt(root_entity->min_start, st->vtime)) {
+		struct bfq_queue *bfqq = bfq_entity_to_bfqq(root_entity);
+
+		if (bfqq)
+			bfq_log_bfqq(bfqq->bfqd, bfqq,
+				     "new value %llu",
+				     ((root_entity->min_start>>10)*1000)>>12);
+#ifdef CONFIG_BFQ_GROUP_IOSCHED
+		else {
+			struct bfq_group *bfqg =
+				container_of(root_entity, struct bfq_group,
+					     entity);
 
+			bfq_log_bfqg((struct bfq_data *)bfqg->bfqd, bfqg,
+				     "new value %llu",
+				     ((root_entity->min_start>>10)*1000)>>12);
+		}
+#endif
+		return root_entity->min_start;
+	}
 	return st->vtime;
 }
 
@@ -1356,6 +1628,8 @@ static struct bfq_entity *bfq_first_active_entity(struct bfq_service_tree *st,
 		if (!bfq_gt(entry->start, vtime))
 			first = entry;
 
+		BFQ_BUG_ON(bfq_gt(entry->min_start, vtime));
+
 		if (node->rb_left) {
 			entry = rb_entry(node->rb_left,
 					 struct bfq_entity, rb_node);
@@ -1369,6 +1643,7 @@ static struct bfq_entity *bfq_first_active_entity(struct bfq_service_tree *st,
 		node = node->rb_right;
 	}
 
+	BFQ_BUG_ON(!first && !RB_EMPTY_ROOT(&st->active));
 	return first;
 }
 
@@ -1399,6 +1674,7 @@ __bfq_lookup_next_entity(struct bfq_service_tree *st, bool in_service)
 {
 	struct bfq_entity *entity;
 	u64 new_vtime;
+	struct bfq_queue *bfqq;
 
 	if (RB_EMPTY_ROOT(&st->active))
 		return NULL;
@@ -1423,6 +1699,29 @@ __bfq_lookup_next_entity(struct bfq_service_tree *st, bool in_service)
 		bfq_update_vtime(st, new_vtime);
 
 	entity = bfq_first_active_entity(st, new_vtime);
+	BFQ_BUG_ON(bfq_gt(entity->start, new_vtime));
+
+	/* Log some information */
+	bfqq = bfq_entity_to_bfqq(entity);
+	if (bfqq)
+		bfq_log_bfqq(bfqq->bfqd, bfqq,
+			     "start %llu vtime %llu st %p",
+			     ((entity->start>>10)*1000)>>12,
+			     ((new_vtime>>10)*1000)>>12, st);
+#ifdef CONFIG_BFQ_GROUP_IOSCHED
+	else {
+		struct bfq_group *bfqg =
+			container_of(entity, struct bfq_group, entity);
+
+		bfq_log_bfqg((struct bfq_data *)bfqg->bfqd, bfqg,
+			     "start %llu vtime %llu (%llu) st %p",
+			     ((entity->start>>10)*1000)>>12,
+			     ((st->vtime>>10)*1000)>>12,
+			     ((new_vtime>>10)*1000)>>12, st);
+	}
+#endif
+
+	BFQ_BUG_ON(!entity);
 
 	return entity;
 }
@@ -1442,8 +1741,11 @@ static struct bfq_entity *bfq_lookup_next_entity(struct bfq_sched_data *sd,
 	struct bfq_service_tree *st = sd->service_tree;
 	struct bfq_service_tree *idle_class_st = st + (BFQ_IOPRIO_CLASSES - 1);
 	struct bfq_entity *entity = NULL;
+	struct bfq_queue *bfqq;
 	int class_idx = 0;
 
+	BFQ_BUG_ON(!sd);
+	BFQ_BUG_ON(!st);
 	/*
 	 * Choose from idle class, if needed to guarantee a minimum
 	 * bandwidth to this class (and if there is some active entity
@@ -1487,9 +1789,29 @@ static struct bfq_entity *bfq_lookup_next_entity(struct bfq_sched_data *sd,
 			break;
 	}
 
+	BFQ_BUG_ON(!entity &&
+	       (!RB_EMPTY_ROOT(&st->active) || !RB_EMPTY_ROOT(&(st+1)->active) ||
+		!RB_EMPTY_ROOT(&(st+2)->active)));
+
 	if (!entity)
 		return NULL;
 
+	/* Log some information */
+	bfqq = bfq_entity_to_bfqq(entity);
+	if (bfqq)
+		bfq_log_bfqq(bfqq->bfqd, bfqq, "chosen from st %p %d",
+			     st + class_idx, class_idx);
+#ifdef CONFIG_BFQ_GROUP_IOSCHED
+	else {
+		struct bfq_group *bfqg =
+			container_of(entity, struct bfq_group, entity);
+
+		bfq_log_bfqg((struct bfq_data *)bfqg->bfqd, bfqg,
+			     "chosen from st %p %d",
+			     st + class_idx, class_idx);
+	}
+#endif
+
 	return entity;
 }
 
@@ -1509,6 +1831,8 @@ struct bfq_queue *bfq_get_next_queue(struct bfq_data *bfqd)
 	struct bfq_sched_data *sd;
 	struct bfq_queue *bfqq;
 
+	BFQ_BUG_ON(bfqd->in_service_queue);
+
 	if (bfq_tot_busy_queues(bfqd) == 0)
 		return NULL;
 
@@ -1519,6 +1843,25 @@ struct bfq_queue *bfq_get_next_queue(struct bfq_data *bfqd)
 	 */
 	sd = &bfqd->root_group->sched_data;
 	for (; sd ; sd = entity->my_sched_data) {
+#ifdef CONFIG_BFQ_GROUP_IOSCHED
+		if (entity) {
+			struct bfq_group *bfqg =
+				container_of(entity, struct bfq_group, entity);
+
+			bfq_log_bfqg(bfqd, bfqg,
+				     "lookup in this group");
+			if (!sd->next_in_service)
+				pr_crit("lookup in this group");
+		} else {
+			bfq_log_bfqg(bfqd, bfqd->root_group,
+				     "lookup in root group");
+			if (!sd->next_in_service)
+				pr_crit("lookup in root group");
+		}
+#endif
+
+		BFQ_BUG_ON(!sd->next_in_service);
+
 		/*
 		 * WARNING. We are about to set the in-service entity
 		 * to sd->next_in_service, i.e., to the (cached) value
@@ -1577,9 +1920,29 @@ struct bfq_queue *bfq_get_next_queue(struct bfq_data *bfqd)
 		 * whether the parent entity of the leaf entity
 		 * becomes the next-to-serve, and so on.
 		 */
+
+		/* Log some information */
+		bfqq = bfq_entity_to_bfqq(entity);
+		if (bfqq)
+			bfq_log_bfqq(bfqd, bfqq,
+			     "this queue, finish %llu",
+				(((entity->finish>>10)*1000)>>10)>>2);
+#ifdef CONFIG_BFQ_GROUP_IOSCHED
+		else {
+			struct bfq_group *bfqg =
+				container_of(entity, struct bfq_group, entity);
+
+			bfq_log_bfqg(bfqd, bfqg,
+			     "this entity, finish %llu",
+				(((entity->finish>>10)*1000)>>10)>>2);
+		}
+#endif
+
 	}
 
+	BFQ_BUG_ON(!entity);
 	bfqq = bfq_entity_to_bfqq(entity);
+	BFQ_BUG_ON(!bfqq);
 
 	/*
 	 * We can finally update all next-to-serve entities along the
@@ -1647,6 +2010,11 @@ void bfq_deactivate_bfqq(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 void bfq_activate_bfqq(struct bfq_data *bfqd, struct bfq_queue *bfqq)
 {
 	struct bfq_entity *entity = &bfqq->entity;
+	struct bfq_service_tree *st = bfq_entity_service_tree(entity);
+
+	BFQ_BUG_ON(bfqq == bfqd->in_service_queue);
+	BFQ_BUG_ON(entity->tree != &st->active && entity->tree != &st->idle &&
+	       entity->on_st_or_in_serv);
 
 	bfq_activate_requeue_entity(entity, bfq_bfqq_non_blocking_wait_rq(bfqq),
 				    false, false);
@@ -1670,17 +2038,24 @@ void bfq_requeue_bfqq(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 void bfq_del_bfqq_busy(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 		       bool expiration)
 {
+	BFQ_BUG_ON(!bfq_bfqq_busy(bfqq));
+	BFQ_BUG_ON(!RB_EMPTY_ROOT(&bfqq->sort_list));
+
 	bfq_log_bfqq(bfqd, bfqq, "del from busy");
 
 	bfq_clear_bfqq_busy(bfqq);
 
+	BFQ_BUG_ON(bfq_tot_busy_queues(bfqd) == 0);
 	bfqd->busy_queues[bfqq->ioprio_class - 1]--;
 
-	if (bfqq->wr_coeff > 1)
+	if (bfqq->wr_coeff > 1) {
 		bfqd->wr_busy_queues--;
+		BFQ_BUG_ON(bfqd->wr_busy_queues < 0);
+	}
 
 	bfqg_stats_update_dequeue(bfqq_group(bfqq));
 
+	BFQ_BUG_ON(bfqq->entity.budget < 0);
 	bfq_deactivate_bfqq(bfqd, bfqq, true, expiration);
 
 	if (!bfqq->dispatched)
@@ -1692,6 +2067,9 @@ void bfq_del_bfqq_busy(struct bfq_data *bfqd, struct bfq_queue *bfqq,
  */
 void bfq_add_bfqq_busy(struct bfq_data *bfqd, struct bfq_queue *bfqq)
 {
+	BFQ_BUG_ON(bfq_bfqq_busy(bfqq));
+	BFQ_BUG_ON(bfqq == bfqd->in_service_queue);
+
 	bfq_log_bfqq(bfqd, bfqq, "add to busy");
 
 	bfq_activate_bfqq(bfqd, bfqq);
@@ -1704,8 +2082,12 @@ void bfq_add_bfqq_busy(struct bfq_data *bfqd, struct bfq_queue *bfqq)
 			bfq_weights_tree_add(bfqd, bfqq,
 					     &bfqd->queue_weights_tree);
 
-	if (bfqq->wr_coeff > 1)
+	if (bfqq->wr_coeff > 1) {
 		bfqd->wr_busy_queues++;
+		BFQ_BUG_ON(bfqd->wr_busy_queues > bfq_tot_busy_queues(bfqd));
+	}
+
+	BFQ_BUG_ON(!hlist_unhashed(&bfqq->woken_list_node) && !bfqq->waker_bfqq);
 
 	/* Move bfqq to the head of the woken list of its waker */
 	if (!hlist_unhashed(&bfqq->woken_list_node) &&
diff --git a/include/linux/blk-mq.h b/include/linux/blk-mq.h
index d319ffa59..5f5620944 100644
--- a/include/linux/blk-mq.h
+++ b/include/linux/blk-mq.h
@@ -57,6 +57,10 @@ typedef __u32 __bitwise req_flags_t;
 #define RQF_TIMED_OUT		((__force req_flags_t)(1 << 21))
 /* queue has elevator attached */
 #define RQF_ELV			((__force req_flags_t)(1 << 22))
+/* DEBUG: rq in bfq-dev dispatch list */
+#define RQF_DISP_LIST   ((__force req_flags_t)(1 << 23))
+/* DEBUG: rq had get_rq_private executed on it */
+#define RQF_GOT ((__force req_flags_t)(1 << 24))
 
 /* flags that prevent us from merging requests: */
 #define RQF_NOMERGE_FLAGS \
@@ -788,7 +792,7 @@ static inline int blk_mq_request_completed(struct request *rq)
 }
 
 /*
- * 
+ *
  * Set the state to complete when completing a request from inside ->queue_rq.
  * This is used by drivers that want to ensure special complete actions that
  * need access to the request are called on failure, e.g. by nvme for
diff --git a/include/linux/sched.h b/include/linux/sched.h
index e806326ec..b2c4223d3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1498,6 +1498,8 @@ struct task_struct {
 	 */
 	randomized_struct_fields_end
 
+	struct hlist_node task_list_node;
+
 	/* CPU-specific state of this task: */
 	struct thread_struct		thread;
 
-- 
2.35.1.677.gabf474a5dd


From fe73958f3920e3db889ea60e5b3c6fd6bcf19a68 Mon Sep 17 00:00:00 2001
From: Piotr Gorski <lucjan.lucjanov@gmail.com>
Date: Tue, 22 Mar 2022 17:29:34 +0100
Subject: [PATCH 02/12] elevator: set default scheduler to bfq for blk-mq

Signed-off-by: Piotr Gorski <lucjan.lucjanov@gmail.com>
---
 block/elevator.c | 10 ++++++++--
 1 file changed, 8 insertions(+), 2 deletions(-)

diff --git a/block/elevator.c b/block/elevator.c
index 482df2a35..e3b239880 100644
--- a/block/elevator.c
+++ b/block/elevator.c
@@ -630,19 +630,25 @@ static inline bool elv_support_iosched(struct request_queue *q)
 }
 
 /*
- * For single queue devices, default to using mq-deadline. If we have multiple
- * queues or mq-deadline is not available, default to "none".
+ * Always use bfq as our elevator if possible. If bfq is unavailable default
+ * to mq-deadline for single-queue and none for multi-queue devices.
  */
 static struct elevator_type *elevator_get_default(struct request_queue *q)
 {
 	if (q->tag_set && q->tag_set->flags & BLK_MQ_F_NO_SCHED_BY_DEFAULT)
 		return NULL;
 
+#ifndef CONFIG_IOSCHED_BFQ
 	if (q->nr_hw_queues != 1 &&
 	    !blk_mq_is_shared_tags(q->tag_set->flags))
 		return NULL;
+#endif
 
+#if defined(CONFIG_IOSCHED_BFQ)
+	return elevator_get(q, "bfq", false);
+#else
 	return elevator_get(q, "mq-deadline", false);
+#endif
 }
 
 /*
-- 
2.35.1.677.gabf474a5dd


From 2f247f976523d837df6538f5c001906c7ebde9be Mon Sep 17 00:00:00 2001
From: Jan Kara <jack () suse ! cz>
Date: Thu, 7 Apr 2022 14:07:38 +0000
Subject: [PATCH 03/12] bfq: Fix warning in bfqq_request_over_limit()

People are occasionally reporting a warning bfqq_request_over_limit()
triggering reporting that BFQ's idea of cgroup hierarchy (and its depth)
does not match what generic blkcg code thinks. This can actually happen
when bfqq gets moved between BFQ groups while bfqq_request_over_limit()
is running. Make sure the code is safe against BFQ queue being moved to
a different BFQ group.

Fixes: 76f1df88bbc2 ("bfq: Limit number of requests consumed by each cgroup")
CC: stable@vger.kernel.org
Link: https://lore.kernel.org/all/CAJCQCtTw_2C7ZSz7as5Gvq=OmnDiio=HRkQekqWpKot84sQhFA@mail.gmail.com/
Reported-by: Chris Murphy <lists@colorremedies.com>
Reported-by: "yukuai (C)" <yukuai3@huawei.com>
Signed-off-by: Jan Kara <jack@suse.cz>
---
 block/bfq-iosched.c | 12 +++++++++---
 1 file changed, 9 insertions(+), 3 deletions(-)

diff --git a/block/bfq-iosched.c b/block/bfq-iosched.c
index dace517fb..461b17770 100644
--- a/block/bfq-iosched.c
+++ b/block/bfq-iosched.c
@@ -580,7 +580,7 @@ static bool bfqq_request_over_limit(struct bfq_queue *bfqq, int limit)
 	struct bfq_entity *entity = &bfqq->entity;
 	struct bfq_entity *inline_entities[BFQ_LIMIT_INLINE_DEPTH];
 	struct bfq_entity **entities = inline_entities;
-	int depth, level;
+	int depth, level, alloc_depth = BFQ_LIMIT_INLINE_DEPTH;
 	int class_idx = bfqq->ioprio_class - 1;
 	struct bfq_sched_data *sched_data;
 	unsigned long wsum;
@@ -589,15 +589,21 @@ static bool bfqq_request_over_limit(struct bfq_queue *bfqq, int limit)
 	if (!entity->on_st_or_in_serv)
 		return false;
 
+retry:
+	spin_lock_irq(&bfqd->lock);
 	/* +1 for bfqq entity, root cgroup not included */
 	depth = bfqg_to_blkg(bfqq_group(bfqq))->blkcg->css.cgroup->level + 1;
-	if (depth > BFQ_LIMIT_INLINE_DEPTH) {
+	if (depth > alloc_depth) {
+		spin_unlock_irq(&bfqd->lock);
+		if (entities != inline_entities)
+			kfree(entities);
 		entities = kmalloc_array(depth, sizeof(*entities), GFP_NOIO);
 		if (!entities)
 			return false;
+		alloc_depth = depth;
+		goto retry;
 	}
 
-	spin_lock_irq(&bfqd->lock);
 	sched_data = entity->sched_data;
 	/* Gather our ancestors as we need to traverse them in reverse order */
 	level = 0;
-- 
2.35.1.677.gabf474a5dd


From 7648b02f6fa157311db979d6efcf5c2da3faf030 Mon Sep 17 00:00:00 2001
From: Jan Kara <jack@suse.cz>
Date: Wed, 30 Mar 2022 14:42:44 +0200
Subject: [PATCH 04/12] bfq: Avoid false marking of bic as stably merged

bfq_setup_cooperator() can mark bic as stably merged even though it
decides to not merge its bfqqs (when bfq_setup_merge() returns NULL).
Make sure to mark bic as stably merged only if we are really going to
merge bfqqs.

CC: stable@vger.kernel.org
Fixes: 430a67f9d616 ("block, bfq: merge bursts of newly-created queues")
Signed-off-by: Jan Kara <jack@suse.cz>
---
 block/bfq-iosched.c | 9 ++++++---
 1 file changed, 6 insertions(+), 3 deletions(-)

diff --git a/block/bfq-iosched.c b/block/bfq-iosched.c
index 461b17770..d5fe6836a 100644
--- a/block/bfq-iosched.c
+++ b/block/bfq-iosched.c
@@ -3247,9 +3247,12 @@ bfq_setup_cooperator(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 						     new_bfqq);
 				}
 
-				bic->stably_merged = true;
-				if (new_bfqq && new_bfqq->bic)
-					new_bfqq->bic->stably_merged = true;
+				if (new_bfqq) {
+					bic->stably_merged = true;
+					if (new_bfqq->bic)
+						new_bfqq->bic->stably_merged =
+									true;
+				}
 				return new_bfqq;
 			} else {
 				bfq_log_bfqq(bfqd, bfqq,
-- 
2.35.1.677.gabf474a5dd


From 9b16eb27f72ba10fb0607af50f3bf5759ab7f25f Mon Sep 17 00:00:00 2001
From: Jan Kara <jack@suse.cz>
Date: Wed, 30 Mar 2022 14:42:45 +0200
Subject: [PATCH 05/12] bfq: Avoid merging queues with different parents

It can happen that the parent of a bfqq changes between the moment we
decide two queues are worth to merge (and set bic->stable_merge_bfqq)
and the moment bfq_setup_merge() is called. This can happen e.g. because
the process submitted IO for a different cgroup and thus bfqq got
reparented. It can even happen that the bfqq we are merging with has
parent cgroup that is already offline and going to be destroyed in which
case the merge can lead to use-after-free issues such as:

BUG: KASAN: use-after-free in __bfq_deactivate_entity+0x9cb/0xa50
Read of size 8 at addr ffff88800693c0c0 by task runc:[2:INIT]/10544

CPU: 0 PID: 10544 Comm: runc:[2:INIT] Tainted: G            E     5.15.2-0.g5fb85fd-default #1 openSUSE Tumbleweed (unreleased) f1f3b891c72369aebecd2e43e4641a6358867c70
Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.14.0-0-g155821a-rebuilt.opensuse.org 04/01/2014
Call Trace:
 <IRQ>
 dump_stack_lvl+0x46/0x5a
 print_address_description.constprop.0+0x1f/0x140
 ? __bfq_deactivate_entity+0x9cb/0xa50
 kasan_report.cold+0x7f/0x11b
 ? __bfq_deactivate_entity+0x9cb/0xa50
 __bfq_deactivate_entity+0x9cb/0xa50
 ? update_curr+0x32f/0x5d0
 bfq_deactivate_entity+0xa0/0x1d0
 bfq_del_bfqq_busy+0x28a/0x420
 ? resched_curr+0x116/0x1d0
 ? bfq_requeue_bfqq+0x70/0x70
 ? check_preempt_wakeup+0x52b/0xbc0
 __bfq_bfqq_expire+0x1a2/0x270
 bfq_bfqq_expire+0xd16/0x2160
 ? try_to_wake_up+0x4ee/0x1260
 ? bfq_end_wr_async_queues+0xe0/0xe0
 ? _raw_write_unlock_bh+0x60/0x60
 ? _raw_spin_lock_irq+0x81/0xe0
 bfq_idle_slice_timer+0x109/0x280
 ? bfq_dispatch_request+0x4870/0x4870
 __hrtimer_run_queues+0x37d/0x700
 ? enqueue_hrtimer+0x1b0/0x1b0
 ? kvm_clock_get_cycles+0xd/0x10
 ? ktime_get_update_offsets_now+0x6f/0x280
 hrtimer_interrupt+0x2c8/0x740

Fix the problem by checking that the parent of the two bfqqs we are
merging in bfq_setup_merge() is the same.

Link: https://lore.kernel.org/linux-block/20211125172809.GC19572@quack2.suse.cz/
CC: stable@vger.kernel.org
Fixes: 430a67f9d616 ("block, bfq: merge bursts of newly-created queues")
Signed-off-by: Jan Kara <jack@suse.cz>
---
 block/bfq-iosched.c | 8 ++++++++
 1 file changed, 8 insertions(+)

diff --git a/block/bfq-iosched.c b/block/bfq-iosched.c
index d5fe6836a..d401a0831 100644
--- a/block/bfq-iosched.c
+++ b/block/bfq-iosched.c
@@ -3085,6 +3085,14 @@ bfq_setup_merge(struct bfq_queue *bfqq, struct bfq_queue *new_bfqq)
 	if (process_refs == 0 || new_process_refs == 0)
 		return NULL;
 
+	/*
+	 * Make sure merged queues belong to the same parent. Parents could
+	 * have changed since the time we decided the two queues are suitable
+	 * for merging.
+	 */
+	if (new_bfqq->entity.parent != bfqq->entity.parent)
+		return NULL;
+
 	bfq_log_bfqq(bfqq->bfqd, bfqq, "scheduling merge with queue %d",
 		bfq_get_first_task_pid(new_bfqq));
 
-- 
2.35.1.677.gabf474a5dd


From b1073ffcab43fc214ad665c0052597b0c1167139 Mon Sep 17 00:00:00 2001
From: Jan Kara <jack@suse.cz>
Date: Wed, 30 Mar 2022 14:42:46 +0200
Subject: [PATCH 06/12] bfq: Split shared queues on move between cgroups

When bfqq is shared by multiple processes it can happen that one of the
processes gets moved to a different cgroup (or just starts submitting IO
for different cgroup). In case that happens we need to split the merged
bfqq as otherwise we will have IO for multiple cgroups in one bfqq and
we will just account IO time to wrong entities etc.

Similarly if the bfqq is scheduled to merge with another bfqq but the
merge didn't happen yet, cancel the merge as it need not be valid
anymore.

CC: stable@vger.kernel.org
Fixes: e21b7a0b9887 ("block, bfq: add full hierarchical scheduling and cgroups support")
Signed-off-by: Jan Kara <jack@suse.cz>
---
 block/bfq-cgroup.c  | 36 +++++++++++++++++++++++++++++++++---
 block/bfq-iosched.c |  2 +-
 block/bfq-iosched.h |  1 +
 3 files changed, 35 insertions(+), 4 deletions(-)

diff --git a/block/bfq-cgroup.c b/block/bfq-cgroup.c
index 92a7a78c8..f4ef98d64 100644
--- a/block/bfq-cgroup.c
+++ b/block/bfq-cgroup.c
@@ -778,9 +778,39 @@ static struct bfq_group *__bfq_bic_change_cgroup(struct bfq_data *bfqd,
 	}
 
 	if (sync_bfqq) {
-		entity = &sync_bfqq->entity;
-		if (entity->sched_data != &bfqg->sched_data)
-			bfq_bfqq_move(bfqd, sync_bfqq, bfqg);
+		if (!sync_bfqq->new_bfqq && !bfq_bfqq_coop(sync_bfqq)) {
+			/* We are the only user of this bfqq, just move it */
+			if (sync_bfqq->entity.sched_data != &bfqg->sched_data)
+				bfq_bfqq_move(bfqd, sync_bfqq, bfqg);
+		} else {
+			struct bfq_queue *bfqq;
+
+			/*
+			 * The queue was merged to a different queue. Check
+			 * that the merge chain still belongs to the same
+			 * cgroup.
+			 */
+			for (bfqq = sync_bfqq; bfqq; bfqq = bfqq->new_bfqq)
+				if (bfqq->entity.sched_data !=
+				    &bfqg->sched_data)
+					break;
+			if (bfqq) {
+				/*
+				 * Some queue changed cgroup so the merge is
+				 * not valid anymore. We cannot easily just
+				 * cancel the merge (by clearing new_bfqq) as
+				 * there may be other processes using this
+				 * queue and holding refs to all queues below
+				 * sync_bfqq->new_bfqq. Similarly if the merge
+				 * already happened, we need to detach from
+				 * bfqq now so that we cannot merge bio to a
+				 * request from the old cgroup.
+				 */
+				bfq_put_cooperator(sync_bfqq);
+				bfq_release_process_ref(bfqd, sync_bfqq);
+				bic_set_bfqq(bic, NULL, 1);
+			}
+		}
 	}
 
 	return bfqg;
diff --git a/block/bfq-iosched.c b/block/bfq-iosched.c
index d401a0831..668de9d0b 100644
--- a/block/bfq-iosched.c
+++ b/block/bfq-iosched.c
@@ -6045,7 +6045,7 @@ void bfq_put_queue(struct bfq_queue *bfqq)
 	__bfq_put_queue(bfqq);
 }
 
-static void bfq_put_cooperator(struct bfq_queue *bfqq)
+void bfq_put_cooperator(struct bfq_queue *bfqq)
 {
 	struct bfq_queue *__bfqq, *next;
 
diff --git a/block/bfq-iosched.h b/block/bfq-iosched.h
index 48e113f2c..ddd6fbeb8 100644
--- a/block/bfq-iosched.h
+++ b/block/bfq-iosched.h
@@ -987,6 +987,7 @@ void bfq_weights_tree_remove(struct bfq_data *bfqd,
 void bfq_bfqq_expire(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 		     bool compensate, enum bfqq_expiration reason);
 void bfq_put_queue(struct bfq_queue *bfqq);
+void bfq_put_cooperator(struct bfq_queue *bfqq);
 void bfq_end_wr_async_queues(struct bfq_data *bfqd, struct bfq_group *bfqg);
 void bfq_release_process_ref(struct bfq_data *bfqd, struct bfq_queue *bfqq);
 void bfq_schedule_dispatch(struct bfq_data *bfqd);
-- 
2.35.1.677.gabf474a5dd


From 0504329cb7aee07d7aa726f932dc8121cb99a596 Mon Sep 17 00:00:00 2001
From: Jan Kara <jack@suse.cz>
Date: Wed, 30 Mar 2022 14:42:47 +0200
Subject: [PATCH 07/12] bfq: Update cgroup information before merging bio

When the process is migrated to a different cgroup (or in case of
writeback just starts submitting bios associated with a different
cgroup) bfq_merge_bio() can operate with stale cgroup information in
bic. Thus the bio can be merged to a request from a different cgroup or
it can result in merging of bfqqs for different cgroups or bfqqs of
already dead cgroups and causing possible use-after-free issues. Fix the
problem by updating cgroup information in bfq_merge_bio().

CC: stable@vger.kernel.org
Fixes: e21b7a0b9887 ("block, bfq: add full hierarchical scheduling and cgroups support")
Signed-off-by: Jan Kara <jack@suse.cz>
---
 block/bfq-iosched.c | 11 +++++++++--
 1 file changed, 9 insertions(+), 2 deletions(-)

diff --git a/block/bfq-iosched.c b/block/bfq-iosched.c
index 668de9d0b..ece7f965e 100644
--- a/block/bfq-iosched.c
+++ b/block/bfq-iosched.c
@@ -2736,10 +2736,17 @@ static bool bfq_bio_merge(struct request_queue *q, struct bio *bio,
 
 	spin_lock_irq(&bfqd->lock);
 
-	if (bic)
+	if (bic) {
+		/*
+		 * Make sure cgroup info is uptodate for current process before
+		 * considering the merge.
+		 */
+		bfq_bic_update_cgroup(bic, bio);
+
 		bfqd->bio_bfqq = bic_to_bfqq(bic, op_is_sync(bio->bi_opf));
-	else
+	} else {
 		bfqd->bio_bfqq = NULL;
+	}
 	bfqd->bio_bic = bic;
 	/* Set next flag just for testing purposes */
 	bfqd->bio_bfqq_set = true;
-- 
2.35.1.677.gabf474a5dd


From 0b529b23179f7cb20e5598e703d8cd7a1412d049 Mon Sep 17 00:00:00 2001
From: Jan Kara <jack@suse.cz>
Date: Wed, 30 Mar 2022 14:42:48 +0200
Subject: [PATCH 08/12] bfq: Drop pointless unlock-lock pair

In bfq_insert_request() we unlock bfqd->lock only to call
trace_block_rq_insert() and then lock bfqd->lock again. This is really
pointless since tracing is disabled if we really care about performance
and even if the tracepoint is enabled, it is a quick call.

Signed-off-by: Jan Kara <jack@suse.cz>
---
 block/bfq-iosched.c | 3 ---
 1 file changed, 3 deletions(-)

diff --git a/block/bfq-iosched.c b/block/bfq-iosched.c
index ece7f965e..ad51f8e75 100644
--- a/block/bfq-iosched.c
+++ b/block/bfq-iosched.c
@@ -6999,11 +6999,8 @@ static void bfq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 		return;
 	}
 
-	spin_unlock_irq(&bfqd->lock);
-
 	trace_block_rq_insert(rq);
 
-	spin_lock_irq(&bfqd->lock);
 	// XXX next function also takes a lock on bfqq
 	bfqq = bfq_init_rq(rq);
 
-- 
2.35.1.677.gabf474a5dd


From a2141de394607c92f717b992f479107611d6b7fa Mon Sep 17 00:00:00 2001
From: Jan Kara <jack@suse.cz>
Date: Wed, 30 Mar 2022 14:42:49 +0200
Subject: [PATCH 09/12] bfq: Remove pointless bfq_init_rq() calls

We call bfq_init_rq() from request merging functions where requests we
get should have already gone through bfq_init_rq() during insert and
anyway we want to do anything only if the request is already tracked by
BFQ. So replace calls to bfq_init_rq() with RQ_BFQQ() instead to simply
skip requests untracked by BFQ. We move bfq_init_rq() call in
bfq_insert_request() a bit earlier to cover request merging and thus
can transfer FIFO position in case of a merge.

Signed-off-by: Jan Kara <jack@suse.cz>
---
 block/bfq-iosched.c | 15 +++++++--------
 1 file changed, 7 insertions(+), 8 deletions(-)

diff --git a/block/bfq-iosched.c b/block/bfq-iosched.c
index ad51f8e75..4af634138 100644
--- a/block/bfq-iosched.c
+++ b/block/bfq-iosched.c
@@ -2788,8 +2788,6 @@ static int bfq_request_merge(struct request_queue *q, struct request **req,
 	return ELEVATOR_NO_MERGE;
 }
 
-static struct bfq_queue *bfq_init_rq(struct request *rq);
-
 static void bfq_request_merged(struct request_queue *q, struct request *req,
 			       enum elv_merge type)
 {
@@ -2800,7 +2798,7 @@ static void bfq_request_merged(struct request_queue *q, struct request *req,
 	    blk_rq_pos(req) <
 	    blk_rq_pos(container_of(rb_prev(&req->rb_node),
 				    struct request, rb_node))) {
-		struct bfq_queue *bfqq = bfq_init_rq(req);
+		struct bfq_queue *bfqq = RQ_BFQQ(req);
 		struct bfq_data *bfqd;
 		struct request *prev, *next_rq;
 
@@ -2861,8 +2859,8 @@ static void bfq_request_merged(struct request_queue *q, struct request *req,
 static void bfq_requests_merged(struct request_queue *q, struct request *rq,
 				struct request *next)
 {
-	struct bfq_queue *bfqq = bfq_init_rq(rq),
-		*next_bfqq = bfq_init_rq(next);
+	struct bfq_queue *bfqq = RQ_BFQQ(rq),
+		*next_bfqq = RQ_BFQQ(next);
 
 	if (!bfqq)
 		goto remove;
@@ -6976,6 +6974,8 @@ static inline void bfq_update_insert_stats(struct request_queue *q,
 					   unsigned int cmd_flags) {}
 #endif /* CONFIG_BFQ_CGROUP_DEBUG */
 
+static struct bfq_queue *bfq_init_rq(struct request *rq);
+
 static void bfq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 			       bool at_head)
 {
@@ -6991,6 +6991,8 @@ static void bfq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 		bfqg_stats_update_legacy_io(q, rq);
 #endif
 	spin_lock_irq(&bfqd->lock);
+	// XXX next function also takes a lock on bfqq
+	bfqq = bfq_init_rq(rq);
 	BFQ_BUG_ON(bfq_tot_busy_queues(bfqd) <= 0 && bfqd->queued > 0);
 
 	if (blk_mq_sched_try_insert_merge(q, rq, &free)) {
@@ -7001,9 +7003,6 @@ static void bfq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 
 	trace_block_rq_insert(rq);
 
-	// XXX next function also takes a lock on bfqq
-	bfqq = bfq_init_rq(rq);
-
 	BFQ_BUG_ON(!bfqq && !(at_head || blk_rq_is_passthrough(rq)));
 	BFQ_BUG_ON(bfqq && bic_to_bfqq(RQ_BIC(rq), rq_is_sync(rq)) != bfqq);
 	BFQ_BUG_ON(bfqq->entity.weight == 0);
-- 
2.35.1.677.gabf474a5dd


From 4c43a0e6775f8ccfbeda193883067ae0f9d628ea Mon Sep 17 00:00:00 2001
From: Jan Kara <jack@suse.cz>
Date: Wed, 30 Mar 2022 14:42:50 +0200
Subject: [PATCH 10/12] bfq: Track whether bfq_group is still online

Track whether bfq_group is still online. We cannot rely on
blkcg_gq->online because that gets cleared only after all policies are
offlined and we need something that gets updated already under
bfqd->lock when we are cleaning up our bfq_group to be able to guarantee
that when we see online bfq_group, it will stay online while we are
holding bfqd->lock lock.

Signed-off-by: Jan Kara <jack@suse.cz>
---
 block/bfq-cgroup.c  | 3 ++-
 block/bfq-iosched.h | 2 ++
 2 files changed, 4 insertions(+), 1 deletion(-)

diff --git a/block/bfq-cgroup.c b/block/bfq-cgroup.c
index f4ef98d64..c73769bdf 100644
--- a/block/bfq-cgroup.c
+++ b/block/bfq-cgroup.c
@@ -573,6 +573,7 @@ static void bfq_pd_init(struct blkg_policy_data *pd)
 				   */
 	bfqg->bfqd = bfqd;
 	bfqg->active_entities = 0;
+	bfqg->online = true;
 	bfqg->rq_pos_tree = RB_ROOT;
 }
 
@@ -623,7 +624,6 @@ struct bfq_group *bfq_find_set_group(struct bfq_data *bfqd,
 	struct bfq_entity *entity;
 
 	bfqg = bfq_lookup_bfqg(bfqd, blkcg);
-
 	if (unlikely(!bfqg))
 		return NULL;
 
@@ -1029,6 +1029,7 @@ static void bfq_pd_offline(struct blkg_policy_data *pd)
 
 put_async_queues:
 	bfq_put_async_queues(bfqd, bfqg);
+	bfqg->online = false;
 
 	spin_unlock_irqrestore(&bfqd->lock, flags);
 	/*
diff --git a/block/bfq-iosched.h b/block/bfq-iosched.h
index ddd6fbeb8..2c03a9605 100644
--- a/block/bfq-iosched.h
+++ b/block/bfq-iosched.h
@@ -936,6 +936,8 @@ struct bfq_group {
 
 	/* reference counter (see comments in bfq_bic_update_cgroup) */
 	int ref;
+	/* Is bfq_group still online? */
+	bool online;
 
 	struct bfq_entity entity;
 	struct bfq_sched_data sched_data;
-- 
2.35.1.677.gabf474a5dd


From b4c2be4b255027317128bfc0f6133000e6eb17c4 Mon Sep 17 00:00:00 2001
From: Jan Kara <jack@suse.cz>
Date: Wed, 30 Mar 2022 14:42:51 +0200
Subject: [PATCH 11/12] bfq: Get rid of __bio_blkcg() usage

BFQ usage of __bio_blkcg() is a relict from the past. Furthermore if bio
would not be associated with any blkcg, the usage of __bio_blkcg() in
BFQ is prone to races with the task being migrated between cgroups as
__bio_blkcg() calls at different places could return different blkcgs.

Convert BFQ to the new situation where bio->bi_blkg is initialized in
bio_set_dev() and thus practically always valid. This allows us to save
blkcg_gq lookup and noticeably simplify the code.

Signed-off-by: Jan Kara <jack@suse.cz>
---
 block/bfq-cgroup.c  | 63 +++++++++++++++++----------------------------
 block/bfq-iosched.c | 10 +------
 block/bfq-iosched.h |  3 +--
 3 files changed, 25 insertions(+), 51 deletions(-)

diff --git a/block/bfq-cgroup.c b/block/bfq-cgroup.c
index c73769bdf..e5fcd26aa 100644
--- a/block/bfq-cgroup.c
+++ b/block/bfq-cgroup.c
@@ -606,27 +606,11 @@ static void bfq_group_set_parent(struct bfq_group *bfqg,
 	entity->sched_data = &parent->sched_data;
 }
 
-static struct bfq_group *bfq_lookup_bfqg(struct bfq_data *bfqd,
-					 struct blkcg *blkcg)
+static void bfq_link_bfqg(struct bfq_data *bfqd, struct bfq_group *bfqg)
 {
-	struct blkcg_gq *blkg;
-
-	blkg = blkg_lookup(blkcg, bfqd->queue);
-	if (likely(blkg))
-		return blkg_to_bfqg(blkg);
-	return NULL;
-}
-
-struct bfq_group *bfq_find_set_group(struct bfq_data *bfqd,
-				     struct blkcg *blkcg)
-{
-	struct bfq_group *bfqg, *parent;
+	struct bfq_group *parent;
 	struct bfq_entity *entity;
 
-	bfqg = bfq_lookup_bfqg(bfqd, blkcg);
-	if (unlikely(!bfqg))
-		return NULL;
-
 	/*
 	 * Update chain of bfq_groups as we might be handling a leaf group
 	 * which, along with some of its relatives, has not been hooked yet
@@ -645,8 +629,15 @@ struct bfq_group *bfq_find_set_group(struct bfq_data *bfqd,
 			bfq_group_set_parent(curr_bfqg, parent);
 		}
 	}
+}
 
-	return bfqg;
+struct bfq_group *bfq_bio_bfqg(struct bfq_data *bfqd, struct bio *bio)
+{
+	struct blkcg_gq *blkg = bio->bi_blkg;
+
+	if (!blkg)
+		return bfqd->root_group;
+	return blkg_to_bfqg(blkg);
 }
 
 /**
@@ -745,25 +736,15 @@ void bfq_bfqq_move(struct bfq_data *bfqd, struct bfq_queue *bfqq,
  * Move bic to blkcg, assuming that bfqd->lock is held; which makes
  * sure that the reference to cgroup is valid across the call (see
  * comments in bfq_bic_update_cgroup on this issue)
- *
- * NOTE: an alternative approach might have been to store the current
- * cgroup in bfqq and getting a reference to it, reducing the lookup
- * time here, at the price of slightly more complex code.
  */
-static struct bfq_group *__bfq_bic_change_cgroup(struct bfq_data *bfqd,
-						struct bfq_io_cq *bic,
-						struct blkcg *blkcg)
+static void *__bfq_bic_change_cgroup(struct bfq_data *bfqd,
+				     struct bfq_io_cq *bic,
+				     struct bfq_group *bfqg)
 {
 	struct bfq_queue *async_bfqq = bic_to_bfqq(bic, 0);
 	struct bfq_queue *sync_bfqq = bic_to_bfqq(bic, 1);
-	struct bfq_group *bfqg;
 	struct bfq_entity *entity;
 
-	bfqg = bfq_find_set_group(bfqd, blkcg);
-
-	if (unlikely(!bfqg))
-		bfqg = bfqd->root_group;
-
 	if (async_bfqq) {
 		entity = &async_bfqq->entity;
 
@@ -819,20 +800,24 @@ static struct bfq_group *__bfq_bic_change_cgroup(struct bfq_data *bfqd,
 void bfq_bic_update_cgroup(struct bfq_io_cq *bic, struct bio *bio)
 {
 	struct bfq_data *bfqd = bic_to_bfqd(bic);
-	struct bfq_group *bfqg = NULL;
+	struct bfq_group *bfqg = bfq_bio_bfqg(bfqd, bio);
 	uint64_t serial_nr;
 
-	rcu_read_lock();
-	serial_nr = __bio_blkcg(bio)->css.serial_nr;
+	serial_nr = bfqg_to_blkg(bfqg)->blkcg->css.serial_nr;
 
 	/*
 	 * Check whether blkcg has changed.  The condition may trigger
 	 * spuriously on a newly created cic but there's no harm.
 	 */
 	if (unlikely(!bfqd) || likely(bic->blkcg_serial_nr == serial_nr))
-		goto out;
+		return;
 
-	bfqg = __bfq_bic_change_cgroup(bfqd, bic, __bio_blkcg(bio));
+	/*
+	 * New cgroup for this process. Make sure it is linked to bfq internal
+	 * cgroup hierarchy.
+	 */
+	bfq_link_bfqg(bfqd, bfqg);
+	__bfq_bic_change_cgroup(bfqd, bic, bfqg);
 	/*
 	 * Update blkg_path for bfq_log_* functions. We cache this
 	 * path, and update it here, for the following
@@ -885,8 +870,6 @@ void bfq_bic_update_cgroup(struct bfq_io_cq *bic, struct bio *bio)
 	 */
 	blkg_path(bfqg_to_blkg(bfqg), bfqg->blkg_path, sizeof(bfqg->blkg_path));
 	bic->blkcg_serial_nr = serial_nr;
-out:
-	rcu_read_unlock();
 }
 
 /**
@@ -1522,7 +1505,7 @@ void bfq_end_wr_async(struct bfq_data *bfqd)
 	bfq_end_wr_async_queues(bfqd, bfqd->root_group);
 }
 
-struct bfq_group *bfq_find_set_group(struct bfq_data *bfqd, struct blkcg *blkcg)
+struct bfq_group *bfq_bio_bfqg(struct bfq_data *bfqd, struct bio *bio)
 {
 	return bfqd->root_group;
 }
diff --git a/block/bfq-iosched.c b/block/bfq-iosched.c
index 4af634138..1d28a482a 100644
--- a/block/bfq-iosched.c
+++ b/block/bfq-iosched.c
@@ -6522,14 +6522,7 @@ static struct bfq_queue *bfq_get_queue(struct bfq_data *bfqd,
 	struct bfq_queue *bfqq;
 	struct bfq_group *bfqg;
 
-	rcu_read_lock();
-
-	bfqg = bfq_find_set_group(bfqd, __bio_blkcg(bio));
-	if (!bfqg) {
-		bfqq = &bfqd->oom_bfqq;
-		goto out;
-	}
-
+	bfqg = bfq_bio_bfqg(bfqd, bio);
 	if (!is_sync) {
 		async_bfqq = bfq_async_queue_prio(bfqd, bfqg, ioprio_class,
 						  ioprio);
@@ -6590,7 +6583,6 @@ static struct bfq_queue *bfq_get_queue(struct bfq_data *bfqd,
 		bfqq = bfq_do_or_sched_stable_merge(bfqd, bfqq, bic);
 	}
 
-	rcu_read_unlock();
 	return bfqq;
 }
 
diff --git a/block/bfq-iosched.h b/block/bfq-iosched.h
index 2c03a9605..cb6f338e3 100644
--- a/block/bfq-iosched.h
+++ b/block/bfq-iosched.h
@@ -1018,8 +1018,7 @@ void bfq_bfqq_move(struct bfq_data *bfqd, struct bfq_queue *bfqq,
 void bfq_init_entity(struct bfq_entity *entity, struct bfq_group *bfqg);
 void bfq_bic_update_cgroup(struct bfq_io_cq *bic, struct bio *bio);
 void bfq_end_wr_async(struct bfq_data *bfqd);
-struct bfq_group *bfq_find_set_group(struct bfq_data *bfqd,
-				     struct blkcg *blkcg);
+struct bfq_group *bfq_bio_bfqg(struct bfq_data *bfqd, struct bio *bio);
 struct blkcg_gq *bfqg_to_blkg(struct bfq_group *bfqg);
 struct bfq_group *bfqq_group(struct bfq_queue *bfqq);
 struct bfq_group *bfq_create_group_hierarchy(struct bfq_data *bfqd, int node);
-- 
2.35.1.677.gabf474a5dd


From a05c86548b48e8698c679425d74b24c1200f2a72 Mon Sep 17 00:00:00 2001
From: Jan Kara <jack@suse.cz>
Date: Wed, 30 Mar 2022 14:42:52 +0200
Subject: [PATCH 12/12] bfq: Make sure bfqg for which we are queueing requests
 is online

Bios queued into BFQ IO scheduler can be associated with a cgroup that
was already offlined. This may then cause insertion of this bfq_group
into a service tree. But this bfq_group will get freed as soon as last
bio associated with it is completed leading to use after free issues for
service tree users. Fix the problem by making sure we always operate on
online bfq_group. If the bfq_group associated with the bio is not
online, we pick the first online parent.

CC: stable@vger.kernel.org
Fixes: e21b7a0b9887 ("block, bfq: add full hierarchical scheduling and cgroups support")
Signed-off-by: Jan Kara <jack@suse.cz>
---
 block/bfq-cgroup.c | 15 ++++++++++++---
 1 file changed, 12 insertions(+), 3 deletions(-)

diff --git a/block/bfq-cgroup.c b/block/bfq-cgroup.c
index e5fcd26aa..8e3a2dc68 100644
--- a/block/bfq-cgroup.c
+++ b/block/bfq-cgroup.c
@@ -634,10 +634,19 @@ static void bfq_link_bfqg(struct bfq_data *bfqd, struct bfq_group *bfqg)
 struct bfq_group *bfq_bio_bfqg(struct bfq_data *bfqd, struct bio *bio)
 {
 	struct blkcg_gq *blkg = bio->bi_blkg;
+	struct bfq_group *bfqg;
 
-	if (!blkg)
-		return bfqd->root_group;
-	return blkg_to_bfqg(blkg);
+	while (blkg) {
+		bfqg = blkg_to_bfqg(blkg);
+		if (bfqg->online) {
+			bio_associate_blkg_from_css(bio, &blkg->blkcg->css);
+			return bfqg;
+		}
+		blkg = blkg->parent;
+	}
+	bio_associate_blkg_from_css(bio,
+				&bfqg_to_blkg(bfqd->root_group)->blkcg->css);
+	return bfqd->root_group;
 }
 
 /**
-- 
2.35.1.677.gabf474a5dd

