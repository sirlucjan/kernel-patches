From 97369e7bcde52b98b1df6470c1128acaa897b9a9 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Fri, 25 Jun 2021 07:24:08 +0000
Subject: [PATCH 176/265] sched/alt: Optimization and code clean-up

---
 kernel/sched/alt_core.c | 20 +++++++-------------
 kernel/sched/pds.h      |  2 +-
 2 files changed, 8 insertions(+), 14 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 57c34cf29956..a8ba783b07ff 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -77,7 +77,7 @@ __read_mostly int sysctl_resched_latency_warn_once = 1;
 #define STOP_PRIO		(MAX_RT_PRIO - 1)
 
 /* Default time slice is 4 in ms, can be set via kernel parameter "sched_timeslice" */
-u64 sched_timeslice_ns __read_mostly = (2 << 20);
+u64 sched_timeslice_ns __read_mostly = (4 << 20);
 
 static inline void requeue_task(struct task_struct *p, struct rq *rq);
 
@@ -193,9 +193,8 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 			cpumask_andnot(&sched_rq_watermark[i],
 				       &sched_rq_watermark[i], cpumask_of(cpu));
 #ifdef CONFIG_SCHED_SMT
-		if (!static_branch_likely(&sched_smt_present))
-			return;
-		if (IDLE_WM == last_wm)
+		if (static_branch_likely(&sched_smt_present) &&
+		    IDLE_WM == last_wm)
 			cpumask_andnot(&sched_sg_idle_mask,
 				       &sched_sg_idle_mask, cpu_smt_mask(cpu));
 #endif
@@ -205,10 +204,9 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 	for (i = last_wm + 1; i <= watermark; i++)
 		cpumask_set_cpu(cpu, &sched_rq_watermark[i]);
 #ifdef CONFIG_SCHED_SMT
-	if (!static_branch_likely(&sched_smt_present))
-		return;
-	if (IDLE_WM == watermark) {
+	if (static_branch_likely(&sched_smt_present) && IDLE_WM == watermark) {
 		cpumask_t tmp;
+
 		cpumask_and(&tmp, cpu_smt_mask(cpu), &sched_rq_watermark[IDLE_WM]);
 		if (cpumask_equal(&tmp, cpu_smt_mask(cpu)))
 			cpumask_or(&sched_sg_idle_mask, cpu_smt_mask(cpu),
@@ -1003,13 +1001,10 @@ static void hrtick_clear(struct rq *rq)
 static enum hrtimer_restart hrtick(struct hrtimer *timer)
 {
 	struct rq *rq = container_of(timer, struct rq, hrtick_timer);
-	struct task_struct *p;
 
 	WARN_ON_ONCE(cpu_of(rq) != smp_processor_id());
 
 	raw_spin_lock(&rq->lock);
-	p = rq->curr;
-	p->time_slice = 0;
 	resched_curr(rq);
 	raw_spin_unlock(&rq->lock);
 
@@ -2733,9 +2728,7 @@ void wake_up_new_task(struct task_struct *p)
 	struct rq *rq;
 
 	raw_spin_lock_irqsave(&p->pi_lock, flags);
-
 	p->state = TASK_RUNNING;
-
 	rq = cpu_rq(select_task_rq(p));
 #ifdef CONFIG_SMP
 	rseq_migrate(p);
@@ -2743,6 +2736,7 @@ void wake_up_new_task(struct task_struct *p)
 	 * Fork balancing, do it here and not earlier because:
 	 * - cpus_ptr can change in the fork path
 	 * - any previously selected CPU might disappear through hotplug
+	 *
 	 * Use __set_task_cpu() to avoid calling sched_class::migrate_task_rq,
 	 * as we're not fully set-up yet.
 	 */
@@ -2750,8 +2744,8 @@ void wake_up_new_task(struct task_struct *p)
 #endif
 
 	raw_spin_lock(&rq->lock);
-
 	update_rq_clock(rq);
+
 	activate_task(p, rq);
 	trace_sched_wakeup_new(p);
 	check_preempt_curr(rq);
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index c23294178c2b..06d88e72b543 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -63,7 +63,7 @@ static inline void sched_renew_deadline(struct task_struct *p, const struct rq *
 
 int task_running_nice(struct task_struct *p)
 {
-	return task_sched_prio(p) > DEFAULT_PRIO;
+	return (p->prio > DEFAULT_PRIO);
 }
 
 static inline void update_rq_time_edge(struct rq *rq)
-- 
2.35.1.677.gabf474a5dd

