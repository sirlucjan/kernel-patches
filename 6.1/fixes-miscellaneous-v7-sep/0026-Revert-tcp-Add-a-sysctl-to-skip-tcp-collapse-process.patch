From 1a6f819bcbbf825104342802921ece061d06ad2c Mon Sep 17 00:00:00 2001
From: Piotr Gorski <lucjan.lucjanov@gmail.com>
Date: Thu, 22 Dec 2022 13:13:04 +0100
Subject: [PATCH 26/26] Revert "tcp: Add a sysctl to skip tcp collapse
 processing when the receive buffer is full"

This reverts commit 76b83991ef0a9dc04dbde903a99affc8d6eaee9b.

Signed-off-by: Piotr Gorski <lucjan.lucjanov@gmail.com>
---
 include/net/netns/ipv4.h   |  1 -
 include/trace/events/tcp.h |  7 -------
 net/ipv4/sysctl_net_ipv4.c |  7 -------
 net/ipv4/tcp_input.c       | 36 ------------------------------------
 net/ipv4/tcp_ipv4.c        |  2 --
 5 files changed, 53 deletions(-)

diff --git a/include/net/netns/ipv4.h b/include/net/netns/ipv4.h
index d6d7f9942..1b8004679 100644
--- a/include/net/netns/ipv4.h
+++ b/include/net/netns/ipv4.h
@@ -188,7 +188,6 @@ struct netns_ipv4 {
 	int sysctl_udp_rmem_min;
 
 	u8 sysctl_fib_notify_on_flag_change;
-	unsigned int sysctl_tcp_collapse_max_bytes;
 
 #ifdef CONFIG_NET_L3_MASTER_DEV
 	u8 sysctl_udp_l3mdev_accept;
diff --git a/include/trace/events/tcp.h b/include/trace/events/tcp.h
index 7026df84a..901b44023 100644
--- a/include/trace/events/tcp.h
+++ b/include/trace/events/tcp.h
@@ -187,13 +187,6 @@ DEFINE_EVENT(tcp_event_sk, tcp_rcv_space_adjust,
 	TP_ARGS(sk)
 );
 
-DEFINE_EVENT(tcp_event_sk, tcp_collapse_max_bytes_exceeded,
-
-	TP_PROTO(struct sock *sk),
-
-	TP_ARGS(sk)
-);
-
 TRACE_EVENT(tcp_retransmit_synack,
 
 	TP_PROTO(const struct sock *sk, const struct request_sock *req),
diff --git a/net/ipv4/sysctl_net_ipv4.c b/net/ipv4/sysctl_net_ipv4.c
index 0e364b98c..9b8a6db7a 100644
--- a/net/ipv4/sysctl_net_ipv4.c
+++ b/net/ipv4/sysctl_net_ipv4.c
@@ -1384,13 +1384,6 @@ static struct ctl_table ipv4_net_table[] = {
 		.extra1		= SYSCTL_ZERO,
 		.extra2		= SYSCTL_TWO,
 	},
-	{
-		.procname	= "tcp_collapse_max_bytes",
-		.data		= &init_net.ipv4.sysctl_tcp_collapse_max_bytes,
-		.maxlen		= sizeof(unsigned int),
-		.mode		= 0644,
-		.proc_handler	= proc_douintvec_minmax,
-	},
 	{ }
 };
 
diff --git a/net/ipv4/tcp_input.c b/net/ipv4/tcp_input.c
index f2b70b05d..0640453fc 100644
--- a/net/ipv4/tcp_input.c
+++ b/net/ipv4/tcp_input.c
@@ -5384,7 +5384,6 @@ static bool tcp_prune_ofo_queue(struct sock *sk)
 static int tcp_prune_queue(struct sock *sk)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
-	struct net *net = sock_net(sk);
 
 	NET_INC_STATS(sock_net(sk), LINUX_MIB_PRUNECALLED);
 
@@ -5396,39 +5395,6 @@ static int tcp_prune_queue(struct sock *sk)
 	if (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf)
 		return 0;
 
-	/* For context and additional information about this patch, see the
-	 * blog post at
-	 *
-	 * sysctl:  net.ipv4.tcp_collapse_max_bytes
-	 *
-	 * If tcp_collapse_max_bytes is non-zero, attempt to collapse the
-	 * queue to free up memory if the current amount of memory allocated
-	 * is less than tcp_collapse_max_bytes.  Otherwise, the packet is
-	 * dropped without attempting to collapse the queue.
-	 *
-	 * If tcp_collapse_max_bytes is zero, this feature is disabled
-	 * and the default Linux behavior is used.  The default Linux
-	 * behavior is to always perform the attempt to collapse the
-	 * queue to free up memory.
-	 *
-	 * When the receive queue is small, we want to collapse the
-	 * queue.  There are two reasons for this: (a) the latency of
-	 * performing the collapse will be small on a small queue, and
-	 * (b) we want to avoid sending a congestion signal (via a
-	 * packet drop) to the sender when the receive queue is small.
-	 *
-	 * The result is that we avoid latency spikes caused by the
-	 * time it takes to perform the collapse logic when the receive
-	 * queue is large and full, while preserving existing behavior
-	 * and performance for all other cases.
-	 */
-	if (net->ipv4.sysctl_tcp_collapse_max_bytes &&
-		(atomic_read(&sk->sk_rmem_alloc) > net->ipv4.sysctl_tcp_collapse_max_bytes)) {
-		/* We are dropping the packet */
-		trace_tcp_collapse_max_bytes_exceeded(sk);
-		goto do_not_collapse;
-	}
-
 	tcp_collapse_ofo_queue(sk);
 	if (!skb_queue_empty(&sk->sk_receive_queue))
 		tcp_collapse(sk, &sk->sk_receive_queue, NULL,
@@ -5447,8 +5413,6 @@ static int tcp_prune_queue(struct sock *sk)
 	if (atomic_read(&sk->sk_rmem_alloc) <= sk->sk_rcvbuf)
 		return 0;
 
-do_not_collapse:
-
 	/* If we are really being abused, tell the caller to silently
 	 * drop receive data on the floor.  It will get retransmitted
 	 * and hopefully then we'll have sufficient space.
diff --git a/net/ipv4/tcp_ipv4.c b/net/ipv4/tcp_ipv4.c
index 77967b2db..da46357f5 100644
--- a/net/ipv4/tcp_ipv4.c
+++ b/net/ipv4/tcp_ipv4.c
@@ -3215,8 +3215,6 @@ static int __net_init tcp_sk_init(struct net *net)
 	else
 		net->ipv4.tcp_congestion_control = &tcp_reno;
 
-	net->ipv4.sysctl_tcp_collapse_max_bytes = 0;
-
 	return 0;
 }
 
-- 
2.39.0.rc2.1.gbd5df96b79

