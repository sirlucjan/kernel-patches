From d6413625ee6b13e454b08f7afc57e0da1c93d18b Mon Sep 17 00:00:00 2001
From: Suren Baghdasaryan <surenb@google.com>
Date: Sun, 4 Dec 2022 15:47:03 -0800
Subject: [PATCH 60/61] mm: separate vma->lock from vm_area_struct

vma->lock being part of the vm_area_struct causes performance regression
during page faults because during contention its count and owner fields
are constantly updated and having other parts of vm_area_struct used
during page fault handling next to them causes constant cache line
bouncing. Fix that by moving the lock outside of the vm_area_struct.
All attempts to keep vma->lock inside vm_area_struct in a separate
cache line still produce performance regression especially on NUMA
machines. Smallest regression was achieved when lock is placed in the
fourth cache line but that bloats vm_area_struct to 256 bytes:

slabinfo before the changes:
 <name>            ... <objsize> <objperslab> <pagesperslab> : ...
vm_area_struct    ...    152   53    2 : ...

slabinfo with lock in the fourth cache line:
 <name>            ... <objsize> <objperslab> <pagesperslab> : ...
vm_area_struct    ...    216   37    2 : ...

slabinfo with lock separated:
 <name>            ... <objsize> <objperslab> <pagesperslab> : ...
rw_semaphore      ...     40  102    1 : ...
vm_area_struct    ...    168   48    2 : ...

Assuming 40000 vm_area_structs, memory consumption would be:
baseline: 6040kB
lock at the fourth cache line: 8656kB
separate lock (vm_area_structs+rw_semaphores): 6672kB+1572kB=8244kB

Signed-off-by: Suren Baghdasaryan <surenb@google.com>
---
 include/linux/mm.h       | 23 ++++++++++-------------
 include/linux/mm_types.h |  2 +-
 kernel/fork.c            | 19 +++++++++++++++++++
 3 files changed, 30 insertions(+), 14 deletions(-)

diff --git a/include/linux/mm.h b/include/linux/mm.h
index e5e7ec769..3f99f44bb 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -614,11 +614,8 @@ struct vm_operations_struct {
 };
 
 #ifdef CONFIG_PER_VMA_LOCK
-static inline void vma_init_lock(struct vm_area_struct *vma)
-{
-	init_rwsem(&vma->lock);
-	vma->vm_lock_seq = -1;
-}
+
+void vma_init_lock(struct vm_area_struct *vma);
 
 static inline void vma_write_lock(struct vm_area_struct *vma)
 {
@@ -634,9 +631,9 @@ static inline void vma_write_lock(struct vm_area_struct *vma)
 	if (vma->vm_lock_seq == mm_lock_seq)
 		return;
 
-	down_write(&vma->lock);
+	down_write(vma->lock);
 	vma->vm_lock_seq = mm_lock_seq;
-	up_write(&vma->lock);
+	up_write(vma->lock);
 }
 
 /*
@@ -650,7 +647,7 @@ static inline bool vma_read_trylock(struct vm_area_struct *vma)
 	if (vma->vm_lock_seq == READ_ONCE(vma->vm_mm->mm_lock_seq))
 		return false;
 
-	if (unlikely(down_read_trylock(&vma->lock) == 0))
+	if (unlikely(down_read_trylock(vma->lock) == 0))
 		return false;
 
 	/*
@@ -660,7 +657,7 @@ static inline bool vma_read_trylock(struct vm_area_struct *vma)
 	 * modification invalidates all existing locks.
 	 */
 	if (unlikely(vma->vm_lock_seq == READ_ONCE(vma->vm_mm->mm_lock_seq))) {
-		up_read(&vma->lock);
+		up_read(vma->lock);
 		return false;
 	}
 	return true;
@@ -668,13 +665,13 @@ static inline bool vma_read_trylock(struct vm_area_struct *vma)
 
 static inline void vma_read_unlock(struct vm_area_struct *vma)
 {
-	up_read(&vma->lock);
+	up_read(vma->lock);
 }
 
 static inline void vma_assert_locked(struct vm_area_struct *vma)
 {
-	lockdep_assert_held(&vma->lock);
-	VM_BUG_ON_VMA(!rwsem_is_locked(&vma->lock), vma);
+	lockdep_assert_held(vma->lock);
+	VM_BUG_ON_VMA(!rwsem_is_locked(vma->lock), vma);
 }
 
 static inline void vma_assert_write_locked(struct vm_area_struct *vma)
@@ -689,7 +686,7 @@ static inline void vma_assert_write_locked(struct vm_area_struct *vma)
 
 static inline void vma_assert_no_reader(struct vm_area_struct *vma)
 {
-	VM_BUG_ON_VMA(rwsem_is_locked(&vma->lock) &&
+	VM_BUG_ON_VMA(rwsem_is_locked(vma->lock) &&
 		      vma->vm_lock_seq != READ_ONCE(vma->vm_mm->mm_lock_seq),
 		      vma);
 }
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 965b86286..0c2bbede2 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -480,7 +480,7 @@ struct vm_area_struct {
 
 #ifdef CONFIG_PER_VMA_LOCK
 	int vm_lock_seq;
-	struct rw_semaphore lock;
+	struct rw_semaphore *lock;
 #endif
 
 	union {
diff --git a/kernel/fork.c b/kernel/fork.c
index 99029c161..770293599 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -481,10 +481,26 @@ struct vm_area_struct *vm_area_dup(struct vm_area_struct *orig)
 }
 
 #ifdef CONFIG_PER_VMA_LOCK
+
+static struct kmem_cache *vma_lock_cachep;
+
+static void __init vma_lock_cache_init(void)
+{
+	vma_lock_cachep = KMEM_CACHE(rw_semaphore, SLAB_PANIC|SLAB_ACCOUNT);
+}
+
+void vma_init_lock(struct vm_area_struct *vma)
+{
+	vma->lock = kmem_cache_alloc(vma_lock_cachep, GFP_KERNEL);
+	init_rwsem(vma->lock);
+	vma->vm_lock_seq = -1;
+}
+
 static inline void __vm_area_free(struct vm_area_struct *vma)
 {
 	/* The vma should either have no lock holders or be write-locked. */
 	vma_assert_no_reader(vma);
+	kmem_cache_free(vma_lock_cachep, vma->lock);
 	kmem_cache_free(vm_area_cachep, vma);
 }
 
@@ -540,6 +556,8 @@ void vm_area_free(struct vm_area_struct *vma)
 
 #else /* CONFIG_PER_VMA_LOCK */
 
+static void vma_lock_cache_init(void) {}
+
 void drain_free_vmas(struct mm_struct *mm) {}
 
 void vm_area_free(struct vm_area_struct *vma)
@@ -3129,6 +3147,7 @@ void __init proc_caches_init(void)
 			sizeof_field(struct mm_struct, saved_auxv),
 			NULL);
 	vm_area_cachep = KMEM_CACHE(vm_area_struct, SLAB_PANIC|SLAB_ACCOUNT);
+	vma_lock_cache_init();
 	mmap_init();
 	nsproxy_cache_init();
 }
-- 
2.39.0.rc2.1.gbd5df96b79

