From 3598256f528523474463f104d356b6a3bc03cc39 Mon Sep 17 00:00:00 2001
From: Peter Jung <admin@ptr1337.dev>
Date: Fri, 13 Jan 2023 19:33:58 +0100
Subject: [PATCH 24/26] Revert "RFC! sched/numa: Process Adaptive autoNUMA"

This reverts commit 4ca083022a4566c86334f260905f5b577d639b4a.
---
 include/linux/mm_types.h |  14 --
 kernel/sched/debug.c     |   2 -
 kernel/sched/fair.c      | 344 +--------------------------------------
 kernel/sched/sched.h     |   2 -
 4 files changed, 4 insertions(+), 358 deletions(-)

diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 9824b1b36..500e53679 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -665,20 +665,6 @@ struct mm_struct {
 
 		/* numa_scan_seq prevents two threads remapping PTEs. */
 		int numa_scan_seq;
-
-		/* Process-based Adaptive NUMA */
-		atomic_long_t faults_locality[2];
-		atomic_long_t faults_shared[2];
-		unsigned long faults_locality_history[2];
-		unsigned long faults_shared_history[2];
-
-		spinlock_t pan_numa_lock;
-		unsigned int numa_scan_period;
-		int remote_fault_rates[2]; /* histogram of remote fault rate */
-		long scanned_pages;
-		bool trend;
-		int slope;
-		u8 hist_trend;
 #endif
 		/*
 		 * An operation with batched TLB flushing is going on. Anything
diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index a8377d0e5..1637b65ba 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -334,8 +334,6 @@ static __init int sched_init_debug(void)
 	debugfs_create_u32("scan_period_max_ms", 0644, numa, &sysctl_numa_balancing_scan_period_max);
 	debugfs_create_u32("scan_size_mb", 0644, numa, &sysctl_numa_balancing_scan_size);
 	debugfs_create_u32("hot_threshold_ms", 0644, numa, &sysctl_numa_balancing_hot_threshold);
-	debugfs_create_u32("pan_scan_period_min", 0644, numa, &sysctl_pan_scan_period_min);
-	debugfs_create_u32("pan_scan_period_max", 0644, numa, &sysctl_pan_scan_period_max);
 #endif
 
 	debugfs_create_file("debug", 0444, debugfs_sched, NULL, &sched_debug_fops);
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 7028287cd..472594172 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -1097,10 +1097,6 @@ unsigned int sysctl_numa_balancing_hot_threshold = MSEC_PER_SEC;
 /* Restrict the NUMA promotion throughput (MB/s) for each target node. */
 unsigned int sysctl_numa_balancing_promote_rate_limit = 65536;
 
-/* Clips of max and min scanning periods */
-unsigned int sysctl_pan_scan_period_min = 50;
-unsigned int sysctl_pan_scan_period_max = 5000;
-
 struct numa_group {
 	refcount_t refcount;
 
@@ -2334,301 +2330,6 @@ static void numa_group_count_active_nodes(struct numa_group *numa_group)
 	numa_group->active_nodes = active_nodes;
 }
 
-/**********************************************/
-/*  Process-based Adaptive NUMA (PAN) Design  */
-/**********************************************/
-#define SLOPE(N, D) ((N)/(D))
-
-static unsigned int pan_scan_max(struct task_struct *p)
-{
-	unsigned long smax, nr_scan_pages;
-	unsigned long rss = 0;
-
-	smax = sysctl_pan_scan_period_max;
-	nr_scan_pages = sysctl_numa_balancing_scan_size << (20 - PAGE_SHIFT);
-
-	rss = get_mm_rss(p->mm);
-	if (!rss)
-		rss = nr_scan_pages;
-
-	if (READ_ONCE(p->mm->numa_scan_seq) == 0) {
-		smax = p->mm->scanned_pages * sysctl_pan_scan_period_max;
-		smax = smax / rss;
-		smax = max_t(unsigned long, sysctl_pan_scan_period_min, smax);
-	}
-
-	return smax;
-}
-
-/*
- * Process-based Adaptive NUMA scan period update alogirthm
- *
- * These are the important concepts behind the scan period update:
- *
- * - increase trend (of scan period)
- *   scan period => up, memory coverage => down, overhead => down,
- *   accuracy => down
- * - decrease trend
- *   scan period => down, memory coverage => up, overhead => up,
- *   accuracy => up
- * - trend: Reflects the current active trend
- *   1 means increasing trend, 0 means decreasing trend
- * - slope
- *   it controls scan_period: new_scan_period = current_scan_period *
- *                                              100 / slope
- * - hist_trend: Reflects the intended trend in the last two
- *   windows. Uses the last two bits (bit0 and bit1) for the same.
- *   1 if increasing trend was intended, 0 if decreasing was intended.
- */
-
-/*
- * Check if the scan period needs updation when the remote fault
- * rate has changed (delta > 5)
- *
- * Returns TRUE if scan period needs updation, else FALSE.
- */
-static bool pan_changed_rate_update(struct mm_struct *mm, int ps_ratio,
-				    int oldest_remote_fault_rate,
-				    int fault_rate_diff)
-{
-	u8 value;
-
-	/*
-	 * Set the intended trend for the current window.
-	 * - If the remote fault rate has decreased, set the
-	 *   intended trend to increasing.
-	 * - Otherwise leave the intended trend as decreasing.
-	 */
-	mm->hist_trend = mm->hist_trend << 1;
-	if (fault_rate_diff < 5)
-		mm->hist_trend |= 0x01;
-
-	value = mm->hist_trend & 0x03;
-
-	if (fault_rate_diff < -5 && value == 3) {
-		/*
-		 * The remote fault rate has decreased and the intended
-		 * trend was set to increasing in the previous window.
-		 *
-		 * If on decreasing trend, reverse the trend and change
-		 * the slope using the fault rates from (current-1)
-		 * and (current-2) windows.
-		 *
-		 * If already on increasing trend, change the slope using
-		 * the fault rates from (current) and (current-1) windows.
-		 */
-		if (!mm->trend) {
-			mm->trend = true;
-			mm->slope = SLOPE(mm->remote_fault_rates[0] * 100,
-					  oldest_remote_fault_rate);
-		} else {
-			mm->slope = SLOPE(mm->remote_fault_rates[1] * 100,
-					  mm->remote_fault_rates[0]);
-		}
-	} else if (fault_rate_diff > 5 && value == 0) {
-		/*
-		 * The remote fault rate has increased and the intended
-		 * trend was set to decreasing in the previous window.
-		 *
-		 * If on increasing trend,
-		 *  - If shared fault ratio is more than 30%, don't yet
-		 *  reverse the trend, just mark the intended trend as
-		 *  increasing.
-		 *  - Otherwise reverse the trend. Change the slope using
-		 *  the fault rates from (current-1) and (current-2) windows.
-		 *
-		 *  If on decreasing trend
-		 *  - Continue with a changed slope using the fault
-		 *  rates from (current) and (current-1) windows.
-		 */
-		if (mm->trend) {
-			if (ps_ratio < 7) {
-				mm->hist_trend |= 0x01;
-				return true;
-			}
-
-			mm->trend = false;
-			mm->slope = SLOPE(mm->remote_fault_rates[0] * 100,
-					  oldest_remote_fault_rate);
-		} else {
-			mm->slope = SLOPE(mm->remote_fault_rates[1] * 100,
-					  mm->remote_fault_rates[0]);
-		}
-	} else if (value == 1 || value == 2) {
-		/*
-		 * The intended trend is oscillating
-		 *
-		 * If on decreasing trend and the shared fault ratio
-		 * is more than 30%, reverse the trend and change the slope.
-		 *
-		 * If on increasing trend, continue as is.
-		 */
-		if (!mm->trend && ps_ratio < 7) {
-			mm->hist_trend |= 0x01;
-			mm->trend = true;
-			mm->slope = SLOPE(100 * 100,
-					  100 + ((7 - ps_ratio) * 10));
-		}
-		return false;
-	}
-	return true;
-}
-
-/*
- * Check if the scan period needs updation when the remote fault
- * rate has remained more or less the same (delta <= 5)
- *
- * Returns TRUE if scan period needs updation, else FALSE.
- */
-static bool pan_const_rate_update(struct mm_struct *mm, int ps_ratio,
-				  int oldest_remote_fault_rate)
-{
-	int diff1, diff2;
-
-	mm->hist_trend = mm->hist_trend << 1;
-
-	/*
-	 * If we are in the increasing trend, don't change anything
-	 * except the intended trend for this window that was reset
-	 * to decreasing by default.
-	 */
-	if (mm->trend)
-		return false;
-
-	/* We are in the decreasing trend, reverse under some condidtions. */
-	diff1 = oldest_remote_fault_rate - mm->remote_fault_rates[0];
-	diff2 = mm->remote_fault_rates[0] - mm->remote_fault_rates[1];
-
-	if (ps_ratio < 7) {
-		/*
-		 * More than 30% of the pages are shared, so no point in
-		 * further reducing the scan period. If increasing trend
-		 * was intended in the previous window also, then reverse
-		 * the trend to increasing. Else just record the increasing
-		 * intended trend for this window and return.
-		 */
-		mm->hist_trend |= 0x01;
-		if ((mm->hist_trend & 0x03) == 3) {
-			mm->trend = true;
-			mm->slope = SLOPE(100 * 100,
-					  (100 + ((7 - ps_ratio) * 10)));
-		} else
-			return false;
-	} else if (diff1 >= 0 && diff2 >= 0 && mm->numa_scan_seq > 1) {
-		/*
-		 * Remote fault rate has reduced successively in the last
-		 * two windows and address space has been scanned at least
-		 * once. If increasing trend was intended in the previous
-		 * window also, then reverse the trend to increasing. Else
-		 * just record the increasing trend for this window and return.
-		 */
-		mm->hist_trend |= 0x01;
-		if ((mm->hist_trend & 0x03) == 3) {
-			mm->trend = true;
-			mm->slope = SLOPE(100 * 100, 110);
-			mm->hist_trend |= 0x03;
-		} else
-			return false;
-	}
-	return true;
-}
-
-static void pan_calculate_scan_period(struct task_struct *p)
-{
-	int remote_fault_rate, oldest_remote_fault_rate, ps_ratio, i, diff;
-	struct mm_struct *mm = p->mm;
-	unsigned long remote_hist = mm->faults_locality_history[0];
-	unsigned long local_hist = mm->faults_locality_history[1];
-	unsigned long shared_hist = mm->faults_shared_history[0];
-	unsigned long priv_hist = mm->faults_shared_history[1];
-	bool need_update;
-
-	ps_ratio = (priv_hist * 10) / (priv_hist + shared_hist + 1);
-	remote_fault_rate = (remote_hist * 100) / (local_hist + remote_hist + 1);
-
-	/* Keep the remote fault ratio at least 1% */
-	remote_fault_rate = max(remote_fault_rate, 1);
-	for (i = 0; i < 2; i++)
-		if (mm->remote_fault_rates[i] == 0)
-			mm->remote_fault_rates[i] = 1;
-
-	/* Shift right in mm->remote_fault_rates[] to keep track of history */
-	oldest_remote_fault_rate = mm->remote_fault_rates[0];
-	mm->remote_fault_rates[0] = mm->remote_fault_rates[1];
-	mm->remote_fault_rates[1] = remote_fault_rate;
-	diff = remote_fault_rate - oldest_remote_fault_rate;
-
-	if (abs(diff) <= 5)
-		need_update = pan_const_rate_update(mm, ps_ratio,
-						    oldest_remote_fault_rate);
-	else
-		need_update = pan_changed_rate_update(mm, ps_ratio,
-						      oldest_remote_fault_rate,
-						      diff);
-
-	if (need_update) {
-		if (mm->slope == 0)
-			mm->slope = 100;
-		mm->numa_scan_period = (100 * mm->numa_scan_period) / mm->slope;
-	}
-}
-
-/*
- * Update the cumulative history of local/remote and private/shared
- * statistics. If the numbers are too small worthy of updating,
- * return FALSE, otherwise return TRUE.
- */
-static bool pan_update_history(struct task_struct *p)
-{
-	unsigned long local, remote, shared, private;
-	long diff;
-	int i;
-
-	remote = atomic_long_read(&p->mm->faults_locality[0]);
-	local = atomic_long_read(&p->mm->faults_locality[1]);
-	shared = atomic_long_read(&p->mm->faults_shared[0]);
-	private = atomic_long_read(&p->mm->faults_shared[1]);
-
-	/* skip if the activities in this window are too small */
-	if (local + remote < 100)
-		return false;
-
-	/* decay over the time window by 1/4 */
-	diff = local - (long)(p->mm->faults_locality_history[1] / 4);
-	p->mm->faults_locality_history[1] += diff;
-	diff = remote - (long)(p->mm->faults_locality_history[0] / 4);
-	p->mm->faults_locality_history[0] += diff;
-
-	/* decay over the time window by 1/2 */
-	diff = shared - (long)(p->mm->faults_shared_history[0] / 2);
-	p->mm->faults_shared_history[0] += diff;
-	diff = private - (long)(p->mm->faults_shared_history[1] / 2);
-	p->mm->faults_shared_history[1] += diff;
-
-	/* clear the statistics for the next window */
-	for (i = 0; i < 2; i++) {
-		atomic_long_set(&(p->mm->faults_locality[i]), 0);
-		atomic_long_set(&(p->mm->faults_shared[i]), 0);
-	}
-
-	return true;
-}
-
-/*
- * Updates mm->numa_scan_period under mm->pan_numa_lock.
- */
-static unsigned long pan_get_scan_period(struct task_struct *p)
-{
-	if (pan_update_history(p))
-		pan_calculate_scan_period(p);
-
-	p->mm->numa_scan_period = clamp(p->mm->numa_scan_period,
-					READ_ONCE(sysctl_pan_scan_period_min),
-					pan_scan_max(p));
-
-	return p->mm->numa_scan_period;
-}
-
 /*
  * When adapting the scan rate, the period is divided into NUMA_PERIOD_SLOTS
  * increments. The more local the fault statistics are, the higher the scan
@@ -3170,9 +2871,6 @@ void task_numa_fault(int last_cpupid, int mem_node, int pages, int flags)
 			task_numa_group(p, last_cpupid, flags, &priv);
 	}
 
-	atomic_long_add(pages, &(p->mm->faults_locality[local]));
-	atomic_long_add(pages, &(p->mm->faults_shared[priv]));
-
 	/*
 	 * If a workload spans multiple NUMA nodes, a shared fault that
 	 * occurs wholly within the set of nodes that the workload is
@@ -3260,20 +2958,12 @@ static void task_numa_work(struct callback_head *work)
 	if (time_before(now, migrate))
 		return;
 
-	if (p->mm->numa_scan_period == 0) {
-		p->numa_scan_period_max = task_scan_max(p);
-		p->numa_scan_period = task_scan_start(p);
-		mm->numa_scan_period = p->numa_scan_period;
-	} else if (p->numa_scan_period == 0) {
+	if (p->numa_scan_period == 0) {
 		p->numa_scan_period_max = task_scan_max(p);
 		p->numa_scan_period = task_scan_start(p);
 	}
 
-	if (!spin_trylock(&p->mm->pan_numa_lock))
-		return;
-	next_scan = now + msecs_to_jiffies(pan_get_scan_period(p));
-	spin_unlock(&p->mm->pan_numa_lock);
-
+	next_scan = now + msecs_to_jiffies(p->numa_scan_period);
 	if (cmpxchg(&mm->numa_next_scan, migrate, next_scan) != migrate)
 		return;
 
@@ -3362,7 +3052,6 @@ static void task_numa_work(struct callback_head *work)
 		mm->numa_scan_offset = start;
 	else
 		reset_ptenuma_scan(p);
-	mm->scanned_pages += ((sysctl_numa_balancing_scan_size << (20 - PAGE_SHIFT)) - pages);
 	mmap_read_unlock(mm);
 
 	/*
@@ -3377,26 +3066,6 @@ static void task_numa_work(struct callback_head *work)
 	}
 }
 
-/* Init Process-based Adaptive NUMA */
-static void pan_init_numa(struct task_struct *p)
-{
-	struct mm_struct *mm = p->mm;
-	int i;
-
-	spin_lock_init(&mm->pan_numa_lock);
-	mm->numa_scan_period = sysctl_numa_balancing_scan_delay;
-	mm->scanned_pages = 0;
-	mm->trend = false;
-	mm->hist_trend = 0;
-	mm->slope = 100;
-
-	for (i = 0; i < 2; i++) {
-		mm->faults_locality_history[i] = 0;
-		mm->faults_shared_history[i] = 0;
-		mm->remote_fault_rates[i] = 1;
-	}
-}
-
 void init_numa_balancing(unsigned long clone_flags, struct task_struct *p)
 {
 	int mm_users = 0;
@@ -3407,7 +3076,6 @@ void init_numa_balancing(unsigned long clone_flags, struct task_struct *p)
 		if (mm_users == 1) {
 			mm->numa_next_scan = jiffies + msecs_to_jiffies(sysctl_numa_balancing_scan_delay);
 			mm->numa_scan_seq = 0;
-			pan_init_numa(p);
 		}
 	}
 	p->node_stamp			= 0;
@@ -3459,9 +3127,6 @@ static void task_tick_numa(struct rq *rq, struct task_struct *curr)
 	if (!curr->mm || (curr->flags & (PF_EXITING | PF_KTHREAD)) || work->next != work)
 		return;
 
-	if (!spin_trylock(&curr->mm->pan_numa_lock))
-		return;
-
 	/*
 	 * Using runtime rather than walltime has the dual advantage that
 	 * we (mostly) drive the selection from busy threads and that the
@@ -3469,17 +3134,16 @@ static void task_tick_numa(struct rq *rq, struct task_struct *curr)
 	 * NUMA placement.
 	 */
 	now = curr->se.sum_exec_runtime;
-	period = (u64)curr->mm->numa_scan_period * NSEC_PER_MSEC;
+	period = (u64)curr->numa_scan_period * NSEC_PER_MSEC;
 
 	if (now > curr->node_stamp + period) {
 		if (!curr->node_stamp)
-			curr->mm->numa_scan_period = task_scan_start(curr);
+			curr->numa_scan_period = task_scan_start(curr);
 		curr->node_stamp += period;
 
 		if (!time_before(jiffies, curr->mm->numa_next_scan))
 			task_work_add(curr, work, TWA_RESUME);
 	}
-	spin_unlock(&curr->mm->pan_numa_lock);
 }
 
 static void update_scan_period(struct task_struct *p, int new_cpu)
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 23b6dc73f..96dc9d4ef 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2477,8 +2477,6 @@ extern unsigned int sysctl_numa_balancing_scan_period_min;
 extern unsigned int sysctl_numa_balancing_scan_period_max;
 extern unsigned int sysctl_numa_balancing_scan_size;
 extern unsigned int sysctl_numa_balancing_hot_threshold;
-extern unsigned int sysctl_pan_scan_period_min;
-extern unsigned int sysctl_pan_scan_period_max;
 #endif
 
 #ifdef CONFIG_SCHED_HRTICK
-- 
2.39.0.rc2.1.gbd5df96b79

