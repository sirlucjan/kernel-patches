From 814d0498ae973b19fc575868b6d850bed207af30 Mon Sep 17 00:00:00 2001
From: Torge Matthies <openglfreak@googlemail.com>
Date: Sun, 21 Nov 2021 23:58:50 +0100
Subject: [PATCH 1/7] sched/alt: Optimize loops in update_sched_rq_watermark.

With the old code, gcc misses an optimization opportunity and compiles
the loops to five instructions each:

    0x0000000000000ed3 <+83>:	lock bts %rdi,(%rax)
    0x0000000000000ed8 <+88>:	dec    %rdx
    0x0000000000000edb <+91>:	add    $0x400,%rax
    0x0000000000000ee1 <+97>:	cmp    %rdx,%rsi
    0x0000000000000ee4 <+100>:	jne    0xed3 <update_sched_rq_watermark+83>
    ...
    0x0000000000000f13 <+147>:	lock btr %rdi,(%rax)
    0x0000000000000f18 <+152>:	dec    %rdx
    0x0000000000000f1b <+155>:	add    $0x400,%rax
    0x0000000000000f21 <+161>:	cmp    %rcx,%rdx
    0x0000000000000f24 <+164>:	jne    0xf13 <update_sched_rq_watermark+147>

With this change, the loops get optimized to four instructions each:

    0x0000000000000ed7 <+87>:	lock bts %rsi,(%rdx)
    0x0000000000000edc <+92>:	add    $0x400,%rdx
    0x0000000000000ee3 <+99>:	dec    %rcx
    0x0000000000000ee6 <+102>:	jne    0xed7 <update_sched_rq_watermark+87>
    ...
    0x0000000000000f1a <+154>:	lock btr %rsi,(%rax)
    0x0000000000000f1f <+159>:	add    $0x400,%rax
    0x0000000000000f25 <+165>:	dec    %rdx
    0x0000000000000f28 <+168>:	jne    0xf1a <update_sched_rq_watermark+154>

Signed-off-by: Torge Matthies <openglfreak@googlemail.com>
---
 kernel/sched/alt_core.c | 8 ++++----
 1 file changed, 4 insertions(+), 4 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index b348f014f..601086d40 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -185,8 +185,8 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 	rq->watermark = watermark;
 	cpu = cpu_of(rq);
 	if (watermark < last_wm) {
-		for (i = last_wm; i > watermark; i--)
-			cpumask_clear_cpu(cpu, sched_rq_watermark + SCHED_BITS - 1 - i);
+		for (i = last_wm - watermark; i > 0; i--)
+			cpumask_clear_cpu(cpu, sched_rq_watermark + SCHED_BITS - 1 - (i + watermark));
 #ifdef CONFIG_SCHED_SMT
 		if (static_branch_likely(&sched_smt_present) &&
 		    IDLE_TASK_SCHED_PRIO == last_wm)
@@ -196,8 +196,8 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 		return;
 	}
 	/* last_wm < watermark */
-	for (i = watermark; i > last_wm; i--)
-		cpumask_set_cpu(cpu, sched_rq_watermark + SCHED_BITS - 1 - i);
+	for (i = watermark - last_wm; i > 0; i--)
+		cpumask_set_cpu(cpu, sched_rq_watermark + SCHED_BITS - 1 - (i + last_wm));
 #ifdef CONFIG_SCHED_SMT
 	if (static_branch_likely(&sched_smt_present) &&
 	    IDLE_TASK_SCHED_PRIO == watermark) {
-- 
2.35.1.354.g715d08a9e5


From c56f757176b65d0e0784bc6acf5e7541a7576500 Mon Sep 17 00:00:00 2001
From: Juuso Alasuutari <juuso.alasuutari@gmail.com>
Date: Sat, 16 Oct 2021 17:01:28 +0300
Subject: [PATCH 2/7] sched/alt: Alfred Chen's PoC sched_preferred_cpumask
 patch

The hard-coded cpumask value is based on torturing my R9 3950X and
arbitrarily picking the top four cores. I have no idea if the same
cores are supposed to have the highest boost in every specimen.

Also fixed a typo in the cpumask variable.

Signed-off-by: Juuso Alasuutari <juuso.alasuutari@gmail.com>
---
 arch/x86/kernel/itmt.c   |  1 +
 kernel/sched/alt_core.c  | 29 ++++++++++++++++++-----------
 kernel/sched/alt_sched.h |  4 +++-
 3 files changed, 22 insertions(+), 12 deletions(-)

diff --git a/arch/x86/kernel/itmt.c b/arch/x86/kernel/itmt.c
index 9ff480e94..d1ea2d0c0 100644
--- a/arch/x86/kernel/itmt.c
+++ b/arch/x86/kernel/itmt.c
@@ -190,6 +190,7 @@ void sched_set_itmt_core_prio(int prio, int core_cpu)
 {
 	int cpu, i = 1;
 
+	pr_info("sched_set_itmt_core_prio: %d -- %d\n", core_cpu, prio);
 	for_each_cpu(cpu, topology_sibling_cpumask(core_cpu)) {
 		int smt_prio;
 
diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 601086d40..9b648c949 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -120,6 +120,8 @@ DEFINE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_LEVELS], sched_cpu_topo_masks);
 DEFINE_PER_CPU(cpumask_t *, sched_cpu_llc_mask);
 DEFINE_PER_CPU(cpumask_t *, sched_cpu_topo_end_mask);
 
+static cpumask_t sched_preferred_cpumask ____cacheline_aligned_in_smp;
+
 #ifdef CONFIG_SCHED_SMT
 DEFINE_STATIC_KEY_FALSE(sched_smt_present);
 EXPORT_SYMBOL_GPL(sched_smt_present);
@@ -7075,11 +7077,14 @@ static void sched_init_topology_cpumask_early(void)
 	}
 }
 
-#define TOPOLOGY_CPUMASK(name, mask, last)\
+#define TOPOLOGY_CPUMASK(name, mask, itmt, last)\
 	if (cpumask_and(topo, topo, mask)) {					\
+		if (itmt && cpumask_and(topo, topo, &sched_preferred_cpumask))	\
+			pr_info("sched: cpu#%02d topo: 0x%016lx - "#name" itmt",\
+				cpu, (topo++)->bits[0]);			\
 		cpumask_copy(topo, mask);					\
-		printk(KERN_INFO "sched: cpu#%02d topo: 0x%08lx - "#name,	\
-		       cpu, (topo++)->bits[0]);					\
+		pr_info("sched: cpu#%02d topo: 0x%016lx - "#name,		\
+			cpu, (topo++)->bits[0]);				\
 	}									\
 	if (!last)								\
 		cpumask_complement(topo, mask)
@@ -7089,6 +7094,8 @@ static void sched_init_topology_cpumask(void)
 	int cpu;
 	cpumask_t *topo;
 
+	sched_preferred_cpumask.bits[0] = 0x00600060;
+
 	for_each_online_cpu(cpu) {
 		/* take chance to reset time slice for idle tasks */
 		cpu_rq(cpu)->idle->time_slice = sched_timeslice_ns;
@@ -7097,21 +7104,21 @@ static void sched_init_topology_cpumask(void)
 
 		cpumask_complement(topo, cpumask_of(cpu));
 #ifdef CONFIG_SCHED_SMT
-		TOPOLOGY_CPUMASK(smt, topology_sibling_cpumask(cpu), false);
+		TOPOLOGY_CPUMASK(smt, topology_sibling_cpumask(cpu), false, false);
 #endif
 		per_cpu(sd_llc_id, cpu) = cpumask_first(cpu_coregroup_mask(cpu));
 		per_cpu(sched_cpu_llc_mask, cpu) = topo;
-		TOPOLOGY_CPUMASK(coregroup, cpu_coregroup_mask(cpu), false);
+		TOPOLOGY_CPUMASK(coregroup, cpu_coregroup_mask(cpu), true, false);
 
-		TOPOLOGY_CPUMASK(core, topology_core_cpumask(cpu), false);
+		TOPOLOGY_CPUMASK(core, topology_core_cpumask(cpu), true, false);
 
-		TOPOLOGY_CPUMASK(others, cpu_online_mask, true);
+		TOPOLOGY_CPUMASK(others, cpu_online_mask, true, true);
 
 		per_cpu(sched_cpu_topo_end_mask, cpu) = topo;
-		printk(KERN_INFO "sched: cpu#%02d llc_id = %d, llc_mask idx = %d\n",
-		       cpu, per_cpu(sd_llc_id, cpu),
-		       (int) (per_cpu(sched_cpu_llc_mask, cpu) -
-			      per_cpu(sched_cpu_topo_masks, cpu)));
+		pr_info("sched: cpu#%02d llc_id = %d, llc_mask idx = %d\n",
+			cpu, per_cpu(sd_llc_id, cpu),
+			(int) (per_cpu(sched_cpu_llc_mask, cpu) -
+			       per_cpu(sched_cpu_topo_masks, cpu)));
 	}
 }
 #endif
diff --git a/kernel/sched/alt_sched.h b/kernel/sched/alt_sched.h
index 6ff979a29..1e42ce057 100644
--- a/kernel/sched/alt_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -302,14 +302,16 @@ enum {
 #ifdef CONFIG_SCHED_SMT
 	SMT_LEVEL_SPACE_HOLDER,
 #endif
+	COREGROUP_ITMT_LEVEL_SPACE_HOLDER,
 	COREGROUP_LEVEL_SPACE_HOLDER,
+	CORE_ITMT_LEVEL_SPACE_HOLDER,
 	CORE_LEVEL_SPACE_HOLDER,
+	OTHER_ITMT_LEVEL_SPACE_HOLDER,
 	OTHER_LEVEL_SPACE_HOLDER,
 	NR_CPU_AFFINITY_LEVELS
 };
 
 DECLARE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_LEVELS], sched_cpu_topo_masks);
-DECLARE_PER_CPU(cpumask_t *, sched_cpu_llc_mask);
 
 static inline int
 __best_mask_cpu(const cpumask_t *cpumask, const cpumask_t *mask)
-- 
2.35.1.354.g715d08a9e5


From d4303ba420b446c6cbe0d7ee7033e4d686b8f1bc Mon Sep 17 00:00:00 2001
From: Piotr Gorski <lucjan.lucjanov@gmail.com>
Date: Tue, 18 Jan 2022 12:45:15 +0100
Subject: [PATCH 3/7] Revert "sched/alt: Alfred Chen's PoC
 sched_preferred_cpumask patch"

This reverts commit 442ee3a94443b440b3d44ef71de51ae03cd07108.

Signed-off-by: Piotr Gorski <lucjan.lucjanov@gmail.com>
---
 arch/x86/kernel/itmt.c   |  1 -
 kernel/sched/alt_core.c  | 29 +++++++++++------------------
 kernel/sched/alt_sched.h |  4 +---
 3 files changed, 12 insertions(+), 22 deletions(-)

diff --git a/arch/x86/kernel/itmt.c b/arch/x86/kernel/itmt.c
index d1ea2d0c0..9ff480e94 100644
--- a/arch/x86/kernel/itmt.c
+++ b/arch/x86/kernel/itmt.c
@@ -190,7 +190,6 @@ void sched_set_itmt_core_prio(int prio, int core_cpu)
 {
 	int cpu, i = 1;
 
-	pr_info("sched_set_itmt_core_prio: %d -- %d\n", core_cpu, prio);
 	for_each_cpu(cpu, topology_sibling_cpumask(core_cpu)) {
 		int smt_prio;
 
diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 9b648c949..601086d40 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -120,8 +120,6 @@ DEFINE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_LEVELS], sched_cpu_topo_masks);
 DEFINE_PER_CPU(cpumask_t *, sched_cpu_llc_mask);
 DEFINE_PER_CPU(cpumask_t *, sched_cpu_topo_end_mask);
 
-static cpumask_t sched_preferred_cpumask ____cacheline_aligned_in_smp;
-
 #ifdef CONFIG_SCHED_SMT
 DEFINE_STATIC_KEY_FALSE(sched_smt_present);
 EXPORT_SYMBOL_GPL(sched_smt_present);
@@ -7077,14 +7075,11 @@ static void sched_init_topology_cpumask_early(void)
 	}
 }
 
-#define TOPOLOGY_CPUMASK(name, mask, itmt, last)\
+#define TOPOLOGY_CPUMASK(name, mask, last)\
 	if (cpumask_and(topo, topo, mask)) {					\
-		if (itmt && cpumask_and(topo, topo, &sched_preferred_cpumask))	\
-			pr_info("sched: cpu#%02d topo: 0x%016lx - "#name" itmt",\
-				cpu, (topo++)->bits[0]);			\
 		cpumask_copy(topo, mask);					\
-		pr_info("sched: cpu#%02d topo: 0x%016lx - "#name,		\
-			cpu, (topo++)->bits[0]);				\
+		printk(KERN_INFO "sched: cpu#%02d topo: 0x%08lx - "#name,	\
+		       cpu, (topo++)->bits[0]);					\
 	}									\
 	if (!last)								\
 		cpumask_complement(topo, mask)
@@ -7094,8 +7089,6 @@ static void sched_init_topology_cpumask(void)
 	int cpu;
 	cpumask_t *topo;
 
-	sched_preferred_cpumask.bits[0] = 0x00600060;
-
 	for_each_online_cpu(cpu) {
 		/* take chance to reset time slice for idle tasks */
 		cpu_rq(cpu)->idle->time_slice = sched_timeslice_ns;
@@ -7104,21 +7097,21 @@ static void sched_init_topology_cpumask(void)
 
 		cpumask_complement(topo, cpumask_of(cpu));
 #ifdef CONFIG_SCHED_SMT
-		TOPOLOGY_CPUMASK(smt, topology_sibling_cpumask(cpu), false, false);
+		TOPOLOGY_CPUMASK(smt, topology_sibling_cpumask(cpu), false);
 #endif
 		per_cpu(sd_llc_id, cpu) = cpumask_first(cpu_coregroup_mask(cpu));
 		per_cpu(sched_cpu_llc_mask, cpu) = topo;
-		TOPOLOGY_CPUMASK(coregroup, cpu_coregroup_mask(cpu), true, false);
+		TOPOLOGY_CPUMASK(coregroup, cpu_coregroup_mask(cpu), false);
 
-		TOPOLOGY_CPUMASK(core, topology_core_cpumask(cpu), true, false);
+		TOPOLOGY_CPUMASK(core, topology_core_cpumask(cpu), false);
 
-		TOPOLOGY_CPUMASK(others, cpu_online_mask, true, true);
+		TOPOLOGY_CPUMASK(others, cpu_online_mask, true);
 
 		per_cpu(sched_cpu_topo_end_mask, cpu) = topo;
-		pr_info("sched: cpu#%02d llc_id = %d, llc_mask idx = %d\n",
-			cpu, per_cpu(sd_llc_id, cpu),
-			(int) (per_cpu(sched_cpu_llc_mask, cpu) -
-			       per_cpu(sched_cpu_topo_masks, cpu)));
+		printk(KERN_INFO "sched: cpu#%02d llc_id = %d, llc_mask idx = %d\n",
+		       cpu, per_cpu(sd_llc_id, cpu),
+		       (int) (per_cpu(sched_cpu_llc_mask, cpu) -
+			      per_cpu(sched_cpu_topo_masks, cpu)));
 	}
 }
 #endif
diff --git a/kernel/sched/alt_sched.h b/kernel/sched/alt_sched.h
index 1e42ce057..6ff979a29 100644
--- a/kernel/sched/alt_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -302,16 +302,14 @@ enum {
 #ifdef CONFIG_SCHED_SMT
 	SMT_LEVEL_SPACE_HOLDER,
 #endif
-	COREGROUP_ITMT_LEVEL_SPACE_HOLDER,
 	COREGROUP_LEVEL_SPACE_HOLDER,
-	CORE_ITMT_LEVEL_SPACE_HOLDER,
 	CORE_LEVEL_SPACE_HOLDER,
-	OTHER_ITMT_LEVEL_SPACE_HOLDER,
 	OTHER_LEVEL_SPACE_HOLDER,
 	NR_CPU_AFFINITY_LEVELS
 };
 
 DECLARE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_LEVELS], sched_cpu_topo_masks);
+DECLARE_PER_CPU(cpumask_t *, sched_cpu_llc_mask);
 
 static inline int
 __best_mask_cpu(const cpumask_t *cpumask, const cpumask_t *mask)
-- 
2.35.1.354.g715d08a9e5


From c100bb3062ebe0e7412a62b55e50644839c2a0cd Mon Sep 17 00:00:00 2001
From: Piotr Gorski <lucjan.lucjanov@gmail.com>
Date: Sun, 13 Mar 2022 23:29:08 +0100
Subject: [PATCH 4/7] Revert "sched/alt: Optimize loops in
 update_sched_rq_watermark."

This reverts commit 88adfd0b1372e2db6b31bdae6eee98fc3f1602c7.

Signed-off-by: Piotr Gorski <lucjan.lucjanov@gmail.com>
---
 kernel/sched/alt_core.c | 8 ++++----
 1 file changed, 4 insertions(+), 4 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 601086d40..b348f014f 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -185,8 +185,8 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 	rq->watermark = watermark;
 	cpu = cpu_of(rq);
 	if (watermark < last_wm) {
-		for (i = last_wm - watermark; i > 0; i--)
-			cpumask_clear_cpu(cpu, sched_rq_watermark + SCHED_BITS - 1 - (i + watermark));
+		for (i = last_wm; i > watermark; i--)
+			cpumask_clear_cpu(cpu, sched_rq_watermark + SCHED_BITS - 1 - i);
 #ifdef CONFIG_SCHED_SMT
 		if (static_branch_likely(&sched_smt_present) &&
 		    IDLE_TASK_SCHED_PRIO == last_wm)
@@ -196,8 +196,8 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 		return;
 	}
 	/* last_wm < watermark */
-	for (i = watermark - last_wm; i > 0; i--)
-		cpumask_set_cpu(cpu, sched_rq_watermark + SCHED_BITS - 1 - (i + last_wm));
+	for (i = watermark; i > last_wm; i--)
+		cpumask_set_cpu(cpu, sched_rq_watermark + SCHED_BITS - 1 - i);
 #ifdef CONFIG_SCHED_SMT
 	if (static_branch_likely(&sched_smt_present) &&
 	    IDLE_TASK_SCHED_PRIO == watermark) {
-- 
2.35.1.354.g715d08a9e5


From 584b3540e39eccfedeeed0149cf7a82c010a40b1 Mon Sep 17 00:00:00 2001
From: Torge Matthies <openglfreak@googlemail.com>
Date: Fri, 11 Mar 2022 02:52:30 +0100
Subject: [PATCH 5/7] sched/alt: Transpose the sched_rq_watermark array.

---
 kernel/sched/alt_core.c | 124 +++++++++++++++++++++++++++++++++-------
 1 file changed, 104 insertions(+), 20 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index b348f014f..f7d8604e1 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -147,7 +147,87 @@ DEFINE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues);
 #ifdef CONFIG_SCHED_SMT
 static cpumask_t sched_sg_idle_mask ____cacheline_aligned_in_smp;
 #endif
-static cpumask_t sched_rq_watermark[SCHED_BITS] ____cacheline_aligned_in_smp;
+
+#define BITS_PER_ATOMIC_LONG_T BITS_PER_LONG
+typedef struct sched_bitmask {
+	atomic_long_t bits[DIV_ROUND_UP(SCHED_BITS, BITS_PER_ATOMIC_LONG_T)];
+} sched_bitmask_t;
+static sched_bitmask_t sched_rq_watermark[NR_CPUS] ____cacheline_aligned_in_smp;
+
+#define x(p, set, mask)                                \
+	do {                                           \
+		if (set)                               \
+			atomic_long_or((mask), (p));   \
+		else                                   \
+			atomic_long_and(~(mask), (p)); \
+	} while (0)
+
+static __always_inline void sched_rq_watermark_fill_downwards(int cpu, unsigned int end,
+		unsigned int start, bool set)
+{
+	unsigned int start_idx, start_bit;
+	unsigned int end_idx, end_bit;
+	atomic_long_t *p;
+
+	if (end == start) {
+		return;
+	}
+
+	start_idx = start / BITS_PER_ATOMIC_LONG_T;
+	start_bit = start % BITS_PER_ATOMIC_LONG_T;
+	end_idx = (end - 1) / BITS_PER_ATOMIC_LONG_T;
+	end_bit = (end - 1) % BITS_PER_ATOMIC_LONG_T;
+	p = &sched_rq_watermark[cpu].bits[end_idx];
+
+	if (end_idx == start_idx) {
+		x(p, set, (~0UL >> (BITS_PER_ATOMIC_LONG_T - 1 - end_bit)) & (~0UL << start_bit));
+		return;
+	}
+
+	if (end_bit != BITS_PER_ATOMIC_LONG_T - 1) {
+		x(p, set, (~0UL >> (BITS_PER_ATOMIC_LONG_T - 1 - end_bit)));
+		p -= 1;
+		end_idx -= 1;
+	}
+
+	while (end_idx != start_idx) {
+		atomic_long_set(p, set ? ~0UL : 0);
+		p -= 1;
+		end_idx -= 1;
+	}
+
+	x(p, set, ~0UL << start_bit);
+}
+
+#undef x
+
+static __always_inline bool sched_rq_watermark_and(cpumask_t *dstp, const cpumask_t *cpus, int prio, bool not)
+{
+	int cpu;
+	bool ret = false;
+	int idx = prio / BITS_PER_ATOMIC_LONG_T;
+	int bit = prio % BITS_PER_ATOMIC_LONG_T;
+
+	cpumask_clear(dstp);
+	for_each_cpu(cpu, cpus)
+		if (test_bit(bit, (long*)&sched_rq_watermark[cpu].bits[idx].counter) == !not) {
+			__cpumask_set_cpu(cpu, dstp);
+			ret = true;
+		}
+	return ret;
+}
+
+static __always_inline bool sched_rq_watermark_test(const cpumask_t *cpus, int prio, bool not)
+{
+	int cpu;
+	int idx = prio / BITS_PER_ATOMIC_LONG_T;
+	int bit = prio % BITS_PER_ATOMIC_LONG_T;
+
+	for_each_cpu(cpu, cpus)
+		if (test_bit(bit, (long*)&sched_rq_watermark[cpu].bits[idx].counter) == !not)
+			return true;
+	return false;
+}
 
 /* sched_queue related functions */
 static inline void sched_queue_init(struct sched_queue *q)
@@ -176,7 +256,6 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 {
 	unsigned long watermark = find_first_bit(rq->queue.bitmap, SCHED_QUEUE_BITS);
 	unsigned long last_wm = rq->watermark;
-	unsigned long i;
 	int cpu;
 
 	if (watermark == last_wm)
@@ -185,28 +264,25 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 	rq->watermark = watermark;
 	cpu = cpu_of(rq);
 	if (watermark < last_wm) {
-		for (i = last_wm; i > watermark; i--)
-			cpumask_clear_cpu(cpu, sched_rq_watermark + SCHED_BITS - 1 - i);
+		sched_rq_watermark_fill_downwards(cpu, SCHED_BITS - 1 - watermark, SCHED_BITS - 1 - last_wm, false);
 #ifdef CONFIG_SCHED_SMT
 		if (static_branch_likely(&sched_smt_present) &&
-		    IDLE_TASK_SCHED_PRIO == last_wm)
+		    unlikely(IDLE_TASK_SCHED_PRIO == last_wm))
 			cpumask_andnot(&sched_sg_idle_mask,
 				       &sched_sg_idle_mask, cpu_smt_mask(cpu));
 #endif
 		return;
 	}
 	/* last_wm < watermark */
-	for (i = watermark; i > last_wm; i--)
-		cpumask_set_cpu(cpu, sched_rq_watermark + SCHED_BITS - 1 - i);
+	sched_rq_watermark_fill_downwards(cpu, SCHED_BITS - 1 - last_wm, SCHED_BITS - 1 - watermark, true);
 #ifdef CONFIG_SCHED_SMT
 	if (static_branch_likely(&sched_smt_present) &&
-	    IDLE_TASK_SCHED_PRIO == watermark) {
-		cpumask_t tmp;
+	    unlikely(IDLE_TASK_SCHED_PRIO == watermark)) {
+		const cpumask_t *smt_mask = cpu_smt_mask(cpu);
 
-		cpumask_and(&tmp, cpu_smt_mask(cpu), sched_rq_watermark);
-		if (cpumask_equal(&tmp, cpu_smt_mask(cpu)))
+		if (!sched_rq_watermark_test(smt_mask, 0, true))
 			cpumask_or(&sched_sg_idle_mask,
-				   &sched_sg_idle_mask, cpu_smt_mask(cpu));
+				   &sched_sg_idle_mask, smt_mask);
 	}
 #endif
 }
@@ -1905,9 +1981,9 @@ static inline int select_task_rq(struct task_struct *p)
 #ifdef CONFIG_SCHED_SMT
 	    cpumask_and(&tmp, &chk_mask, &sched_sg_idle_mask) ||
 #endif
-	    cpumask_and(&tmp, &chk_mask, sched_rq_watermark) ||
-	    cpumask_and(&tmp, &chk_mask,
-			sched_rq_watermark + SCHED_BITS - task_sched_prio(p)))
+	    sched_rq_watermark_and(&tmp, &chk_mask, 0, false) ||
+	    sched_rq_watermark_and(&tmp, &chk_mask,
+			SCHED_BITS - task_sched_prio(p), false))
 		return best_mask_cpu(task_cpu(p), &tmp);
 
 	return best_mask_cpu(task_cpu(p), &chk_mask);
@@ -3955,7 +4031,7 @@ static inline void sg_balance_check(struct rq *rq)
 	 * find potential cpus which can migrate the current running task
 	 */
 	if (cpumask_test_cpu(cpu, &sched_sg_idle_mask) &&
-	    cpumask_andnot(&chk, cpu_online_mask, sched_rq_watermark) &&
+	    sched_rq_watermark_and(&chk, cpu_online_mask, 0, true) &&
 	    cpumask_andnot(&chk, &chk, &sched_rq_pending_mask)) {
 		int i;
 
@@ -4263,9 +4339,8 @@ static inline void schedule_debug(struct task_struct *prev, bool preempt)
 #ifdef ALT_SCHED_DEBUG
 void alt_sched_debug(void)
 {
-	printk(KERN_INFO "sched: pending: 0x%04lx, idle: 0x%04lx, sg_idle: 0x%04lx\n",
+	printk(KERN_INFO "sched: pending: 0x%04lx, sg_idle: 0x%04lx\n",
 	       sched_rq_pending_mask.bits[0],
-	       sched_rq_watermark[0].bits[0],
 	       sched_sg_idle_mask.bits[0]);
 }
 #else
@@ -7178,8 +7253,17 @@ void __init sched_init(void)
 	wait_bit_init();
 
 #ifdef CONFIG_SMP
-	for (i = 0; i < SCHED_BITS; i++)
-		cpumask_copy(sched_rq_watermark + i, cpu_present_mask);
+	for (i = 0; i < nr_cpu_ids; i++) {
+		long val = cpumask_test_cpu(i, cpu_present_mask) ? -1L : 0;
+		int j;
+		for (j = 0; j < DIV_ROUND_UP(SCHED_BITS, BITS_PER_ATOMIC_LONG_T); j++)
+			atomic_long_set(&sched_rq_watermark[i].bits[j], val);
+	}
+	for (i = nr_cpu_ids; i < NR_CPUS; i++) {
+		int j;
+		for (j = 0; j < DIV_ROUND_UP(SCHED_BITS, BITS_PER_ATOMIC_LONG_T); j++)
+			atomic_long_set(&sched_rq_watermark[i].bits[j], 0);
+	}
 #endif
 
 #ifdef CONFIG_CGROUP_SCHED
-- 
2.35.1.354.g715d08a9e5


From 4d89d785a00219d5d3bddf95a904a8267fac6c3e Mon Sep 17 00:00:00 2001
From: Tor Vic <torvic9@mailbox.org>
Date: Wed, 9 Mar 2022 14:03:08 +0100
Subject: [PATCH 6/7] alt_core.c: Add potentially missing assignment of
 p->on_cpu in sched_fork

---
 kernel/sched/alt_core.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index f7d8604e1..5e47ebfec 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -3052,6 +3052,9 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 #ifdef CONFIG_SCHED_INFO
 	if (unlikely(sched_info_on()))
 		memset(&p->sched_info, 0, sizeof(p->sched_info));
+#endif
+#if defined(CONFIG_SMP)
+	p->on_cpu = 0;
 #endif
 	init_task_preempt_count(p);
 
-- 
2.35.1.354.g715d08a9e5


From 045f11a81aa466de93422418b2ecb42ef0e03a0d Mon Sep 17 00:00:00 2001
From: Torge Matthies <openglfreak@googlemail.com>
Date: Tue, 15 Mar 2022 23:08:54 +0100
Subject: [PATCH 7/7] sched/alt: Add memory barriers around atomics.

---
 kernel/sched/alt_core.c | 4 ++++
 1 file changed, 4 insertions(+)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 5e47ebfec..4d7a4e1e5 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -156,10 +156,12 @@ static sched_bitmask_t sched_rq_watermark[NR_CPUS] ____cacheline_aligned_in_smp;
 
 #define x(p, set, mask)                                \
 	do {                                           \
+		smp_mb__before_atomic();               \
 		if (set)                               \
 			atomic_long_or((mask), (p));   \
 		else                                   \
 			atomic_long_and(~(mask), (p)); \
+		smp_mb__after_atomic();                \
 	} while (0)
 
 static __always_inline void sched_rq_watermark_fill_downwards(int cpu, unsigned int end,
@@ -191,7 +193,9 @@ static __always_inline void sched_rq_watermark_fill_downwards(int cpu, unsigned
 	}
 
 	while (end_idx != start_idx) {
+		smp_mb__before_atomic();
 		atomic_long_set(p, set ? ~0UL : 0);
+		smp_mb__after_atomic();
 		p -= 1;
 		end_idx -= 1;
 	}
-- 
2.35.1.354.g715d08a9e5

