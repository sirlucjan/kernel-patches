From aab0c7824b534df6f1eaee1409bfae560adab588 Mon Sep 17 00:00:00 2001
From: Hamad Al Marri <hamad.s.almarri@gmail.com>
Date: Sat, 13 Nov 2021 21:54:14 +0300
Subject: [PATCH 07/29] fix when enabling CONFIG_NUMA_BALANCING

---
 kernel/sched/bs.c        |  2 +-
 kernel/sched/fair_numa.h | 34 -------------------
 kernel/sched/tt_stats.h  | 70 +++++++++++++++++++++-------------------
 3 files changed, 37 insertions(+), 69 deletions(-)

diff --git a/kernel/sched/bs.c b/kernel/sched/bs.c
index 76d9ca2ad..2813bf71c 100644
--- a/kernel/sched/bs.c
+++ b/kernel/sched/bs.c
@@ -6,8 +6,8 @@
  */
 #include "sched.h"
 #include "pelt.h"
-#include "fair_numa.h"
 #include "tt_stats.h"
+#include "fair_numa.h"
 #include "bs.h"
 
 unsigned int __read_mostly tt_max_lifetime	= 22000; // in ms
diff --git a/kernel/sched/fair_numa.h b/kernel/sched/fair_numa.h
index fd49208da..a0860a283 100644
--- a/kernel/sched/fair_numa.h
+++ b/kernel/sched/fair_numa.h
@@ -486,40 +486,16 @@ struct task_numa_env {
 	int best_cpu;
 };
 
-static inline unsigned long cfs_rq_load_avg(struct cfs_rq *cfs_rq)
-{
-	return cfs_rq->avg.load_avg;
-}
-
 static unsigned long cpu_load(struct rq *rq)
 {
 	return cfs_rq_load_avg(&rq->cfs);
 }
 
-static inline unsigned long cfs_rq_runnable_avg(struct cfs_rq *cfs_rq)
-{
-	return cfs_rq->avg.runnable_avg;
-}
-
 static unsigned long cpu_runnable(struct rq *rq)
 {
 	return cfs_rq_runnable_avg(&rq->cfs);
 }
 
-static inline unsigned long cpu_util(int cpu)
-{
-	struct cfs_rq *cfs_rq;
-	unsigned int util;
-
-	cfs_rq = &cpu_rq(cpu)->cfs;
-	util = READ_ONCE(cfs_rq->avg.util_avg);
-
-	if (sched_feat(UTIL_EST))
-		util = max(util, READ_ONCE(cfs_rq->avg.util_est.enqueued));
-
-	return min_t(unsigned long, util, capacity_orig_of(cpu));
-}
-
 /*
  * Allow a NUMA imbalance if busy CPUs is less than 25% of the domain.
  * This is an approximation as the number of running tasks may not be
@@ -590,11 +566,6 @@ static inline int numa_idle_core(int idle_core, int cpu)
 }
 #endif
 
-static unsigned long capacity_of(int cpu)
-{
-	return cpu_rq(cpu)->cpu_capacity;
-}
-
 /*
  * Gather all necessary information to make NUMA balancing placement
  * decisions that are compatible with standard load balancer. This
@@ -716,11 +687,6 @@ static bool load_too_imbalanced(long src_load, long dst_load,
 	return (imb > old_imb);
 }
 
-static unsigned long task_h_load(struct task_struct *p)
-{
-	return p->se.avg.load_avg;
-}
-
 /*
  * Maximum NUMA importance can be 1998 (2*999);
  * SMALLIMP @ 30 would be close to 1998/64.
diff --git a/kernel/sched/tt_stats.h b/kernel/sched/tt_stats.h
index 0f8f7e61d..2e897866c 100644
--- a/kernel/sched/tt_stats.h
+++ b/kernel/sched/tt_stats.h
@@ -221,6 +221,42 @@ static inline void cfs_rq_util_change(struct cfs_rq *cfs_rq, int flags)
 	}
 }
 
+#if defined(CONFIG_NUMA_BALANCING) || (defined(CONFIG_SMP) && defined(CONFIG_TT_ACCOUNTING_STATS))
+static unsigned long capacity_of(int cpu)
+{
+	return cpu_rq(cpu)->cpu_capacity;
+}
+
+static inline unsigned long cfs_rq_load_avg(struct cfs_rq *cfs_rq)
+{
+	return cfs_rq->avg.load_avg;
+}
+
+static inline unsigned long cfs_rq_runnable_avg(struct cfs_rq *cfs_rq)
+{
+	return cfs_rq->avg.runnable_avg;
+}
+
+static inline unsigned long cpu_util(int cpu)
+{
+	struct cfs_rq *cfs_rq;
+	unsigned int util;
+
+	cfs_rq = &cpu_rq(cpu)->cfs;
+	util = READ_ONCE(cfs_rq->avg.util_avg);
+
+	if (sched_feat(UTIL_EST))
+		util = max(util, READ_ONCE(cfs_rq->avg.util_est.enqueued));
+
+	return min_t(unsigned long, util, capacity_orig_of(cpu));
+}
+
+static unsigned long task_h_load(struct task_struct *p)
+{
+	return p->se.avg.load_avg;
+}
+#endif
+
 #if defined(CONFIG_SMP) && defined(CONFIG_TT_ACCOUNTING_STATS)
 /*
  * The margin used when comparing utilization with CPU capacity.
@@ -229,11 +265,6 @@ static inline void cfs_rq_util_change(struct cfs_rq *cfs_rq, int flags)
  */
 #define fits_capacity(cap, max)	((cap) * 1280 < (max) * 1024)
 
-static unsigned long capacity_of(int cpu)
-{
-	return cpu_rq(cpu)->cpu_capacity;
-}
-
 static inline void
 enqueue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se)
 {
@@ -517,16 +548,6 @@ static void remove_entity_load_avg(struct sched_entity *se)
 	raw_spin_unlock_irqrestore(&cfs_rq->removed.lock, flags);
 }
 
-static inline unsigned long cfs_rq_runnable_avg(struct cfs_rq *cfs_rq)
-{
-	return cfs_rq->avg.runnable_avg;
-}
-
-static inline unsigned long cfs_rq_load_avg(struct cfs_rq *cfs_rq)
-{
-	return cfs_rq->avg.load_avg;
-}
-
 static inline unsigned long task_util(struct task_struct *p)
 {
 	return READ_ONCE(p->se.avg.util_avg);
@@ -696,11 +717,6 @@ static inline int task_fits_capacity(struct task_struct *p, long capacity)
 	return fits_capacity(uclamp_task_util(p), capacity);
 }
 
-static unsigned long task_h_load(struct task_struct *p)
-{
-	return p->se.avg.load_avg;
-}
-
 static inline void update_misfit_status(struct task_struct *p, struct rq *rq)
 {
 	if (!static_branch_unlikely(&sched_asym_cpucapacity))
@@ -780,20 +796,6 @@ static inline void check_schedstat_required(void)
 }
 
 #if defined(CONFIG_SMP) && defined(CONFIG_TT_ACCOUNTING_STATS)
-static inline unsigned long cpu_util(int cpu)
-{
-	struct cfs_rq *cfs_rq;
-	unsigned int util;
-
-	cfs_rq = &cpu_rq(cpu)->cfs;
-	util = READ_ONCE(cfs_rq->avg.util_avg);
-
-	if (sched_feat(UTIL_EST))
-		util = max(util, READ_ONCE(cfs_rq->avg.util_est.enqueued));
-
-	return min_t(unsigned long, util, capacity_orig_of(cpu));
-}
-
 static inline bool cpu_overutilized(int cpu)
 {
 	return !fits_capacity(cpu_util(cpu), capacity_of(cpu));
-- 
2.34.1.75.gabe6bb3905

