From 0d50db1822489572107c1b478fc5b00be7794934 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Mon, 17 May 2021 10:25:28 +0000
Subject: [PATCH 150/232] sched/pds: PDS improvement.

PDS uses bitmap queue as queue data structure. Rename maybe needed after
all improvement are done.
---
 include/linux/sched.h     |   7 +-
 include/linux/skip_list.h | 175 ------------------------
 init/init_task.c          |   3 +-
 kernel/sched/alt_core.c   |  11 +-
 kernel/sched/alt_sched.h  |   3 +-
 kernel/sched/pds.h        |   5 +
 kernel/sched/pds_imp.h    | 281 ++++++++++++++++++++++----------------
 7 files changed, 183 insertions(+), 302 deletions(-)
 delete mode 100644 include/linux/skip_list.h

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 1b61e593b843..a10c910a8f90 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -34,7 +34,6 @@
 #include <linux/rseq.h>
 #include <linux/seqlock.h>
 #include <linux/kcsan.h>
-#include <linux/skip_list.h>
 #include <asm/kmap_size.h>
 
 /* task_struct member predeclarations (sorted alphabetically): */
@@ -794,10 +793,8 @@ struct task_struct {
 #ifdef CONFIG_SCHED_PDS
 	u64				deadline;
 	u64				priodl;
-	/* skip list level */
-	int				sl_level;
-	/* skip list node */
-	struct skiplist_node		sl_node;
+	int				sq_idx;
+	struct list_head		sq_node;
 #endif /* CONFIG_SCHED_PDS */
 	/* sched_clock time spent running */
 	u64				sched_time;
diff --git a/include/linux/skip_list.h b/include/linux/skip_list.h
deleted file mode 100644
index 637c83ecbd6b..000000000000
--- a/include/linux/skip_list.h
+++ /dev/null
@@ -1,175 +0,0 @@
-/*
- * Copyright (C) 2016 Alfred Chen.
- *
- * Code based on Con Kolivas's skip list implementation for BFS, and
- * which is based on example originally by William Pugh.
- *
- * Skip Lists are a probabilistic alternative to balanced trees, as
- * described in the June 1990 issue of CACM and were invented by
- * William Pugh in 1987.
- *
- * A couple of comments about this implementation:
- *
- * This file only provides a infrastructure of skip list.
- *
- * skiplist_node is embedded into container data structure, to get rid
- * the dependency of kmalloc/kfree operation in scheduler code.
- *
- * A customized search function should be defined using DEFINE_SKIPLIST_INSERT
- * macro and be used for skip list insert operation.
- *
- * Random Level is also not defined in this file, instead, it should be
- * customized implemented and set to node->level then pass to the customized
- * skiplist_insert function.
- *
- * Levels start at zero and go up to (NUM_SKIPLIST_LEVEL -1)
- *
- * NUM_SKIPLIST_LEVEL in this implementation is 8 instead of origin 16,
- * considering that there will be 256 entries to enable the top level when using
- * random level p=0.5, and that number is more than enough for a run queue usage
- * in a scheduler usage. And it also help to reduce the memory usage of the
- * embedded skip list node in task_struct to about 50%.
- *
- * The insertion routine has been implemented so as to use the
- * dirty hack described in the CACM paper: if a random level is
- * generated that is more than the current maximum level, the
- * current maximum level plus one is used instead.
- *
- * BFS Notes: In this implementation of skiplists, there are bidirectional
- * next/prev pointers and the insert function returns a pointer to the actual
- * node the value is stored. The key here is chosen by the scheduler so as to
- * sort tasks according to the priority list requirements and is no longer used
- * by the scheduler after insertion. The scheduler lookup, however, occurs in
- * O(1) time because it is always the first item in the level 0 linked list.
- * Since the task struct stores a copy of the node pointer upon skiplist_insert,
- * it can also remove it much faster than the original implementation with the
- * aid of prev<->next pointer manipulation and no searching.
- */
-#ifndef _LINUX_SKIP_LIST_H
-#define _LINUX_SKIP_LIST_H
-
-#include <linux/kernel.h>
-
-#define NUM_SKIPLIST_LEVEL (4)
-
-struct skiplist_node {
-	int level;	/* Levels in this node */
-	struct skiplist_node *next[NUM_SKIPLIST_LEVEL];
-	struct skiplist_node *prev[NUM_SKIPLIST_LEVEL];
-};
-
-#define SKIPLIST_NODE_INIT(name) { 0,\
-				   {&name, &name, &name, &name},\
-				   {&name, &name, &name, &name},\
-				 }
-
-/**
- * INIT_SKIPLIST_NODE -- init a skiplist_node, expecially for header
- * @node: the skip list node to be inited.
- */
-static inline void INIT_SKIPLIST_NODE(struct skiplist_node *node)
-{
-	int i;
-
-	node->level = 0;
-	for (i = 0; i < NUM_SKIPLIST_LEVEL; i++) {
-		WRITE_ONCE(node->next[i], node);
-		node->prev[i] = node;
-	}
-}
-
-/**
- * skiplist_entry - get the struct for this entry
- * @ptr: the &struct skiplist_node pointer.
- * @type:       the type of the struct this is embedded in.
- * @member:     the name of the skiplist_node within the struct.
- */
-#define skiplist_entry(ptr, type, member) \
-	container_of(ptr, type, member)
-
-/**
- * DEFINE_SKIPLIST_INSERT_FUNC -- macro to define a customized skip list insert
- * function, which takes two parameters, first one is the header node of the
- * skip list, second one is the skip list node to be inserted
- * @func_name: the customized skip list insert function name
- * @search_func: the search function to be used, which takes two parameters,
- * 1st one is the itrator of skiplist_node in the list, the 2nd is the skip list
- * node to be inserted, the function should return true if search should be
- * continued, otherwise return false.
- * Returns 1 if @node is inserted as the first item of skip list at level zero,
- * otherwise 0
- */
-#define DEFINE_SKIPLIST_INSERT_FUNC(func_name, search_func)\
-static inline int func_name(struct skiplist_node *head, struct skiplist_node *node)\
-{\
-	struct skiplist_node *p, *q;\
-	unsigned int k = head->level;\
-	unsigned int l = node->level;\
-\
-	p = head;\
-	if (l > k) {\
-		l = node->level = ++head->level;\
-\
-		node->next[l] = head;\
-		node->prev[l] = head;\
-		head->next[l] = node;\
-		head->prev[l] = node;\
-\
-		do {\
-			while (q = p->next[k], q != head && search_func(q, node))\
-				p = q;\
-\
-			node->prev[k] = p;\
-			node->next[k] = q;\
-			q->prev[k] = node;\
-			p->next[k] = node;\
-		} while (k--);\
-\
-		return (p == head);\
-	}\
-\
-	while (k > l) {\
-		while (q = p->next[k], q != head && search_func(q, node))\
-			p = q;\
-		k--;\
-	}\
-\
-	do {\
-		while (q = p->next[k], q != head && search_func(q, node))\
-			p = q;\
-\
-		node->prev[k] = p;\
-		node->next[k] = q;\
-		q->prev[k] = node;\
-		p->next[k] = node;\
-	} while (k--);\
-\
-	return (p == head);\
-}
-
-/**
- * skiplist_del_init -- delete skip list node from a skip list and reset it's
- * init state
- * @head: the header node of the skip list to be deleted from.
- * @node: the skip list node to be deleted, the caller need to ensure @node is
- * in skip list which @head represent.
- * Returns 1 if @node is the first item of skip level at level zero, otherwise 0
- */
-static inline int
-skiplist_del_init(struct skiplist_node *head, struct skiplist_node *node)
-{
-	unsigned int i, level = node->level;
-
-	for (i = 0; i <= level; i++) {
-		node->prev[i]->next[i] = node->next[i];
-		node->next[i]->prev[i] = node->prev[i];
-	}
-	if (level == head->level && level) {
-		while (head->next[level] == head && level)
-			level--;
-		head->level = level;
-	}
-
-	return (node->prev[0] == head);
-}
-#endif /* _LINUX_SKIP_LIST_H */
diff --git a/init/init_task.c b/init/init_task.c
index 56824e06305c..bdade0709020 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -107,8 +107,7 @@ struct task_struct init_task
 #endif
 #ifdef CONFIG_SCHED_PDS
 	.deadline	= 0,
-	.sl_level	= 0,
-	.sl_node	= SKIPLIST_NODE_INIT(init_task.sl_node),
+	.sq_node	= LIST_HEAD_INIT(init_task.sq_node),
 #endif
 	.time_slice	= HZ,
 #else
diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 3680162d8d19..01abbf28670f 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -509,6 +509,7 @@ static inline void update_rq_clock(struct rq *rq)
 	if (unlikely(delta <= 0))
 		return;
 	rq->clock += delta;
+	update_rq_time_edge(rq);
 	update_rq_clock_task(rq, delta);
 }
 
@@ -3815,7 +3816,15 @@ void alt_sched_debug(void)
 	       sched_sg_idle_mask.bits[0]);
 }
 #else
-inline void alt_sched_debug(void) {}
+int alt_debug[20];
+
+inline void alt_sched_debug(void)
+{
+	int i;
+
+	for (i = 0; i < 3; i++)
+		printk(KERN_INFO "sched: %d\n", alt_debug[i]);
+}
 #endif
 
 #ifdef	CONFIG_SMP
diff --git a/kernel/sched/alt_sched.h b/kernel/sched/alt_sched.h
index ac11555ba4f1..cfb4669dfbbf 100644
--- a/kernel/sched/alt_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -147,7 +147,8 @@ struct rq {
 	struct bmq queue;
 #endif
 #ifdef CONFIG_SCHED_PDS
-	struct skiplist_node sl_header;
+	struct sched_queue	queue;
+	u64			time_edge;
 #endif
 	unsigned long watermark;
 
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index 623908cf4380..3afc6fd7a27f 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -6,4 +6,9 @@
 #define SCHED_BITS	(MAX_RT_PRIO + NICE_WIDTH / 2 + 1)
 #define IDLE_TASK_SCHED_PRIO	(SCHED_BITS - 1)
 
+struct sched_queue {
+	DECLARE_BITMAP(bitmap, SCHED_BITS);
+	struct list_head heads[SCHED_BITS];
+};
+
 #endif
diff --git a/kernel/sched/pds_imp.h b/kernel/sched/pds_imp.h
index 335ce3a8e3ec..35886852c71a 100644
--- a/kernel/sched/pds_imp.h
+++ b/kernel/sched/pds_imp.h
@@ -11,26 +11,7 @@ static const u64 user_prio2deadline[NICE_WIDTH] = {
 /*  15 */	117870029, 129657031, 142622734, 156885007, 172573507
 };
 
-static const unsigned char dl_level_map[] = {
-/*       0               4               8              12           */
-	19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18,
-/*      16              20              24              28           */
-	18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17,
-/*      32              36              40              44           */
-	17, 17, 17, 17, 16, 16, 16, 16, 16, 16, 16, 16, 15, 15, 15, 15,
-/*      48              52              56              60           */
-	15, 15, 15, 14, 14, 14, 14, 14, 14, 13, 13, 13, 13, 12, 12, 12,
-/*      64              68              72              76           */
-	12, 11, 11, 11, 10, 10, 10,  9,  9,  8,  7,  6,  5,  4,  3,  2,
-/*      80              84              88              92           */
-	 1,  0
-};
-
-/* DEFAULT_SCHED_PRIO:
- * dl_level_map[(user_prio2deadline[39] - user_prio2deadline[0]) >> 21] =
- * dl_level_map[68] =
- * 10
- */
+#define SCHED_PRIO_SLOT		(4ULL << 20)
 #define DEFAULT_SCHED_PRIO (MAX_RT_PRIO + 10)
 
 static inline int normal_prio(struct task_struct *p)
@@ -41,21 +22,46 @@ static inline int normal_prio(struct task_struct *p)
 	return MAX_RT_PRIO;
 }
 
+extern int alt_debug[20];
+
 static inline int
-task_sched_prio(const struct task_struct *p, const struct rq *rq)
+task_sched_prio_normal(const struct task_struct *p, const struct rq *rq)
 {
-	size_t delta;
+	int delta;
+
+	delta = rq->time_edge + 20 - (p->deadline >> 23);
+	if (delta < 0) {
+		delta = 0;
+		alt_debug[0]++;
+	}
+	delta = 19 - min(delta, 19);
+
+	return delta;
+}
 
+static inline int
+task_sched_prio(const struct task_struct *p, const struct rq *rq)
+{
 	if (p == rq->idle)
 		return IDLE_TASK_SCHED_PRIO;
 
 	if (p->prio < MAX_RT_PRIO)
 		return p->prio;
 
-	delta = (rq->clock + user_prio2deadline[39] - p->deadline) >> 21;
-	delta = min((size_t)delta, ARRAY_SIZE(dl_level_map) - 1);
+	return MAX_RT_PRIO + task_sched_prio_normal(p, rq);
+}
+
+static inline int
+task_sched_prio_idx(const struct task_struct *p, const struct rq *rq)
+{
+	if (p == rq->idle)
+		return IDLE_TASK_SCHED_PRIO;
 
-	return MAX_RT_PRIO + dl_level_map[delta];
+	if (p->prio < MAX_RT_PRIO)
+		return p->prio;
+
+	return MAX_RT_PRIO +
+		(task_sched_prio_normal(p, rq) + rq->time_edge) % 20;
 }
 
 int task_running_nice(struct task_struct *p)
@@ -68,6 +74,53 @@ static inline void update_task_priodl(struct task_struct *p)
 	p->priodl = (((u64) (p->prio))<<56) | ((p->deadline)>>8);
 }
 
+
+DECLARE_BITMAP(normal_mask, SCHED_BITS);
+
+static inline void sched_shift_normal_bitmap(unsigned long *mask, unsigned int shift)
+{
+	DECLARE_BITMAP(normal, SCHED_BITS);
+
+	bitmap_and(normal, mask, normal_mask, SCHED_BITS);
+	bitmap_shift_right(normal, normal, shift, SCHED_BITS);
+	bitmap_and(normal, normal, normal_mask, SCHED_BITS);
+
+	bitmap_andnot(mask, mask, normal_mask, SCHED_BITS);
+	bitmap_or(mask, mask, normal, SCHED_BITS);
+}
+
+static inline void update_rq_time_edge(struct rq *rq)
+{
+	struct list_head head;
+	u64 old = rq->time_edge;
+	u64 now = rq->clock >> 23;
+	u64 prio, delta = min(20ULL, now - old);
+
+	if (now == old)
+		return;
+
+	INIT_LIST_HEAD(&head);
+
+	prio = MAX_RT_PRIO;
+	for_each_set_bit_from(prio, rq->queue.bitmap, MAX_RT_PRIO + delta) {
+		u64 idx;
+
+		idx = MAX_RT_PRIO + ((prio - MAX_RT_PRIO) + rq->time_edge) % 20;
+		list_splice_tail_init(rq->queue.heads + idx, &head);
+	}
+	sched_shift_normal_bitmap(rq->queue.bitmap, delta);
+	rq->time_edge = now;
+	if (!list_empty(&head)) {
+		struct task_struct *p;
+
+		list_for_each_entry(p, &head, sq_node)
+			p->sq_idx = MAX_RT_PRIO + now % 20;
+
+		list_splice(&head, rq->queue.heads + MAX_RT_PRIO + now % 20);
+		set_bit(MAX_RT_PRIO, rq->queue.bitmap);
+	}
+}
+
 static inline void requeue_task(struct task_struct *p, struct rq *rq);
 
 static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
@@ -77,40 +130,25 @@ static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
 
 	if (p->prio >= MAX_RT_PRIO)
 		p->deadline = rq->clock +
-			user_prio2deadline[p->static_prio - MAX_RT_PRIO];
+			SCHED_PRIO_SLOT * (p->static_prio - MAX_RT_PRIO + 1);
 	update_task_priodl(p);
 
 	if (SCHED_FIFO != p->policy && task_on_rq_queued(p))
 		requeue_task(p, rq);
 }
 
-/*
- * pds_skiplist_task_search -- search function used in PDS run queue skip list
- * node insert operation.
- * @it: iterator pointer to the node in the skip list
- * @node: pointer to the skiplist_node to be inserted
- *
- * Returns true if key of @it is less or equal to key value of @node, otherwise
- * false.
- */
-static inline bool
-pds_skiplist_task_search(struct skiplist_node *it, struct skiplist_node *node)
-{
-	return (skiplist_entry(it, struct task_struct, sl_node)->priodl <=
-		skiplist_entry(node, struct task_struct, sl_node)->priodl);
-}
-
-/*
- * Define the skip list insert function for PDS
- */
-DEFINE_SKIPLIST_INSERT_FUNC(pds_skiplist_insert, pds_skiplist_task_search);
-
 /*
  * Init the queue structure in rq
  */
 static inline void sched_queue_init(struct rq *rq)
 {
-	INIT_SKIPLIST_NODE(&rq->sl_header);
+	struct sched_queue *q = &rq->queue;
+	int i;
+
+	bitmap_set(normal_mask, MAX_RT_PRIO, 20);
+	bitmap_zero(q->bitmap, SCHED_BITS);
+	for(i = 0; i < SCHED_BITS; i++)
+		INIT_LIST_HEAD(&q->heads[i]);
 }
 
 /*
@@ -119,19 +157,33 @@ static inline void sched_queue_init(struct rq *rq)
  */
 static inline void sched_queue_init_idle(struct rq *rq, struct task_struct *idle)
 {
+	struct sched_queue *q = &rq->queue;
 	/*printk(KERN_INFO "sched: init(%d) - %px\n", cpu_of(rq), idle);*/
-	int default_prio = idle->prio;
 
-	idle->prio = MAX_PRIO;
-	idle->deadline = 0ULL;
-	update_task_priodl(idle);
+	idle->sq_idx = IDLE_TASK_SCHED_PRIO;
+	INIT_LIST_HEAD(&q->heads[idle->sq_idx]);
+	list_add(&idle->sq_node, &q->heads[idle->sq_idx]);
+	set_bit(idle->sq_idx, q->bitmap);
+}
 
-	INIT_SKIPLIST_NODE(&rq->sl_header);
+static inline unsigned long sched_prio2idx(unsigned long idx, struct rq *rq)
+{
+	if (IDLE_TASK_SCHED_PRIO == idx ||
+	    idx < MAX_RT_PRIO)
+		return idx;
 
-	idle->sl_node.level = idle->sl_level;
-	pds_skiplist_insert(&rq->sl_header, &idle->sl_node);
+	return MAX_RT_PRIO +
+		((idx - MAX_RT_PRIO) + rq->time_edge) % 20;
+}
 
-	idle->prio = default_prio;
+static inline unsigned long sched_idx2prio(unsigned long idx, struct rq *rq)
+{
+	if (IDLE_TASK_SCHED_PRIO == idx ||
+	    idx < MAX_RT_PRIO)
+		return idx;
+
+	return MAX_RT_PRIO +
+		((idx - MAX_RT_PRIO) + 20 -  rq->time_edge % 20) % 20;
 }
 
 /*
@@ -139,107 +191,99 @@ static inline void sched_queue_init_idle(struct rq *rq, struct task_struct *idle
  */
 static inline struct task_struct *sched_rq_first_task(struct rq *rq)
 {
-	struct skiplist_node *node = rq->sl_header.next[0];
+	unsigned long idx = find_first_bit(rq->queue.bitmap, SCHED_BITS);
+	const struct list_head *head = &rq->queue.heads[sched_prio2idx(idx, rq)];
 
-	BUG_ON(node == &rq->sl_header);
-	return skiplist_entry(node, struct task_struct, sl_node);
+	/*
+	if (list_empty(head)) {
+		pr_err("BUG: cpu%d(time_edge%llu) prio%lu idx%lu mismatched\n",
+		       rq->cpu, rq->time_edge, idx, sched_prio2idx(idx, rq));
+		BUG();
+	}*/
+	return list_first_entry(head, struct task_struct, sq_node);
 }
 
 static inline struct task_struct *
 sched_rq_next_task(struct task_struct *p, struct rq *rq)
 {
-	struct skiplist_node *next = p->sl_node.next[0];
+	unsigned long idx = p->sq_idx;
+	struct list_head *head = &rq->queue.heads[idx];
+
+	if (list_is_last(&p->sq_node, head)) {
+		idx = find_next_bit(rq->queue.bitmap, SCHED_BITS,
+				    sched_idx2prio(idx, rq) + 1);
+		head = &rq->queue.heads[sched_prio2idx(idx, rq)];
+
+		return list_first_entry(head, struct task_struct, sq_node);
+	}
 
-	BUG_ON(next == &rq->sl_header);
-	return skiplist_entry(next, struct task_struct, sl_node);
+	return list_next_entry(p, sq_node);
 }
 
 static inline unsigned long sched_queue_watermark(struct rq *rq)
 {
-	return task_sched_prio(sched_rq_first_task(rq), rq);
+	return find_first_bit(rq->queue.bitmap, SCHED_BITS);
 }
 
 #define __SCHED_DEQUEUE_TASK(p, rq, flags, func)		\
 	psi_dequeue(p, flags & DEQUEUE_SLEEP);			\
 	sched_info_dequeued(rq, p);				\
 								\
-	if (skiplist_del_init(&rq->sl_header, &p->sl_node)) {	\
+	list_del(&p->sq_node);					\
+	if (list_empty(&rq->queue.heads[p->sq_idx])) {		\
+		clear_bit(sched_idx2prio(p->sq_idx, rq),	\
+			  rq->queue.bitmap);			\
 		func;						\
-	}
+	}							\
+	/*\
+	pr_info("-->: cpu%d(time_edge%llu) prio%lu idx%u\n",	\
+		rq->cpu, rq->time_edge, sched_idx2prio(p->sq_idx, rq), p->sq_idx);	\
+	*/
 
 #define __SCHED_ENQUEUE_TASK(p, rq, flags)				\
 	sched_info_queued(rq, p);					\
 	psi_enqueue(p, flags);						\
 									\
-	p->sl_node.level = p->sl_level;					\
-	pds_skiplist_insert(&rq->sl_header, &p->sl_node)
+	p->sq_idx = task_sched_prio_idx(p, rq);				\
+	list_add_tail(&p->sq_node, &rq->queue.heads[p->sq_idx]);	\
+	set_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);				\
+	/*\
+	pr_info("<--: cpu%d(time_edge%llu) prio%lu idx%u\n",	\
+		rq->cpu, rq->time_edge, sched_idx2prio(p->sq_idx, rq), p->sq_idx);	\
+	*/
 
 /*
  * Requeue a task @p to @rq
  */
 #define __SCHED_REQUEUE_TASK(p, rq, func)					\
 {\
-	bool b_first = skiplist_del_init(&rq->sl_header, &p->sl_node);		\
+	int idx = task_sched_prio_idx(p, rq);					\
 \
-	p->sl_node.level = p->sl_level;						\
-	if (pds_skiplist_insert(&rq->sl_header, &p->sl_node) || b_first) {	\
+	list_del(&p->sq_node);							\
+	list_add_tail(&p->sq_node, &rq->queue.heads[idx]);			\
+	if (idx != p->sq_idx) {						\
+		if (list_empty(&rq->queue.heads[p->sq_idx]))			\
+			clear_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);		\
+		p->sq_idx = idx;						\
+		set_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);				\
 		func;								\
+		/*\
+		pr_info("<->: cpu%d(time_edge%llu) prio%lu idx%u\n",	\
+			rq->cpu, rq->time_edge, sched_idx2prio(p->sq_idx, rq), p->sq_idx);	\
+		*/\
 	}									\
 }
 
 static inline bool sched_task_need_requeue(struct task_struct *p, struct rq *rq)
 {
-	struct skiplist_node *node;
-
-	node = p->sl_node.prev[0];
-	if (node != &rq->sl_header &&
-	    skiplist_entry(node, struct task_struct, sl_node)->priodl > p->priodl)
-		return true;
-
-	node = p->sl_node.next[0];
-	if (node != &rq->sl_header &&
-	    skiplist_entry(node, struct task_struct, sl_node)->priodl < p->priodl)
-		return true;
-
-	return false;
-}
-
-/*
- * pds_skiplist_random_level -- Returns a pseudo-random level number for skip
- * list node which is used in PDS run queue.
- *
- * __ffs() is used to satisfy p = 0.5 between each levels, and there should be
- * platform instruction(known as ctz/clz) for acceleration.
- *
- * The skiplist level for a task is populated when task is created and doesn't
- * change in task's life time. When task is being inserted into run queue, this
- * skiplist level is set to task's sl_node->level, the skiplist insert function
- * may change it based on current level of the skip lsit.
- */
-static inline int pds_skiplist_random_level(const struct task_struct *p)
-{
-	/*
-	 * 1. Some architectures don't have better than microsecond resolution
-	 * so mask out ~microseconds as a factor of the random seed for skiplist
-	 * insertion.
-	 * 2. Use address of task structure pointer as another factor of the
-	 * random seed for task burst forking scenario.
-	 */
-	unsigned long randseed = (task_rq(p)->clock ^ (unsigned long)p) >> 10;
-
-	randseed &= __GENMASK(NUM_SKIPLIST_LEVEL - 1, 0);
-	if (randseed)
-		return __ffs(randseed);
-
-	return (NUM_SKIPLIST_LEVEL - 1);
+	return (task_sched_prio_idx(p, rq) != p->sq_idx);
 }
 
 static void sched_task_fork(struct task_struct *p, struct rq *rq)
 {
-	p->sl_level = pds_skiplist_random_level(p);
 	if (p->prio >= MAX_RT_PRIO)
 		p->deadline = rq->clock +
-			user_prio2deadline[p->static_prio - MAX_RT_PRIO];
+			SCHED_PRIO_SLOT * (p->static_prio - MAX_RT_PRIO + 1);
 	update_task_priodl(p);
 }
 
@@ -261,9 +305,10 @@ int task_prio(const struct task_struct *p)
 	if (p->prio < MAX_RT_PRIO)
 		return (p->prio - MAX_RT_PRIO);
 
-	preempt_disable();
-	ret = task_sched_prio(p, this_rq()) - MAX_RT_PRIO;
-	preempt_enable();
+	/*preempt_disable();
+	ret = task_sched_prio(p, task_rq(p)) - MAX_RT_PRIO;*/
+	ret = p->static_prio - MAX_RT_PRIO;
+	/*preempt_enable();*/
 
 	return ret;
 }
-- 
2.33.1.711.g9d530dc002

