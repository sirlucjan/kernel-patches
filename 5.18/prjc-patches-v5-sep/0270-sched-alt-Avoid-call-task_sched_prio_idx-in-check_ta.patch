From e337fcee5d097193bffdebc3ba55d64e461052e7 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Sat, 2 Apr 2022 16:07:47 +0000
Subject: [PATCH 270/285] sched/alt: Avoid call task_sched_prio_idx() in
 check_task_changed() code path.

---
 kernel/sched/alt_core.c | 14 ++++++--------
 kernel/sched/bmq.h      |  2 +-
 kernel/sched/pds.h      |  2 +-
 3 files changed, 8 insertions(+), 10 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index abd0f2bc531e..a3b1d8bbe53d 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -78,7 +78,7 @@ __read_mostly int sysctl_resched_latency_warn_once = 1;
 /* Default time slice is 4 in ms, can be set via kernel parameter "sched_timeslice" */
 u64 sched_timeslice_ns __read_mostly = (4 << 20);
 
-static inline void requeue_task(struct task_struct *p, struct rq *rq);
+static inline void requeue_task(struct task_struct *p, struct rq *rq, int idx);
 
 #ifdef CONFIG_SCHED_BMQ
 #include "bmq.h"
@@ -784,17 +784,13 @@ static inline void enqueue_task(struct task_struct *p, struct rq *rq, int flags)
 	sched_update_tick_dependency(rq);
 }
 
-static inline void requeue_task(struct task_struct *p, struct rq *rq)
+static inline void requeue_task(struct task_struct *p, struct rq *rq, int idx)
 {
-	int idx;
-
 	lockdep_assert_held(&rq->lock);
 	/*printk(KERN_INFO "sched: requeue(%d) %px %016llx\n", cpu_of(rq), p, p->priodl);*/
 	WARN_ONCE(task_rq(p) != rq, "sched: cpu[%d] requeue task reside on cpu%d\n",
 		  cpu_of(rq), task_cpu(p));
 
-	idx = task_sched_prio_idx(p, rq);
-
 	list_del(&p->sq_node);
 	list_add_tail(&p->sq_node, &rq->queue.heads[idx]);
 	if (idx != p->sq_idx) {
@@ -5034,9 +5030,11 @@ EXPORT_SYMBOL(default_wake_function);
 
 static inline void check_task_changed(struct task_struct *p, struct rq *rq)
 {
+	int idx;
+
 	/* Trigger resched if task sched_prio has been modified. */
-	if (task_on_rq_queued(p) && task_sched_prio_idx(p, rq) != p->sq_idx) {
-		requeue_task(p, rq);
+	if (task_on_rq_queued(p) && (idx = task_sched_prio_idx(p, rq)) != p->sq_idx) {
+		requeue_task(p, rq, idx);
 		check_preempt_curr(rq);
 	}
 }
diff --git a/kernel/sched/bmq.h b/kernel/sched/bmq.h
index be3ee4a553ca..bf7ac80ec242 100644
--- a/kernel/sched/bmq.h
+++ b/kernel/sched/bmq.h
@@ -72,7 +72,7 @@ static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
 	if (SCHED_FIFO != p->policy && task_on_rq_queued(p)) {
 		if (SCHED_RR != p->policy)
 			deboost_task(p);
-		requeue_task(p, rq);
+		requeue_task(p, rq, task_sched_prio_idx(p, rq));
 	}
 }
 
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index 0f1f0d708b77..56a649d02e49 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -101,7 +101,7 @@ static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
 	p->time_slice = sched_timeslice_ns;
 	sched_renew_deadline(p, rq);
 	if (SCHED_FIFO != p->policy && task_on_rq_queued(p))
-		requeue_task(p, rq);
+		requeue_task(p, rq, task_sched_prio_idx(p, rq));
 }
 
 static inline void sched_task_sanity_check(struct task_struct *p, struct rq *rq)
-- 
2.36.1.74.g277cf0bc36

