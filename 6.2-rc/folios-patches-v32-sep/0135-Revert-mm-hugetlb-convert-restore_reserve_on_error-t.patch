From 1ad05ee4dd772ba2b3fd11f347cc9d8e4dcec593 Mon Sep 17 00:00:00 2001
From: Piotr Gorski <lucjan.lucjanov@gmail.com>
Date: Tue, 10 Jan 2023 22:48:19 +0100
Subject: [PATCH 135/273] Revert "mm/hugetlb: convert
 restore_reserve_on_error() to folios"

This reverts commit 8940ad49b4d166666142eb1f6f026eb46fe8d09c.

Signed-off-by: Piotr Gorski <lucjan.lucjanov@gmail.com>
---
 mm/hugetlb.c | 27 +++++++++++++--------------
 1 file changed, 13 insertions(+), 14 deletions(-)

diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index ede6ee777..60d0628d8 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -2826,23 +2826,22 @@ static long vma_del_reservation(struct hstate *h,
 void restore_reserve_on_error(struct hstate *h, struct vm_area_struct *vma,
 			unsigned long address, struct page *page)
 {
-	struct folio *folio = page_folio(page);
 	long rc = vma_needs_reservation(h, vma, address);
 
-	if (folio_test_hugetlb_restore_reserve(folio)) {
+	if (HPageRestoreReserve(page)) {
 		if (unlikely(rc < 0))
 			/*
 			 * Rare out of memory condition in reserve map
-			 * manipulation.  Clear hugetlb_restore_reserve so
-			 * that global reserve count will not be incremented
+			 * manipulation.  Clear HPageRestoreReserve so that
+			 * global reserve count will not be incremented
 			 * by free_huge_page.  This will make it appear
-			 * as though the reservation for this folio was
+			 * as though the reservation for this page was
 			 * consumed.  This may prevent the task from
-			 * faulting in the folio at a later time.  This
+			 * faulting in the page at a later time.  This
 			 * is better than inconsistent global huge page
 			 * accounting of reserve counts.
 			 */
-			folio_clear_hugetlb_restore_reserve(folio);
+			ClearHPageRestoreReserve(page);
 		else if (rc)
 			(void)vma_add_reservation(h, vma, address);
 		else
@@ -2853,7 +2852,7 @@ void restore_reserve_on_error(struct hstate *h, struct vm_area_struct *vma,
 			 * This indicates there is an entry in the reserve map
 			 * not added by alloc_huge_page.  We know it was added
 			 * before the alloc_huge_page call, otherwise
-			 * hugetlb_restore_reserve would be set on the folio.
+			 * HPageRestoreReserve would be set on the page.
 			 * Remove the entry so that a subsequent allocation
 			 * does not consume a reservation.
 			 */
@@ -2862,12 +2861,12 @@ void restore_reserve_on_error(struct hstate *h, struct vm_area_struct *vma,
 				/*
 				 * VERY rare out of memory condition.  Since
 				 * we can not delete the entry, set
-				 * hugetlb_restore_reserve so that the reserve
-				 * count will be incremented when the folio
+				 * HPageRestoreReserve so that the reserve
+				 * count will be incremented when the page
 				 * is freed.  This reserve will be consumed
 				 * on a subsequent allocation.
 				 */
-				folio_set_hugetlb_restore_reserve(folio);
+				SetHPageRestoreReserve(page);
 		} else if (rc < 0) {
 			/*
 			 * Rare out of memory condition from
@@ -2883,12 +2882,12 @@ void restore_reserve_on_error(struct hstate *h, struct vm_area_struct *vma,
 				/*
 				 * For private mappings, no entry indicates
 				 * a reservation is present.  Since we can
-				 * not add an entry, set hugetlb_restore_reserve
-				 * on the folio so reserve count will be
+				 * not add an entry, set SetHPageRestoreReserve
+				 * on the page so reserve count will be
 				 * incremented when freed.  This reserve will
 				 * be consumed on a subsequent allocation.
 				 */
-				folio_set_hugetlb_restore_reserve(folio);
+				SetHPageRestoreReserve(page);
 		} else
 			/*
 			 * No reservation present, do nothing
-- 
2.39.0.rc2.1.gbd5df96b79

