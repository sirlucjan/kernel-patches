From 0741d859c249e45c491cc9848a75a478eb91f3ad Mon Sep 17 00:00:00 2001
From: Piotr Gorski <lucjan.lucjanov@gmail.com>
Date: Fri, 13 Jan 2023 23:37:21 +0100
Subject: [PATCH 207/256] Revert "mm/hugetlb: convert dequeue_hugetlb_page
 functions to folios"

This reverts commit 362fcd4c5a59cfea9d2cfa9ab46043356e79391a.

Signed-off-by: Piotr Gorski <lucjan.lucjanov@gmail.com>
---
 mm/hugetlb.c | 56 ++++++++++++++++++++++++----------------------------
 1 file changed, 26 insertions(+), 30 deletions(-)

diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index bec8abbe3..204d09ca0 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -1286,33 +1286,32 @@ static void enqueue_hugetlb_folio(struct hstate *h, struct folio *folio)
 	folio_set_hugetlb_freed(folio);
 }
 
-static struct folio *dequeue_hugetlb_folio_node_exact(struct hstate *h,
-								int nid)
+static struct page *dequeue_huge_page_node_exact(struct hstate *h, int nid)
 {
-	struct folio *folio;
+	struct page *page;
 	bool pin = !!(current->flags & PF_MEMALLOC_PIN);
 
 	lockdep_assert_held(&hugetlb_lock);
-	list_for_each_entry(folio, &h->hugepage_freelists[nid], lru) {
-		if (pin && !folio_is_longterm_pinnable(folio))
+	list_for_each_entry(page, &h->hugepage_freelists[nid], lru) {
+		if (pin && !is_longterm_pinnable_page(page))
 			continue;
 
-		if (folio_test_hwpoison(folio))
+		if (PageHWPoison(page))
 			continue;
 
-		list_move(&folio->lru, &h->hugepage_activelist);
-		folio_ref_unfreeze(folio, 1);
-		folio_clear_hugetlb_freed(folio);
+		list_move(&page->lru, &h->hugepage_activelist);
+		set_page_refcounted(page);
+		ClearHPageFreed(page);
 		h->free_huge_pages--;
 		h->free_huge_pages_node[nid]--;
-		return folio;
+		return page;
 	}
 
 	return NULL;
 }
 
-static struct folio *dequeue_hugetlb_folio_nodemask(struct hstate *h, gfp_t gfp_mask,
-							int nid, nodemask_t *nmask)
+static struct page *dequeue_huge_page_nodemask(struct hstate *h, gfp_t gfp_mask, int nid,
+		nodemask_t *nmask)
 {
 	unsigned int cpuset_mems_cookie;
 	struct zonelist *zonelist;
@@ -1325,7 +1324,7 @@ static struct folio *dequeue_hugetlb_folio_nodemask(struct hstate *h, gfp_t gfp_
 retry_cpuset:
 	cpuset_mems_cookie = read_mems_allowed_begin();
 	for_each_zone_zonelist_nodemask(zone, z, zonelist, gfp_zone(gfp_mask), nmask) {
-		struct folio *folio;
+		struct page *page;
 
 		if (!cpuset_zone_allowed(zone, gfp_mask))
 			continue;
@@ -1337,9 +1336,9 @@ static struct folio *dequeue_hugetlb_folio_nodemask(struct hstate *h, gfp_t gfp_
 			continue;
 		node = zone_to_nid(zone);
 
-		folio = dequeue_hugetlb_folio_node_exact(h, node);
-		if (folio)
-			return folio;
+		page = dequeue_huge_page_node_exact(h, node);
+		if (page)
+			return page;
 	}
 	if (unlikely(read_mems_allowed_retry(cpuset_mems_cookie)))
 		goto retry_cpuset;
@@ -1357,7 +1356,7 @@ static struct page *dequeue_huge_page_vma(struct hstate *h,
 				unsigned long address, int avoid_reserve,
 				long chg)
 {
-	struct folio *folio = NULL;
+	struct page *page = NULL;
 	struct mempolicy *mpol;
 	gfp_t gfp_mask;
 	nodemask_t *nodemask;
@@ -1379,24 +1378,22 @@ static struct page *dequeue_huge_page_vma(struct hstate *h,
 	nid = huge_node(vma, address, gfp_mask, &mpol, &nodemask);
 
 	if (mpol_is_preferred_many(mpol)) {
-		folio = dequeue_hugetlb_folio_nodemask(h, gfp_mask,
-							nid, nodemask);
+		page = dequeue_huge_page_nodemask(h, gfp_mask, nid, nodemask);
 
 		/* Fallback to all nodes if page==NULL */
 		nodemask = NULL;
 	}
 
-	if (!folio)
-		folio = dequeue_hugetlb_folio_nodemask(h, gfp_mask,
-							nid, nodemask);
+	if (!page)
+		page = dequeue_huge_page_nodemask(h, gfp_mask, nid, nodemask);
 
-	if (folio && !avoid_reserve && vma_has_reserves(vma, chg)) {
-		folio_set_hugetlb_restore_reserve(folio);
+	if (page && !avoid_reserve && vma_has_reserves(vma, chg)) {
+		SetHPageRestoreReserve(page);
 		h->resv_huge_pages--;
 	}
 
 	mpol_cond_put(mpol);
-	return &folio->page;
+	return page;
 
 err:
 	return NULL;
@@ -2482,13 +2479,12 @@ struct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,
 {
 	spin_lock_irq(&hugetlb_lock);
 	if (available_huge_pages(h)) {
-		struct folio *folio;
+		struct page *page;
 
-		folio = dequeue_hugetlb_folio_nodemask(h, gfp_mask,
-						preferred_nid, nmask);
-		if (folio) {
+		page = dequeue_huge_page_nodemask(h, gfp_mask, preferred_nid, nmask);
+		if (page) {
 			spin_unlock_irq(&hugetlb_lock);
-			return &folio->page;
+			return page;
 		}
 	}
 	spin_unlock_irq(&hugetlb_lock);
-- 
2.39.0.rc2.1.gbd5df96b79

