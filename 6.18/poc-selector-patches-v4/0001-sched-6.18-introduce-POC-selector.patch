From c0e06f44e6191dab8f97c4dc76b65cfcb4611705 Mon Sep 17 00:00:00 2001
From: Piotr Gorski <lucjan.lucjanov@gmail.com>
Date: Tue, 3 Feb 2026 09:02:32 +0100
Subject: [PATCH] sched-6.18: introduce POC selector

Signed-off-by: Piotr Gorski <lucjan.lucjanov@gmail.com>
---
 include/linux/sched/topology.h |  15 ++
 init/Kconfig                   |  13 +
 kernel/sched/fair.c            |  10 +
 kernel/sched/idle.c            |   6 +
 kernel/sched/poc_selector.c    | 437 +++++++++++++++++++++++++++++++++
 kernel/sched/sched.h           |   6 +
 kernel/sched/topology.c        |  19 ++
 7 files changed, 506 insertions(+)
 create mode 100644 kernel/sched/poc_selector.c

diff --git a/include/linux/sched/topology.h b/include/linux/sched/topology.h
index 45c0022b9..bfaae3b7b 100644
--- a/include/linux/sched/topology.h
+++ b/include/linux/sched/topology.h
@@ -68,6 +68,21 @@ struct sched_domain_shared {
 	atomic_t	nr_busy_cpus;
 	int		has_idle_cores;
 	int		nr_idle_scan;
+#ifdef CONFIG_SCHED_POC_SELECTOR
+#define POC_MASK_WORDS_MAX	2	/* up to 128 CPUs per LLC */
+	/*
+	 * POC Selector: per-LLC atomic64 idle masks (cake inspired)
+	 *
+	 * Cacheline-aligned: LOCK-prefixed writes to these bitmaps on
+	 * every idle transition must not invalidate the cache line
+	 * containing nr_busy_cpus / has_idle_cores / nr_idle_scan.
+	 */
+	atomic64_t	poc_idle_cpus[POC_MASK_WORDS_MAX] ____cacheline_aligned;
+	atomic64_t	poc_idle_cores[POC_MASK_WORDS_MAX];	/* physical core idle mask */
+	int			poc_cpu_base;		/* smallest CPU ID in this LLC */
+	int			poc_nr_words;		/* number of active 64-bit words */
+	bool		poc_fast_eligible;	/* true when LLC CPU range fits */
+#endif
 };
 
 struct sched_domain {
diff --git a/init/Kconfig b/init/Kconfig
index cab3ad28c..991fe7f8a 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -1435,6 +1435,19 @@ config SCHED_AUTOGROUP
 	  desktop applications.  Task group autogeneration is currently based
 	  upon task session.
 
+config SCHED_POC_SELECTOR
+	bool "Piece-Of-Cake Fast Idle CPU Selector"
+	depends on SMP
+	default y
+	help
+	  Idle CPU selector using cached bitmasks inspired by the scx_cake BPF
+	  scheduler. Reduces select_idle_cpu overhead by using bitmap scanning.
+
+	  This optimization does not affect scheduler fairness - it only
+	  speeds up the process of finding an idle CPU for task wakeup.
+
+	  If unsure, say Y.
+
 config RELAY
 	bool "Kernel->user space relay support (formerly relayfs)"
 	select IRQ_WORK
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index f0c7c9442..3bb30c386 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -7635,6 +7635,9 @@ static inline int select_idle_smt(struct task_struct *p, struct sched_domain *sd
 
 #endif /* !CONFIG_SCHED_SMT */
 
+#ifdef CONFIG_SCHED_POC_SELECTOR
+#include "poc_selector.c"
+#endif
 /*
  * Scan the LLC domain for idle CPUs; this is dynamically regulated by
  * comparing the average scan cost (tracked in sd->avg_scan_cost) against the
@@ -7646,6 +7649,13 @@ static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, bool
 	int i, cpu, idle_cpu = -1, nr = INT_MAX;
 	struct sched_domain_shared *sd_share;
 
+#ifdef CONFIG_SCHED_POC_SELECTOR
+	/* Try fast path POC Selector first (SMT-aware 2-phase search) */
+	cpu = select_idle_cpu_poc(p, sd, has_idle_core, target);
+	if (cpu >= 0)
+		return cpu;
+#endif
+
 	cpumask_and(cpus, sched_domain_span(sd), p->cpus_ptr);
 
 	if (sched_feat(SIS_UTIL)) {
diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index ac9690805..239ad03af 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -275,6 +275,9 @@ static void do_idle(void)
 	__current_set_polling();
 	tick_nohz_idle_enter();
 
+	/* POC Selector: mark CPU as idle */
+	set_cpu_idle_state(cpu, 1);
+
 	while (!need_resched()) {
 
 		/*
@@ -332,6 +335,9 @@ static void do_idle(void)
 		arch_cpu_idle_exit();
 	}
 
+	/* POC Selector: mark CPU as busy */
+	set_cpu_idle_state(cpu, 0);
+
 	/*
 	 * Since we fell out of the loop above, we know TIF_NEED_RESCHED must
 	 * be set, propagate it into PREEMPT_NEED_RESCHED.
diff --git a/kernel/sched/poc_selector.c b/kernel/sched/poc_selector.c
new file mode 100644
index 000000000..8967b4a41
--- /dev/null
+++ b/kernel/sched/poc_selector.c
@@ -0,0 +1,437 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Piece-Of-Cake (POC) CPU Selector
+ *
+ * Fast idle CPU selector inspired by RitzDaCat's scx_cake scheduler
+ * "Piece of Cake" - making idle CPU search a piece of cake!
+ *
+ * Uses per-LLC atomic64_t bitmask arrays for O(1) idle CPU lookup.
+ * Supports up to POC_MASK_WORDS_MAX * 64 CPUs per LLC (default 256).
+ * Each word count variant is macro-expanded and dispatched via switch
+ * on poc_nr_words, so the compiler fully unrolls each variant.
+ *
+ * When the fast path is not eligible (LLC exceeds the supported range
+ * or affinity restrictions apply), returns -1 to let CFS standard
+ * select_idle_cpu handle it.
+ *
+ * Copyright (C) 2026 Masahito Suzuki
+ */
+
+#ifdef CONFIG_SCHED_POC_SELECTOR
+
+#define SCHED_POC_SELECTOR_AUTHOR   "Masahito Suzuki"
+#define SCHED_POC_SELECTOR_PROGNAME "Piece-Of-Cake (POC) CPU Selector"
+
+#define SCHED_POC_SELECTOR_VERSION  "1.3"
+
+/*
+ * Runtime control: sched_poc_selector
+ * - 1 (default): POC Selector enabled
+ * - 0: POC Selector disabled, fallback to standard select_idle_cpu
+ */
+unsigned int __read_mostly sched_poc_selector = 1;
+
+static DEFINE_PER_CPU(unsigned int, poc_rr_seed);
+
+/*
+ * is_idle_core_poc - Check if all SMT siblings of a CPU are idle
+ * @cpu: CPU number to check
+ * @sd_share: sched_domain_shared containing poc_idle_cpus
+ *
+ * Returns: true if ALL SMT siblings are idle, false otherwise
+ *
+ * Indexes into the correct word of the poc_idle_cpus[] array
+ * for each sibling.
+ */
+static bool is_idle_core_poc(int cpu, struct sched_domain_shared *sd_share)
+{
+	int base = sd_share->poc_cpu_base;
+	int nr_words = sd_share->poc_nr_words;
+	int sibling;
+
+	for_each_cpu(sibling, cpu_smt_mask(cpu)) {
+		int bit  = sibling - base;
+		int word = bit >> 6;
+		int pos  = bit & 63;
+
+		if ((unsigned int)word >= nr_words)
+			return false;
+
+		u64 cpus = (u64)atomic64_read(&sd_share->poc_idle_cpus[word]);
+
+		if (!(cpus & (1ULL << pos)))
+			return false;
+	}
+	return true;
+}
+
+/*
+ * set_cpu_idle_state - Update per-LLC idle masks when CPU goes idle/busy
+ * @cpu: CPU number
+ * @state: 0=busy, 1=idle
+ *
+ * Updates the per-LLC atomic64 idle CPU and core masks using lock-free
+ * atomic64_or/atomic64_andnot operations.  Each CPU only modifies its
+ * own bit within a single word, so no additional locking is required.
+ *
+ * CPUs outside the supported range are silently skipped;
+ * the fast path will not be used for those LLCs anyway.
+ */
+void set_cpu_idle_state(int cpu, int state)
+{
+	struct sched_domain_shared *sd_share;
+
+	scoped_guard(rcu) {
+		sd_share = rcu_dereference(per_cpu(sd_llc_shared, cpu));
+		if (!sd_share)
+			break;
+
+		int bit  = cpu - sd_share->poc_cpu_base;
+		int word = bit >> 6;
+		int pos  = bit & 63;
+
+		if ((unsigned int)word >= sd_share->poc_nr_words)
+			break;
+
+		/* Update logical CPU idle mask */
+		if (state > 0)
+			atomic64_or(1ULL << pos, &sd_share->poc_idle_cpus[word]);
+		else
+			atomic64_andnot(1ULL << pos, &sd_share->poc_idle_cpus[word]);
+
+		/*
+		 * Ensure the CPU mask update is visible before
+		 * reading it back in is_idle_core_poc().
+		 *
+		 * On x86, the preceding LOCK'd atomic64_or/andnot
+		 * already provides full ordering, so this compiles
+		 * to a mere compiler barrier (~0 cyc).  On ARM64
+		 * it emits dmb ish.
+		 */
+		smp_mb__after_atomic();
+
+		/*
+		 * Update physical core idle mask (SMT systems only).
+		 *
+		 * On non-SMT, cpu_smt_mask(cpu) = {cpu} only, so
+		 * poc_idle_cores[] would be an exact copy of
+		 * poc_idle_cpus[].  Skip the redundant LOCK'd atomic.
+		 */
+		if (sched_smt_active()) {
+			int core     = cpumask_first(cpu_smt_mask(cpu));
+			int core_bit = core - sd_share->poc_cpu_base;
+			int core_w   = core_bit >> 6;
+			int core_pos = core_bit & 63;
+
+			if ((unsigned int)core_w < sd_share->poc_nr_words) {
+				if (state > 0 && is_idle_core_poc(cpu, sd_share))
+					atomic64_or(1ULL << core_pos,
+						    &sd_share->poc_idle_cores[core_w]);
+				else
+					atomic64_andnot(1ULL << core_pos,
+							&sd_share->poc_idle_cores[core_w]);
+			}
+		}
+	}
+}
+
+/*
+ * POC_CTZ64 — Portable Count Trailing Zeros (64-bit)
+ *
+ * Three-tier architecture detection:
+ *
+ *   Tier 1: Native hardware CTZ with well-defined zero semantics
+ *     x86-64 + BMI1 (__BMI__): TZCNT — returns 64 for input 0
+ *     ARM64:                   RBIT + CLZ
+ *     RISC-V Zbb:              CTZ instruction
+ *
+ *   Tier 2: x86-64 without BMI1 (Bulldozer, pre-Haswell, etc.)
+ *     BSF is fast (~3 cyc) but UNDEFINED for input 0.
+ *     On AMD Bulldozer: BSF(0) leaves dest register unchanged (stale value).
+ *     On Intel pre-Haswell: BSF(0) is architecturally undefined.
+ *     Wrap with explicit zero check to guarantee returning 64.
+ *
+ *   Tier 3: De Bruijn fallback (BPF, unknown architectures)
+ *     Software multiply + 64-entry table lookup, branchless O(1).
+ */
+
+/* Tier 1: Hardware CTZ — zero-safe by definition */
+#if defined(__x86_64__) && defined(__BMI__)
+#define POC_CTZ64(v) ((int)__builtin_ctzll(v))
+#define POC_CTZ64_NAME "HW (TZCNT)"
+
+#elif defined(__aarch64__)
+#define POC_CTZ64(v) ((int)__builtin_ctzll(v))
+#define POC_CTZ64_NAME "HW (RBIT+CLZ)"
+
+#elif defined(__riscv) && defined(__riscv_zbb)
+#define POC_CTZ64(v) ((int)__builtin_ctzll(v))
+#define POC_CTZ64_NAME "HW (ctz)"
+
+/* Tier 2: x86-64 without BMI1 — BSF is fast but zero is undefined */
+#elif defined(__x86_64__)
+static __always_inline int poc_ctz64_bsf(u64 v)
+{
+	if (unlikely(!v))
+		return 64;
+	return (int)__builtin_ctzll(v);
+}
+#define POC_CTZ64(v) poc_ctz64_bsf(v)
+#define POC_CTZ64_NAME "HW (BSF)"
+
+/* Tier 3: De Bruijn fallback — branchless software CTZ */
+#else
+#define DEBRUIJN_CTZ64_CONST 0x03F79D71B4CA8B09ULL
+static const u8 debruijn_ctz64_tab[64] = {
+	 0,  1, 56,  2, 57, 49, 28,  3,
+	61, 58, 42, 50, 38, 29, 17,  4,
+	62, 47, 59, 36, 45, 43, 51, 22,
+	53, 39, 33, 30, 24, 18, 12,  5,
+	63, 55, 48, 27, 60, 41, 37, 16,
+	46, 35, 44, 21, 52, 32, 23, 11,
+	54, 26, 40, 15, 34, 20, 31, 10,
+	25, 14, 19,  9, 13,  8,  7,  6,
+};
+static __always_inline int debruijn_ctz64(u64 v)
+{
+	if (unlikely(!v))
+		return 64;
+	u64 lsb = v & (-(s64)v);
+	u32 idx = (u32)((lsb * DEBRUIJN_CTZ64_CONST) >> 58);
+	return (int)debruijn_ctz64_tab[idx & 63];
+}
+#define POC_CTZ64(v) debruijn_ctz64(v)
+#define POC_CTZ64_NAME "SW (De Bruijn)"
+
+#endif /* POC_CTZ64 */
+
+/*
+ * POC_PTSELECT — Select position of the j-th set bit in a 64-bit word
+ *
+ * Returns the bit position (0-indexed) of the j-th set bit in v.
+ * Undefined behavior if j >= popcount(v).
+ *
+ *   Tier 1 (x86-64 + BMI2, excluding AMD Zen 1/2 slow microcode PDEP):
+ *     PDEP + TZCNT — 4 instructions total.
+ *     PDEP deposits the j-th source bit at the j-th mask position.
+ *
+ *   Tier 2 (fallback): Iterative bit-clear — O(j) iterations
+ *     Clears the lowest set bit j times, then CTZ on remainder.
+ */
+
+#if defined(__x86_64__) && defined(__BMI2__) && \
+    !defined(__znver1) && !defined(__znver2)
+static __always_inline int poc_ptselect(u64 v, int j)
+{
+	u64 deposited;
+
+	asm("pdep %2, %1, %0" : "=r"(deposited) : "r"(1ULL << j), "rm"(v));
+	return POC_CTZ64(deposited);
+}
+#define POC_PTSELECT(v, j) poc_ptselect(v, j)
+#define POC_PTSELECT_NAME "HW (PDEP)"
+
+/*
+ * Tier 2 (fallback): Iterative bit-clear — O(j) iterations.
+ *   Clears the lowest set bit j times, then returns its position via CTZ.
+ */
+#else
+static __always_inline int poc_ptselect_sw(u64 v, int j)
+{
+	int k;
+
+	for (k = 0; k < j; k++)
+		v &= v - 1;	/* clear lowest set bit */
+	return POC_CTZ64(v);
+}
+#define POC_PTSELECT(v, j) poc_ptselect_sw(v, j)
+#define POC_PTSELECT_NAME "SW (loop)"
+
+#endif /* POC_PTSELECT */
+
+/* Map seed in [0, 2^32) to [0, range) without division — Lemire's fastrange */
+#define POC_FASTRANGE(seed, range) ((u32)(((u64)(seed) * (u32)(range)) >> 32))
+
+/*
+ * poc_ptselect_multi - Select the pick-th idle CPU across multi-word mask
+ * @mask: array of idle bitmask words (snapshot)
+ * @pcnt: pre-computed popcount for each word (avoids redundant hweight64)
+ * @nr_words: number of 64-bit words (compile-time constant 1 or 2)
+ * @pick: 0-indexed selection (must be < total set bits across all words)
+ * @base: smallest CPU ID in this LLC (poc_cpu_base)
+ *
+ * Scans words in order, subtracting each word's popcount from pick
+ * until the target word is found, then uses POC_PTSELECT within it.
+ * N is a compile-time constant, so the loop is fully unrolled.
+ *
+ * Returns: CPU number of the pick-th idle CPU, or -1 if pick is
+ *          out of range (should not happen with correct callers).
+ */
+static __always_inline int poc_ptselect_multi(const u64 *mask, const int *pcnt,
+					      int nr_words, int pick, int base)
+{
+	int i, acc = 0;
+
+	for (i = 0; i < nr_words; i++) {
+		if (pick < acc + pcnt[i])
+			return POC_PTSELECT(mask[i], pick - acc)
+				+ (i << 6) + base;
+		acc += pcnt[i];
+	}
+	return -1;
+}
+
+/*
+ * DEFINE_SELECT_IDLE_CPU_POC - Generate an N-word variant of the fast path
+ *
+ * Each variant is fully unrollable by the compiler because N is a
+ * compile-time literal.
+ *
+ * Three-level fast path using per-LLC atomic64_t mask arrays:
+ *
+ *   Level 0: Saturation check -- snapshot idle masks; 0 means no idle CPUs
+ *   Level 1: Target sticky    -- reuse target if still idle (best locality)
+ *   Level 2: PTSelect RR      -- round-robin distributed idle CPU selection
+ *
+ * Level 2 uses PTSelect with a per-CPU seed to distribute wakeups
+ * across idle CPUs, avoiding thundering-herd on burst wakeups where
+ * multiple tasks would otherwise all pick the same CPU.
+ */
+#define DEFINE_SELECT_IDLE_CPU_POC(N)							\
+static int select_idle_cpu_poc_##N(bool has_idle_core,			\
+				   int target,									\
+				   struct sched_domain_shared *sd_share)		\
+{																\
+	int base = sd_share->poc_cpu_base;							\
+	int tgt_bit = target - base;								\
+	u64 cpu_mask[(N)];											\
+	u64 any = 0;												\
+	int i;														\
+																\
+	/* Level 0: Snapshot & saturation check */					\
+	for (i = 0; i < (N); i++) {									\
+		cpu_mask[i] = (u64)atomic64_read(						\
+				&sd_share->poc_idle_cpus[i]);					\
+		any |= cpu_mask[i];										\
+	}															\
+	if (!any)													\
+		return -1;												\
+																\
+	/* Level 1: Target sticky -- maximize cache locality */		\
+	{															\
+		int w   = tgt_bit >> 6;									\
+		int pos = tgt_bit & 63;									\
+																\
+		if ((unsigned int)w < (N) &&							\
+		    (cpu_mask[w] & (1ULL << pos)))						\
+			return target;										\
+	}															\
+																\
+	/* Level 2: Round-robin distributed idle CPU selection */	\
+	{															\
+		int pcnt[(N)], total = 0;								\
+		unsigned int seed;										\
+		for (i = 0; i < (N); i++) {								\
+			pcnt[i] = hweight64(cpu_mask[i]);					\
+			total += pcnt[i];									\
+		}														\
+		seed = __this_cpu_inc_return(poc_rr_seed);				\
+																\
+		if (has_idle_core && sched_smt_active()) {				\
+			u64 core_mask[(N)];									\
+			int core_pcnt[(N)], core_total = 0, cpu;			\
+			for (i = 0; i < (N); i++) {							\
+				core_mask[i] = (u64)atomic64_read(				\
+					&sd_share->poc_idle_cores[i]);				\
+				core_pcnt[i] = hweight64(core_mask[i]);			\
+				core_total += core_pcnt[i];						\
+			}													\
+			if (core_total > 0) {								\
+				cpu = poc_ptselect_multi(core_mask,				\
+					core_pcnt, (N),								\
+					POC_FASTRANGE(seed, core_total), base);		\
+				if (cpu >= 0)									\
+					return cpu;									\
+			}													\
+		}														\
+		return poc_ptselect_multi(cpu_mask, pcnt, (N),			\
+					POC_FASTRANGE(seed, total), base);			\
+	}															\
+}
+
+DEFINE_SELECT_IDLE_CPU_POC(1)
+DEFINE_SELECT_IDLE_CPU_POC(2)
+
+/*
+ * select_idle_cpu_poc - Fast idle CPU selector (cake-inspired atomic64 path)
+ * @p: task to be placed
+ * @sd: scheduling domain
+ * @has_idle_core: true if there are idle physical cores
+ * @target: preferred target CPU
+ *
+ * Returns: idle CPU number if found, -1 otherwise
+ *
+ * Guard checks are performed here; the actual N-word search is
+ * dispatched via switch on poc_nr_words to a fully-unrolled variant.
+ *
+ * Returns -1 (falls through to CFS standard select_idle_cpu) when:
+ *   - Runtime disabled (sched_poc_selector == 0)
+ *   - LLC exceeds POC_MASK_WORDS_MAX * 64 CPUs
+ *   - Task has affinity restrictions
+ *   - No idle CPUs available
+ */
+static int select_idle_cpu_poc(struct task_struct *p,
+				struct sched_domain *sd,
+				bool has_idle_core,
+				int target)
+{
+	struct sched_domain_shared *sd_share;
+	int nr_words;
+
+	if (!sched_poc_selector)
+		return -1;
+
+	/* Affinity-restricted tasks fall through to CFS standard path */
+	if (unlikely(p->nr_cpus_allowed < sd->span_weight))
+		return -1;
+
+	sd_share = rcu_dereference(per_cpu(sd_llc_shared, target));
+	if (!sd_share || !sd_share->poc_fast_eligible)
+		return -1;
+
+	nr_words = sd_share->poc_nr_words;
+
+	switch (nr_words) {
+	case 1: return select_idle_cpu_poc_1(has_idle_core, target, sd_share);
+	case 2: return select_idle_cpu_poc_2(has_idle_core, target, sd_share);
+	}
+
+	return -1;
+}
+
+#ifdef CONFIG_SYSCTL
+static struct ctl_table sched_poc_sysctls[] = {
+	{
+		.procname	= "sched_poc_selector",
+		.data		= &sched_poc_selector,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= proc_douintvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= SYSCTL_ONE,
+	},
+};
+
+static int __init sched_poc_sysctl_init(void)
+{
+	printk(KERN_INFO "%s %s by %s [CTZ: %s, PTSelect: %s]\n",
+		SCHED_POC_SELECTOR_PROGNAME, SCHED_POC_SELECTOR_VERSION,
+		SCHED_POC_SELECTOR_AUTHOR, POC_CTZ64_NAME, POC_PTSELECT_NAME);
+
+	register_sysctl_init("kernel", sched_poc_sysctls);
+	return 0;
+}
+late_initcall(sched_poc_sysctl_init);
+
+#endif /* CONFIG_SYSCTL */
+#endif /* CONFIG_SCHED_POC_SELECTOR */
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 2f8b06b12..5dda6abb6 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -3141,6 +3141,12 @@ extern void nohz_run_idle_balance(int cpu);
 static inline void nohz_run_idle_balance(int cpu) { }
 #endif
 
+#ifdef CONFIG_SCHED_POC_SELECTOR
+extern void set_cpu_idle_state(int cpu, int state);
+#else
+static inline void set_cpu_idle_state(int cpu, int state) { }
+#endif
+
 #include "stats.h"
 
 #if defined(CONFIG_SCHED_CORE) && defined(CONFIG_SCHEDSTATS)
diff --git a/kernel/sched/topology.c b/kernel/sched/topology.c
index c7a4d2fff..2cb763848 100644
--- a/kernel/sched/topology.c
+++ b/kernel/sched/topology.c
@@ -1723,6 +1723,25 @@ sd_init(struct sched_domain_topology_level *tl,
 		sd->shared = *per_cpu_ptr(sdd->sds, sd_id);
 		atomic_inc(&sd->shared->ref);
 		atomic_set(&sd->shared->nr_busy_cpus, sd_weight);
+
+#ifdef CONFIG_SCHED_POC_SELECTOR
+		int range = cpumask_last(sd_span) - sd_id + 1;
+		int nr_words = DIV_ROUND_UP(range, 64);
+		int i;
+
+		sd->shared->poc_cpu_base = sd_id;
+		if (nr_words <= POC_MASK_WORDS_MAX) {
+			sd->shared->poc_nr_words = nr_words;
+			sd->shared->poc_fast_eligible = true;
+		} else {
+			sd->shared->poc_nr_words = 0;
+			sd->shared->poc_fast_eligible = false;
+		}
+		for (i = 0; i < POC_MASK_WORDS_MAX; i++) {
+			atomic64_set(&sd->shared->poc_idle_cpus[i], 0);
+			atomic64_set(&sd->shared->poc_idle_cores[i], 0);
+		}
+#endif
 	}
 
 	sd->private = sdd;
-- 
2.52.0

