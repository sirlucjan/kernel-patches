From a35c3714e74b689c867b51517c429b83fe91b447 Mon Sep 17 00:00:00 2001
From: K Prateek Nayak <kprateek.nayak@amd.com>
Date: Fri, 26 Apr 2024 12:46:11 +0200
Subject: [PATCH 10/12] sched/eevdf: Limit preemption a little more

XXX changelog goes here

TL;DR, adding tasks to the right of the tree moves avg_vruntime right,
which possibly makes curr non-eligible and causes insta-preemption
irrespective of it only having just landed on the CPU.

Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
---
 kernel/sched/fair.c     | 30 +++++++++++++++++++++---------
 kernel/sched/features.h | 11 ++++++++---
 2 files changed, 29 insertions(+), 12 deletions(-)

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 3054902ad..68b98ea4a 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -887,22 +887,34 @@ static struct sched_entity *pick_eevdf(struct cfs_rq *cfs_rq)
 	struct sched_entity *curr = cfs_rq->curr;
 	struct sched_entity *best = NULL;
 
+	if (curr && !curr->on_rq)
+		curr = NULL;
+
 	/*
 	 * We can safely skip eligibility check if there is only one entity
 	 * in this cfs_rq, saving some cycles.
 	 */
 	if (cfs_rq->nr_running == 1)
-		return curr && curr->on_rq ? curr : se;
+		return curr ?: se;
 
-	if (curr && (!curr->on_rq || !entity_eligible(cfs_rq, curr)))
-		curr = NULL;
+	if (curr) {
+		struct sched_entity *ecurr = curr;
 
-	/*
-	 * Once selected, run a task until it either becomes non-eligible or
-	 * until it gets a new slice. See the HACK in set_next_entity().
-	 */
-	if (sched_feat(RUN_TO_PARITY) && curr && curr->vlag == curr->deadline)
-		return curr;
+		if (!entity_eligible(cfs_rq, curr))
+			ecurr = NULL;
+
+		if (sched_feat(RESPECT_SLICE) &&
+		    !(sched_feat(RUN_TO_PARITY) && !ecurr)) {
+			/*
+			 * Allow current to finish it's slice once it is
+			 * selected. See the HACK in set_next_entity().
+			 */
+			if (curr->vlag == curr->deadline)
+				return curr;
+		}
+
+		curr = ecurr;
+	}
 
 	/* Pick the leftmost entity if it's eligible */
 	if (se && entity_eligible(cfs_rq, se)) {
diff --git a/kernel/sched/features.h b/kernel/sched/features.h
index b0cdff096..f17294bc4 100644
--- a/kernel/sched/features.h
+++ b/kernel/sched/features.h
@@ -10,10 +10,15 @@ SCHED_FEAT(PLACE_LAG, true)
  */
 SCHED_FEAT(PLACE_DEADLINE_INITIAL, true)
 /*
- * Inhibit (wakeup) preemption until the current task has either matched the
- * 0-lag point or until is has exhausted it's slice.
+ * Inhibit preemption until the current task has exhausted it's slice.
  */
-SCHED_FEAT(RUN_TO_PARITY, true)
+SCHED_FEAT(RESPECT_SLICE, true)
+/*
+ * Relax RESPECT_SLICE and only protect current until 0-lag. Notably this can
+ * mean no protection at all if a newly placed task moves avg_vruntime left of
+ * current.
+ */
+SCHED_FEAT(RUN_TO_PARITY, false)
 
 /*
  * Prefer to schedule the task we woke last (assuming it failed
-- 
2.44.0.325.g11c821f2f2

