From bd20f0653e591b21cae0fdf54815fd247bee4518 Mon Sep 17 00:00:00 2001
From: Oleksandr Natalenko <oleksandr@natalenko.name>
Date: Tue, 19 May 2020 12:47:59 +0200
Subject: [PATCH 10/11] mm-5.6: update proactive compaction to v5

Signed-off-by: Oleksandr Natalenko <oleksandr@natalenko.name>
---
 .../admin-guide/mm/proactive-compaction.rst   | 26 ------
 Documentation/admin-guide/sysctl/vm.rst       | 13 +++
 MAINTAINERS                                   |  6 --
 include/linux/compaction.h                    |  1 +
 kernel/sysctl.c                               |  9 ++
 mm/compaction.c                               | 93 +++----------------
 mm/page_alloc.c                               |  1 -
 7 files changed, 35 insertions(+), 114 deletions(-)
 delete mode 100644 Documentation/admin-guide/mm/proactive-compaction.rst

diff --git a/Documentation/admin-guide/mm/proactive-compaction.rst b/Documentation/admin-guide/mm/proactive-compaction.rst
deleted file mode 100644
index 510f47e38..000000000
--- a/Documentation/admin-guide/mm/proactive-compaction.rst
+++ /dev/null
@@ -1,26 +0,0 @@
-.. SPDX-License-Identifier: GPL-2.0
-.. _proactive_compaction:
-
-====================
-Proactive Compaction
-====================
-
-Many applications benefit significantly from the use of huge pages.
-However, huge-page allocations often incur a high latency or even fail
-under fragmented memory conditions. Proactive compaction provides an
-effective solution to these problems by doing memory compaction in the
-background.
-
-The process of proactive compaction is controlled by a single tunable:
-
-        /sys/kernel/mm/compaction/proactiveness
-
-This tunable takes a value in the range [0, 100] with a default value of
-20. This tunable determines how aggressively compaction is done in the
-background. Setting it to 0 disables proactive compaction.
-
-Note that compaction has a non-trivial system-wide impact as pages
-belonging to different processes are moved around, which could also lead
-to latency spikes in unsuspecting applications. The kernel employs
-various heuristics to avoid wasting CPU cycles if it detects that
-proactive compaction is not being effective.
diff --git a/Documentation/admin-guide/sysctl/vm.rst b/Documentation/admin-guide/sysctl/vm.rst
index 0329a4d3f..e5d88cabe 100644
--- a/Documentation/admin-guide/sysctl/vm.rst
+++ b/Documentation/admin-guide/sysctl/vm.rst
@@ -119,6 +119,19 @@ all zones are compacted such that free memory is available in contiguous
 blocks where possible. This can be important for example in the allocation of
 huge pages although processes will also directly compact memory as required.
 
+compaction_proactiveness
+========================
+
+This tunable takes a value in the range [0, 100] with a default value of
+20. This tunable determines how aggressively compaction is done in the
+background. Setting it to 0 disables proactive compaction.
+
+Note that compaction has a non-trivial system-wide impact as pages
+belonging to different processes are moved around, which could also lead
+to latency spikes in unsuspecting applications. The kernel employs
+various heuristics to avoid wasting CPU cycles if it detects that
+proactive compaction is not being effective.
+
 
 compact_unevictable_allowed
 ===========================
diff --git a/MAINTAINERS b/MAINTAINERS
index 5c133a4a0..50659d769 100644
--- a/MAINTAINERS
+++ b/MAINTAINERS
@@ -18742,12 +18742,6 @@ L:	linux-mm@kvack.org
 S:	Maintained
 F:	mm/zswap.c
 
-PROACTIVE COMPACTION
-M:	Nitin Gupta <nigupta@nvidia.com>
-L:	linux-mm@kvack.org
-S:	Maintained
-F:	Documentation/admin-guide/mm/proactive-compaction.rst
-
 THE REST
 M:	Linus Torvalds <torvalds@linux-foundation.org>
 L:	linux-kernel@vger.kernel.org
diff --git a/include/linux/compaction.h b/include/linux/compaction.h
index 47c5df1fa..ccd28978b 100644
--- a/include/linux/compaction.h
+++ b/include/linux/compaction.h
@@ -85,6 +85,7 @@ static inline unsigned long compact_gap(unsigned int order)
 
 #ifdef CONFIG_COMPACTION
 extern int sysctl_compact_memory;
+extern int sysctl_compaction_proactiveness;
 extern int sysctl_compaction_handler(struct ctl_table *table, int write,
 			void __user *buffer, size_t *length, loff_t *ppos);
 extern int sysctl_extfrag_threshold;
diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index 8a176d872..51c90906e 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -1458,6 +1458,15 @@ static struct ctl_table vm_table[] = {
 		.mode		= 0200,
 		.proc_handler	= sysctl_compaction_handler,
 	},
+	{
+		.procname	= "compaction_proactiveness",
+		.data		= &sysctl_compaction_proactiveness,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= &one_hundred,
+	},
 	{
 		.procname	= "extfrag_threshold",
 		.data		= &sysctl_extfrag_threshold,
diff --git a/mm/compaction.c b/mm/compaction.c
index f2c0e78f1..bf7f57a47 100644
--- a/mm/compaction.c
+++ b/mm/compaction.c
@@ -26,16 +26,6 @@
 #include "internal.h"
 
 #ifdef CONFIG_COMPACTION
-/*
- * Tunable for proactive compaction, exposed via sysfs:
- *	/sys/kernel/mm/compaction/proactiveness
- *
- * This tunable determines how aggressively the kernel
- * should compact memory in the background. It takes
- * values in the range [0, 100].
- */
-static unsigned int compaction_proactiveness = 20;
-
 static inline void count_compact_event(enum vm_event_item item)
 {
 	count_vm_event(item);
@@ -1890,10 +1880,8 @@ static int fragmentation_score_zone(struct zone *zone)
 	unsigned long score;
 
 	score = zone->present_pages *
-			extfrag_for_order(zone, HUGETLB_PAGE_ORDER);
-	score = div64_ul(score,
-			node_present_pages(zone->zone_pgdat->node_id) + 1);
-	return score;
+			extfrag_for_order(zone, HPAGE_PMD_ORDER);
+	return div64_ul(score, zone->zone_pgdat->node_present_pages + 1);
 }
 
 /*
@@ -1922,7 +1910,7 @@ static int fragmentation_score_wmark(pg_data_t *pgdat, bool low)
 {
 	int wmark_low;
 
-	wmark_low = 100 - compaction_proactiveness;
+	wmark_low = 100 - sysctl_compaction_proactiveness;
 	return low ? wmark_low : min(wmark_low + 10, 100);
 }
 
@@ -1930,7 +1918,7 @@ static bool should_proactive_compact_node(pg_data_t *pgdat)
 {
 	int wmark_high;
 
-	if (!compaction_proactiveness || kswapd_is_running(pgdat))
+	if (!sysctl_compaction_proactiveness || kswapd_is_running(pgdat))
 		return false;
 
 	wmark_high = fragmentation_score_wmark(pgdat, false);
@@ -2411,7 +2399,6 @@ static enum compact_result compact_zone_order(struct zone *zone, int order,
 		.alloc_flags = alloc_flags,
 		.classzone_idx = classzone_idx,
 		.direct_compaction = true,
-		.proactive_compaction = false,
 		.whole_zone = (prio == MIN_COMPACT_PRIORITY),
 		.ignore_skip_hint = (prio == MIN_COMPACT_PRIORITY),
 		.ignore_block_suitable = (prio == MIN_COMPACT_PRIORITY)
@@ -2534,7 +2521,6 @@ static void proactive_compact_node(pg_data_t *pgdat)
 		.ignore_skip_hint = true,
 		.whole_zone = true,
 		.gfp_mask = GFP_KERNEL,
-		.direct_compaction = false,
 		.proactive_compaction = true,
 	};
 
@@ -2564,10 +2550,9 @@ static void compact_node(int nid)
 		.ignore_skip_hint = true,
 		.whole_zone = true,
 		.gfp_mask = GFP_KERNEL,
-		.direct_compaction = false,
-		.proactive_compaction = false,
 	};
 
+
 	for (zoneid = 0; zoneid < MAX_NR_ZONES; zoneid++) {
 
 		zone = &pgdat->node_zones[zoneid];
@@ -2598,6 +2583,13 @@ static void compact_nodes(void)
 /* The written value is actually unused, all memory is compacted */
 int sysctl_compact_memory;
 
+/*
+ * Tunable for proactive compaction. It determines how
+ * aggressively the kernel should compact memory in the
+ * background. It takes values in the range [0, 100].
+ */
+int sysctl_compaction_proactiveness = 20;
+
 /*
  * This is the entry point for compacting all nodes via
  * /proc/sys/vm/compact_memory
@@ -2640,63 +2632,6 @@ void compaction_unregister_node(struct node *node)
 }
 #endif /* CONFIG_SYSFS && CONFIG_NUMA */
 
-#ifdef CONFIG_SYSFS
-
-#define COMPACTION_ATTR_RO(_name) \
-	static struct kobj_attribute _name##_attr = __ATTR_RO(_name)
-
-#define COMPACTION_ATTR(_name) \
-	static struct kobj_attribute _name##_attr = \
-		__ATTR(_name, 0644, _name##_show, _name##_store)
-
-static struct kobject *compaction_kobj;
-
-static ssize_t proactiveness_store(struct kobject *kobj,
-		struct kobj_attribute *attr, const char *buf, size_t count)
-{
-	int err;
-	unsigned long input;
-
-	err = kstrtoul(buf, 10, &input);
-	if (err)
-		return err;
-	if (input > 100)
-		return -EINVAL;
-
-	compaction_proactiveness = input;
-	return count;
-}
-
-static ssize_t proactiveness_show(struct kobject *kobj,
-		struct kobj_attribute *attr, char *buf)
-{
-	return sprintf(buf, "%u\n", compaction_proactiveness);
-}
-
-COMPACTION_ATTR(proactiveness);
-
-static struct attribute *compaction_attrs[] = {
-	&proactiveness_attr.attr,
-	NULL,
-};
-
-static const struct attribute_group compaction_attr_group = {
-	.attrs = compaction_attrs,
-};
-
-static void __init compaction_sysfs_init(void)
-{
-	compaction_kobj = kobject_create_and_add("compaction", mm_kobj);
-	if (!compaction_kobj)
-		return;
-
-	if (sysfs_create_group(compaction_kobj, &compaction_attr_group)) {
-		kobject_put(compaction_kobj);
-		compaction_kobj = NULL;
-	}
-}
-#endif
-
 static inline bool kcompactd_work_requested(pg_data_t *pgdat)
 {
 	return pgdat->kcompactd_max_order > 0 || kthread_should_stop();
@@ -2737,8 +2672,6 @@ static void kcompactd_do_work(pg_data_t *pgdat)
 		.mode = MIGRATE_SYNC_LIGHT,
 		.ignore_skip_hint = false,
 		.gfp_mask = GFP_KERNEL,
-		.direct_compaction = false,
-		.proactive_compaction = false,
 	};
 	trace_mm_compaction_kcompactd_wake(pgdat->node_id, cc.order,
 							cc.classzone_idx);
@@ -2956,8 +2889,6 @@ static int __init kcompactd_init(void)
 		return ret;
 	}
 
-	compaction_sysfs_init();
-
 	for_each_node_state(nid, N_MEMORY)
 		kcompactd_run(nid);
 	return 0;
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 182abe8a5..325f42f59 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -8488,7 +8488,6 @@ int alloc_contig_range(unsigned long start, unsigned long end,
 		.no_set_skip_hint = true,
 		.gfp_mask = current_gfp_context(gfp_mask),
 		.alloc_contig = true,
-               .proactive_compaction = false,
 	};
 	INIT_LIST_HEAD(&cc.migratepages);
 
-- 
2.27.0.rc1.28.gd2ecc46c09

