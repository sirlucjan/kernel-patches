From 1d86f42968b1acddcc857126d826e20e7a8bb2f9 Mon Sep 17 00:00:00 2001
From: Piotr Gorski <lucjan.lucjanov@gmail.com>
Date: Thu, 15 Jul 2021 15:58:04 +0200
Subject: [PATCH 46/51] Revert "bfq: Limit number of requests consumed by each
 cgroup"

This reverts commit 09e2138dffd0f8ffe4f2c12c86d3d97c87621e3e.

Signed-off-by: Piotr Gorski <lucjan.lucjanov@gmail.com>
---
 block/bfq-iosched.c | 54 +++++++++++++++------------------------------
 1 file changed, 18 insertions(+), 36 deletions(-)

diff --git a/block/bfq-iosched.c b/block/bfq-iosched.c
index ad1d26856..262cd5deb 100644
--- a/block/bfq-iosched.c
+++ b/block/bfq-iosched.c
@@ -574,22 +574,6 @@ static struct request *bfq_choose_req(struct bfq_data *bfqd,
 	}
 }
 
-static bool bfqq_request_over_limit(struct bfq_queue *bfqq, int limit)
-{
-	struct bfq_entity *entity = &bfqq->entity;
-
-	for_each_entity(entity) {
-		if (entity->on_st_or_in_serv &&
-		    entity->allocated >= limit * entity->weight /
-					bfq_entity_service_tree(entity)->wsum) {
-			bfq_log_bfqq(bfqq->bfqd, bfqq, "too many requests: allocated %d limit %d weight %d wsum %lu",
-				entity->allocated, limit, entity->weight, bfq_entity_service_tree(entity)->wsum);
-			return true;
-		}
-	}
-	return false;
-}
-
 /*
  * Async I/O can easily starve sync I/O (both sync reads and sync
  * writes), by consuming all tags. Similarly, storms of sync writes,
@@ -600,28 +584,16 @@ static bool bfqq_request_over_limit(struct bfq_queue *bfqq, int limit)
 static void bfq_limit_depth(unsigned int op, struct blk_mq_alloc_data *data)
 {
 	struct bfq_data *bfqd = data->q->elevator->elevator_data;
-	struct bfq_io_cq *bic = data->icq ? icq_to_bic(data->icq) : NULL;
-	struct bfq_queue *bfqq = bic ? bic_to_bfqq(bic, op_is_sync(op)) : NULL;
-	int depth;
 
-	/* Sync reads have full depth available */
 	if (op_is_sync(op) && !op_is_write(op))
-		depth = 0;
-	else
-		depth = bfqd->word_depths[!!bfqd->wr_busy_queues][op_is_sync(op)];
+		return;
 
-	/*
-	 * Does queue (or any parent entity) exceed number of requests that
-	 * should be available to it? Heavily limit depth so that it cannot
-	 * consume more available requests and thus starve other entities.
-	 */
-	if (bfqq && bfqq_request_over_limit(bfqq, data->q->nr_requests))
-		depth = 1;
+	data->shallow_depth =
+		bfqd->word_depths[!!bfqd->wr_busy_queues][op_is_sync(op)];
 
 	bfq_log(bfqd, "[%s] wr_busy %d sync %d depth %u",
-		__func__, bfqd->wr_busy_queues, op_is_sync(op), depth);
-	if (depth)
-		data->shallow_depth = depth;
+			__func__, bfqd->wr_busy_queues, op_is_sync(op),
+			data->shallow_depth);
 }
 
 static struct bfq_queue *
@@ -6919,8 +6891,11 @@ void bfq_put_async_queues(struct bfq_data *bfqd, struct bfq_group *bfqg)
  * See the comments on bfq_limit_depth for the purpose of
  * the depths set in the function. Return minimum shallow depth we'll use.
  */
-static void bfq_update_depths(struct bfq_data *bfqd, struct sbitmap_queue *bt)
+static unsigned int bfq_update_depths(struct bfq_data *bfqd,
+				      struct sbitmap_queue *bt)
 {
+	unsigned int i, j, min_shallow = UINT_MAX;
+
 	/*
 	 * In-word depths if no bfq_queue is being weight-raised:
 	 * leaving 25% of tags only for sync reads.
@@ -6951,15 +6926,22 @@ static void bfq_update_depths(struct bfq_data *bfqd, struct sbitmap_queue *bt)
 	bfqd->word_depths[1][0] = max(((1U << bt->sb.shift) * 3) >> 4, 1U);
 	/* no more than ~37% of tags for sync writes (~20% extra tags) */
 	bfqd->word_depths[1][1] = max(((1U << bt->sb.shift) * 6) >> 4, 1U);
+
+	for (i = 0; i < 2; i++)
+		for (j = 0; j < 2; j++)
+			min_shallow = min(min_shallow, bfqd->word_depths[i][j]);
+
+	return min_shallow;
 }
 
 static void bfq_depth_updated(struct blk_mq_hw_ctx *hctx)
 {
 	struct bfq_data *bfqd = hctx->queue->elevator->elevator_data;
 	struct blk_mq_tags *tags = hctx->sched_tags;
+	unsigned int min_shallow;
 
-	bfq_update_depths(bfqd, tags->bitmap_tags);
-	sbitmap_queue_min_shallow_depth(tags->bitmap_tags, 1);
+	min_shallow = bfq_update_depths(bfqd, tags->bitmap_tags);
+	sbitmap_queue_min_shallow_depth(tags->bitmap_tags, min_shallow);
 }
 
 static int bfq_init_hctx(struct blk_mq_hw_ctx *hctx, unsigned int index)
-- 
2.32.0.93.g670b81a890

