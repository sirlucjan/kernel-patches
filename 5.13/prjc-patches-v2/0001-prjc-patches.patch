From da5d9fa1996eba61d2bcca4dfeb1f6b1d90d6190 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Tue, 15 Jun 2021 09:48:47 +0000
Subject: [PATCH 01/30] sched/alt: irq save/restore in migration_cpu_stop()

---
 kernel/sched/alt_core.c | 6 +++---
 1 file changed, 3 insertions(+), 3 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 2a485c184832..3680162d8d19 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -1344,12 +1344,13 @@ static int migration_cpu_stop(void *data)
 	struct migration_arg *arg = data;
 	struct task_struct *p = arg->task;
 	struct rq *rq = this_rq();
+	unsigned long flags;
 
 	/*
 	 * The original target CPU might have gone down and we might
 	 * be on another CPU but it doesn't matter.
 	 */
-	local_irq_disable();
+	local_irq_save(flags);
 	/*
 	 * We need to explicitly wake pending tasks before running
 	 * __migrate_task() such that we will not miss enforcing cpus_ptr
@@ -1367,9 +1368,8 @@ static int migration_cpu_stop(void *data)
 	if (task_rq(p) == rq && task_on_rq_queued(p))
 		rq = __migrate_task(rq, p, arg->dest_cpu);
 	raw_spin_unlock(&rq->lock);
-	raw_spin_unlock(&p->pi_lock);
+	raw_spin_unlock_irqrestore(&p->pi_lock, flags);
 
-	local_irq_enable();
 	return 0;
 }
 
-- 
2.32.0.93.g670b81a890


From c9e484053bcf97a359834ddca8a40b8fcf6a401e Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Mon, 17 May 2021 10:25:28 +0000
Subject: [PATCH 02/30] sched/pds: PDS improvement.

PDS uses bitmap queue as queue data structure. Rename maybe needed after
all improvement are done.
---
 include/linux/sched.h     |   7 +-
 include/linux/skip_list.h | 175 ------------------------
 init/init_task.c          |   3 +-
 kernel/sched/alt_core.c   |  11 +-
 kernel/sched/alt_sched.h  |   3 +-
 kernel/sched/pds.h        |   5 +
 kernel/sched/pds_imp.h    | 281 ++++++++++++++++++++++----------------
 7 files changed, 183 insertions(+), 302 deletions(-)
 delete mode 100644 include/linux/skip_list.h

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 4c112a46c88d..3217880daa71 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -34,7 +34,6 @@
 #include <linux/rseq.h>
 #include <linux/seqlock.h>
 #include <linux/kcsan.h>
-#include <linux/skip_list.h>
 #include <asm/kmap_size.h>
 
 /* task_struct member predeclarations (sorted alphabetically): */
@@ -724,10 +723,8 @@ struct task_struct {
 #ifdef CONFIG_SCHED_PDS
 	u64				deadline;
 	u64				priodl;
-	/* skip list level */
-	int				sl_level;
-	/* skip list node */
-	struct skiplist_node		sl_node;
+	int				sq_idx;
+	struct list_head		sq_node;
 #endif /* CONFIG_SCHED_PDS */
 	/* sched_clock time spent running */
 	u64				sched_time;
diff --git a/include/linux/skip_list.h b/include/linux/skip_list.h
deleted file mode 100644
index 637c83ecbd6b..000000000000
--- a/include/linux/skip_list.h
+++ /dev/null
@@ -1,175 +0,0 @@
-/*
- * Copyright (C) 2016 Alfred Chen.
- *
- * Code based on Con Kolivas's skip list implementation for BFS, and
- * which is based on example originally by William Pugh.
- *
- * Skip Lists are a probabilistic alternative to balanced trees, as
- * described in the June 1990 issue of CACM and were invented by
- * William Pugh in 1987.
- *
- * A couple of comments about this implementation:
- *
- * This file only provides a infrastructure of skip list.
- *
- * skiplist_node is embedded into container data structure, to get rid
- * the dependency of kmalloc/kfree operation in scheduler code.
- *
- * A customized search function should be defined using DEFINE_SKIPLIST_INSERT
- * macro and be used for skip list insert operation.
- *
- * Random Level is also not defined in this file, instead, it should be
- * customized implemented and set to node->level then pass to the customized
- * skiplist_insert function.
- *
- * Levels start at zero and go up to (NUM_SKIPLIST_LEVEL -1)
- *
- * NUM_SKIPLIST_LEVEL in this implementation is 8 instead of origin 16,
- * considering that there will be 256 entries to enable the top level when using
- * random level p=0.5, and that number is more than enough for a run queue usage
- * in a scheduler usage. And it also help to reduce the memory usage of the
- * embedded skip list node in task_struct to about 50%.
- *
- * The insertion routine has been implemented so as to use the
- * dirty hack described in the CACM paper: if a random level is
- * generated that is more than the current maximum level, the
- * current maximum level plus one is used instead.
- *
- * BFS Notes: In this implementation of skiplists, there are bidirectional
- * next/prev pointers and the insert function returns a pointer to the actual
- * node the value is stored. The key here is chosen by the scheduler so as to
- * sort tasks according to the priority list requirements and is no longer used
- * by the scheduler after insertion. The scheduler lookup, however, occurs in
- * O(1) time because it is always the first item in the level 0 linked list.
- * Since the task struct stores a copy of the node pointer upon skiplist_insert,
- * it can also remove it much faster than the original implementation with the
- * aid of prev<->next pointer manipulation and no searching.
- */
-#ifndef _LINUX_SKIP_LIST_H
-#define _LINUX_SKIP_LIST_H
-
-#include <linux/kernel.h>
-
-#define NUM_SKIPLIST_LEVEL (4)
-
-struct skiplist_node {
-	int level;	/* Levels in this node */
-	struct skiplist_node *next[NUM_SKIPLIST_LEVEL];
-	struct skiplist_node *prev[NUM_SKIPLIST_LEVEL];
-};
-
-#define SKIPLIST_NODE_INIT(name) { 0,\
-				   {&name, &name, &name, &name},\
-				   {&name, &name, &name, &name},\
-				 }
-
-/**
- * INIT_SKIPLIST_NODE -- init a skiplist_node, expecially for header
- * @node: the skip list node to be inited.
- */
-static inline void INIT_SKIPLIST_NODE(struct skiplist_node *node)
-{
-	int i;
-
-	node->level = 0;
-	for (i = 0; i < NUM_SKIPLIST_LEVEL; i++) {
-		WRITE_ONCE(node->next[i], node);
-		node->prev[i] = node;
-	}
-}
-
-/**
- * skiplist_entry - get the struct for this entry
- * @ptr: the &struct skiplist_node pointer.
- * @type:       the type of the struct this is embedded in.
- * @member:     the name of the skiplist_node within the struct.
- */
-#define skiplist_entry(ptr, type, member) \
-	container_of(ptr, type, member)
-
-/**
- * DEFINE_SKIPLIST_INSERT_FUNC -- macro to define a customized skip list insert
- * function, which takes two parameters, first one is the header node of the
- * skip list, second one is the skip list node to be inserted
- * @func_name: the customized skip list insert function name
- * @search_func: the search function to be used, which takes two parameters,
- * 1st one is the itrator of skiplist_node in the list, the 2nd is the skip list
- * node to be inserted, the function should return true if search should be
- * continued, otherwise return false.
- * Returns 1 if @node is inserted as the first item of skip list at level zero,
- * otherwise 0
- */
-#define DEFINE_SKIPLIST_INSERT_FUNC(func_name, search_func)\
-static inline int func_name(struct skiplist_node *head, struct skiplist_node *node)\
-{\
-	struct skiplist_node *p, *q;\
-	unsigned int k = head->level;\
-	unsigned int l = node->level;\
-\
-	p = head;\
-	if (l > k) {\
-		l = node->level = ++head->level;\
-\
-		node->next[l] = head;\
-		node->prev[l] = head;\
-		head->next[l] = node;\
-		head->prev[l] = node;\
-\
-		do {\
-			while (q = p->next[k], q != head && search_func(q, node))\
-				p = q;\
-\
-			node->prev[k] = p;\
-			node->next[k] = q;\
-			q->prev[k] = node;\
-			p->next[k] = node;\
-		} while (k--);\
-\
-		return (p == head);\
-	}\
-\
-	while (k > l) {\
-		while (q = p->next[k], q != head && search_func(q, node))\
-			p = q;\
-		k--;\
-	}\
-\
-	do {\
-		while (q = p->next[k], q != head && search_func(q, node))\
-			p = q;\
-\
-		node->prev[k] = p;\
-		node->next[k] = q;\
-		q->prev[k] = node;\
-		p->next[k] = node;\
-	} while (k--);\
-\
-	return (p == head);\
-}
-
-/**
- * skiplist_del_init -- delete skip list node from a skip list and reset it's
- * init state
- * @head: the header node of the skip list to be deleted from.
- * @node: the skip list node to be deleted, the caller need to ensure @node is
- * in skip list which @head represent.
- * Returns 1 if @node is the first item of skip level at level zero, otherwise 0
- */
-static inline int
-skiplist_del_init(struct skiplist_node *head, struct skiplist_node *node)
-{
-	unsigned int i, level = node->level;
-
-	for (i = 0; i <= level; i++) {
-		node->prev[i]->next[i] = node->next[i];
-		node->next[i]->prev[i] = node->prev[i];
-	}
-	if (level == head->level && level) {
-		while (head->next[level] == head && level)
-			level--;
-		head->level = level;
-	}
-
-	return (node->prev[0] == head);
-}
-#endif /* _LINUX_SKIP_LIST_H */
diff --git a/init/init_task.c b/init/init_task.c
index 412b06506b84..79c14fb5973c 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -106,8 +106,7 @@ struct task_struct init_task
 #endif
 #ifdef CONFIG_SCHED_PDS
 	.deadline	= 0,
-	.sl_level	= 0,
-	.sl_node	= SKIPLIST_NODE_INIT(init_task.sl_node),
+	.sq_node	= LIST_HEAD_INIT(init_task.sq_node),
 #endif
 	.time_slice	= HZ,
 #else
diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 3680162d8d19..01abbf28670f 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -509,6 +509,7 @@ static inline void update_rq_clock(struct rq *rq)
 	if (unlikely(delta <= 0))
 		return;
 	rq->clock += delta;
+	update_rq_time_edge(rq);
 	update_rq_clock_task(rq, delta);
 }
 
@@ -3815,7 +3816,15 @@ void alt_sched_debug(void)
 	       sched_sg_idle_mask.bits[0]);
 }
 #else
-inline void alt_sched_debug(void) {}
+int alt_debug[20];
+
+inline void alt_sched_debug(void)
+{
+	int i;
+
+	for (i = 0; i < 3; i++)
+		printk(KERN_INFO "sched: %d\n", alt_debug[i]);
+}
 #endif
 
 #ifdef	CONFIG_SMP
diff --git a/kernel/sched/alt_sched.h b/kernel/sched/alt_sched.h
index ac11555ba4f1..cfb4669dfbbf 100644
--- a/kernel/sched/alt_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -147,7 +147,8 @@ struct rq {
 	struct bmq queue;
 #endif
 #ifdef CONFIG_SCHED_PDS
-	struct skiplist_node sl_header;
+	struct sched_queue	queue;
+	u64			time_edge;
 #endif
 	unsigned long watermark;
 
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index 623908cf4380..3afc6fd7a27f 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -6,4 +6,9 @@
 #define SCHED_BITS	(MAX_RT_PRIO + NICE_WIDTH / 2 + 1)
 #define IDLE_TASK_SCHED_PRIO	(SCHED_BITS - 1)
 
+struct sched_queue {
+	DECLARE_BITMAP(bitmap, SCHED_BITS);
+	struct list_head heads[SCHED_BITS];
+};
+
 #endif
diff --git a/kernel/sched/pds_imp.h b/kernel/sched/pds_imp.h
index 335ce3a8e3ec..35886852c71a 100644
--- a/kernel/sched/pds_imp.h
+++ b/kernel/sched/pds_imp.h
@@ -11,26 +11,7 @@ static const u64 user_prio2deadline[NICE_WIDTH] = {
 /*  15 */	117870029, 129657031, 142622734, 156885007, 172573507
 };
 
-static const unsigned char dl_level_map[] = {
-/*       0               4               8              12           */
-	19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18,
-/*      16              20              24              28           */
-	18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17,
-/*      32              36              40              44           */
-	17, 17, 17, 17, 16, 16, 16, 16, 16, 16, 16, 16, 15, 15, 15, 15,
-/*      48              52              56              60           */
-	15, 15, 15, 14, 14, 14, 14, 14, 14, 13, 13, 13, 13, 12, 12, 12,
-/*      64              68              72              76           */
-	12, 11, 11, 11, 10, 10, 10,  9,  9,  8,  7,  6,  5,  4,  3,  2,
-/*      80              84              88              92           */
-	 1,  0
-};
-
-/* DEFAULT_SCHED_PRIO:
- * dl_level_map[(user_prio2deadline[39] - user_prio2deadline[0]) >> 21] =
- * dl_level_map[68] =
- * 10
- */
+#define SCHED_PRIO_SLOT		(4ULL << 20)
 #define DEFAULT_SCHED_PRIO (MAX_RT_PRIO + 10)
 
 static inline int normal_prio(struct task_struct *p)
@@ -41,21 +22,46 @@ static inline int normal_prio(struct task_struct *p)
 	return MAX_RT_PRIO;
 }
 
+extern int alt_debug[20];
+
 static inline int
-task_sched_prio(const struct task_struct *p, const struct rq *rq)
+task_sched_prio_normal(const struct task_struct *p, const struct rq *rq)
 {
-	size_t delta;
+	int delta;
+
+	delta = rq->time_edge + 20 - (p->deadline >> 23);
+	if (delta < 0) {
+		delta = 0;
+		alt_debug[0]++;
+	}
+	delta = 19 - min(delta, 19);
+
+	return delta;
+}
 
+static inline int
+task_sched_prio(const struct task_struct *p, const struct rq *rq)
+{
 	if (p == rq->idle)
 		return IDLE_TASK_SCHED_PRIO;
 
 	if (p->prio < MAX_RT_PRIO)
 		return p->prio;
 
-	delta = (rq->clock + user_prio2deadline[39] - p->deadline) >> 21;
-	delta = min((size_t)delta, ARRAY_SIZE(dl_level_map) - 1);
+	return MAX_RT_PRIO + task_sched_prio_normal(p, rq);
+}
+
+static inline int
+task_sched_prio_idx(const struct task_struct *p, const struct rq *rq)
+{
+	if (p == rq->idle)
+		return IDLE_TASK_SCHED_PRIO;
 
-	return MAX_RT_PRIO + dl_level_map[delta];
+	if (p->prio < MAX_RT_PRIO)
+		return p->prio;
+
+	return MAX_RT_PRIO +
+		(task_sched_prio_normal(p, rq) + rq->time_edge) % 20;
 }
 
 int task_running_nice(struct task_struct *p)
@@ -68,6 +74,53 @@ static inline void update_task_priodl(struct task_struct *p)
 	p->priodl = (((u64) (p->prio))<<56) | ((p->deadline)>>8);
 }
 
+
+DECLARE_BITMAP(normal_mask, SCHED_BITS);
+
+static inline void sched_shift_normal_bitmap(unsigned long *mask, unsigned int shift)
+{
+	DECLARE_BITMAP(normal, SCHED_BITS);
+
+	bitmap_and(normal, mask, normal_mask, SCHED_BITS);
+	bitmap_shift_right(normal, normal, shift, SCHED_BITS);
+	bitmap_and(normal, normal, normal_mask, SCHED_BITS);
+
+	bitmap_andnot(mask, mask, normal_mask, SCHED_BITS);
+	bitmap_or(mask, mask, normal, SCHED_BITS);
+}
+
+static inline void update_rq_time_edge(struct rq *rq)
+{
+	struct list_head head;
+	u64 old = rq->time_edge;
+	u64 now = rq->clock >> 23;
+	u64 prio, delta = min(20ULL, now - old);
+
+	if (now == old)
+		return;
+
+	INIT_LIST_HEAD(&head);
+
+	prio = MAX_RT_PRIO;
+	for_each_set_bit_from(prio, rq->queue.bitmap, MAX_RT_PRIO + delta) {
+		u64 idx;
+
+		idx = MAX_RT_PRIO + ((prio - MAX_RT_PRIO) + rq->time_edge) % 20;
+		list_splice_tail_init(rq->queue.heads + idx, &head);
+	}
+	sched_shift_normal_bitmap(rq->queue.bitmap, delta);
+	rq->time_edge = now;
+	if (!list_empty(&head)) {
+		struct task_struct *p;
+
+		list_for_each_entry(p, &head, sq_node)
+			p->sq_idx = MAX_RT_PRIO + now % 20;
+
+		list_splice(&head, rq->queue.heads + MAX_RT_PRIO + now % 20);
+		set_bit(MAX_RT_PRIO, rq->queue.bitmap);
+	}
+}
+
 static inline void requeue_task(struct task_struct *p, struct rq *rq);
 
 static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
@@ -77,40 +130,25 @@ static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
 
 	if (p->prio >= MAX_RT_PRIO)
 		p->deadline = rq->clock +
-			user_prio2deadline[p->static_prio - MAX_RT_PRIO];
+			SCHED_PRIO_SLOT * (p->static_prio - MAX_RT_PRIO + 1);
 	update_task_priodl(p);
 
 	if (SCHED_FIFO != p->policy && task_on_rq_queued(p))
 		requeue_task(p, rq);
 }
 
-/*
- * pds_skiplist_task_search -- search function used in PDS run queue skip list
- * node insert operation.
- * @it: iterator pointer to the node in the skip list
- * @node: pointer to the skiplist_node to be inserted
- *
- * Returns true if key of @it is less or equal to key value of @node, otherwise
- * false.
- */
-static inline bool
-pds_skiplist_task_search(struct skiplist_node *it, struct skiplist_node *node)
-{
-	return (skiplist_entry(it, struct task_struct, sl_node)->priodl <=
-		skiplist_entry(node, struct task_struct, sl_node)->priodl);
-}
-
-/*
- * Define the skip list insert function for PDS
- */
-DEFINE_SKIPLIST_INSERT_FUNC(pds_skiplist_insert, pds_skiplist_task_search);
-
 /*
  * Init the queue structure in rq
  */
 static inline void sched_queue_init(struct rq *rq)
 {
-	INIT_SKIPLIST_NODE(&rq->sl_header);
+	struct sched_queue *q = &rq->queue;
+	int i;
+
+	bitmap_set(normal_mask, MAX_RT_PRIO, 20);
+	bitmap_zero(q->bitmap, SCHED_BITS);
+	for(i = 0; i < SCHED_BITS; i++)
+		INIT_LIST_HEAD(&q->heads[i]);
 }
 
 /*
@@ -119,19 +157,33 @@ static inline void sched_queue_init(struct rq *rq)
  */
 static inline void sched_queue_init_idle(struct rq *rq, struct task_struct *idle)
 {
+	struct sched_queue *q = &rq->queue;
 	/*printk(KERN_INFO "sched: init(%d) - %px\n", cpu_of(rq), idle);*/
-	int default_prio = idle->prio;
 
-	idle->prio = MAX_PRIO;
-	idle->deadline = 0ULL;
-	update_task_priodl(idle);
+	idle->sq_idx = IDLE_TASK_SCHED_PRIO;
+	INIT_LIST_HEAD(&q->heads[idle->sq_idx]);
+	list_add(&idle->sq_node, &q->heads[idle->sq_idx]);
+	set_bit(idle->sq_idx, q->bitmap);
+}
 
-	INIT_SKIPLIST_NODE(&rq->sl_header);
+static inline unsigned long sched_prio2idx(unsigned long idx, struct rq *rq)
+{
+	if (IDLE_TASK_SCHED_PRIO == idx ||
+	    idx < MAX_RT_PRIO)
+		return idx;
 
-	idle->sl_node.level = idle->sl_level;
-	pds_skiplist_insert(&rq->sl_header, &idle->sl_node);
+	return MAX_RT_PRIO +
+		((idx - MAX_RT_PRIO) + rq->time_edge) % 20;
+}
 
-	idle->prio = default_prio;
+static inline unsigned long sched_idx2prio(unsigned long idx, struct rq *rq)
+{
+	if (IDLE_TASK_SCHED_PRIO == idx ||
+	    idx < MAX_RT_PRIO)
+		return idx;
+
+	return MAX_RT_PRIO +
+		((idx - MAX_RT_PRIO) + 20 -  rq->time_edge % 20) % 20;
 }
 
 /*
@@ -139,107 +191,99 @@ static inline void sched_queue_init_idle(struct rq *rq, struct task_struct *idle
  */
 static inline struct task_struct *sched_rq_first_task(struct rq *rq)
 {
-	struct skiplist_node *node = rq->sl_header.next[0];
+	unsigned long idx = find_first_bit(rq->queue.bitmap, SCHED_BITS);
+	const struct list_head *head = &rq->queue.heads[sched_prio2idx(idx, rq)];
 
-	BUG_ON(node == &rq->sl_header);
-	return skiplist_entry(node, struct task_struct, sl_node);
+	/*
+	if (list_empty(head)) {
+		pr_err("BUG: cpu%d(time_edge%llu) prio%lu idx%lu mismatched\n",
+		       rq->cpu, rq->time_edge, idx, sched_prio2idx(idx, rq));
+		BUG();
+	}*/
+	return list_first_entry(head, struct task_struct, sq_node);
 }
 
 static inline struct task_struct *
 sched_rq_next_task(struct task_struct *p, struct rq *rq)
 {
-	struct skiplist_node *next = p->sl_node.next[0];
+	unsigned long idx = p->sq_idx;
+	struct list_head *head = &rq->queue.heads[idx];
+
+	if (list_is_last(&p->sq_node, head)) {
+		idx = find_next_bit(rq->queue.bitmap, SCHED_BITS,
+				    sched_idx2prio(idx, rq) + 1);
+		head = &rq->queue.heads[sched_prio2idx(idx, rq)];
+
+		return list_first_entry(head, struct task_struct, sq_node);
+	}
 
-	BUG_ON(next == &rq->sl_header);
-	return skiplist_entry(next, struct task_struct, sl_node);
+	return list_next_entry(p, sq_node);
 }
 
 static inline unsigned long sched_queue_watermark(struct rq *rq)
 {
-	return task_sched_prio(sched_rq_first_task(rq), rq);
+	return find_first_bit(rq->queue.bitmap, SCHED_BITS);
 }
 
 #define __SCHED_DEQUEUE_TASK(p, rq, flags, func)		\
 	psi_dequeue(p, flags & DEQUEUE_SLEEP);			\
 	sched_info_dequeued(rq, p);				\
 								\
-	if (skiplist_del_init(&rq->sl_header, &p->sl_node)) {	\
+	list_del(&p->sq_node);					\
+	if (list_empty(&rq->queue.heads[p->sq_idx])) {		\
+		clear_bit(sched_idx2prio(p->sq_idx, rq),	\
+			  rq->queue.bitmap);			\
 		func;						\
-	}
+	}							\
+	/*\
+	pr_info("-->: cpu%d(time_edge%llu) prio%lu idx%u\n",	\
+		rq->cpu, rq->time_edge, sched_idx2prio(p->sq_idx, rq), p->sq_idx);	\
+	*/
 
 #define __SCHED_ENQUEUE_TASK(p, rq, flags)				\
 	sched_info_queued(rq, p);					\
 	psi_enqueue(p, flags);						\
 									\
-	p->sl_node.level = p->sl_level;					\
-	pds_skiplist_insert(&rq->sl_header, &p->sl_node)
+	p->sq_idx = task_sched_prio_idx(p, rq);				\
+	list_add_tail(&p->sq_node, &rq->queue.heads[p->sq_idx]);	\
+	set_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);				\
+	/*\
+	pr_info("<--: cpu%d(time_edge%llu) prio%lu idx%u\n",	\
+		rq->cpu, rq->time_edge, sched_idx2prio(p->sq_idx, rq), p->sq_idx);	\
+	*/
 
 /*
  * Requeue a task @p to @rq
  */
 #define __SCHED_REQUEUE_TASK(p, rq, func)					\
 {\
-	bool b_first = skiplist_del_init(&rq->sl_header, &p->sl_node);		\
+	int idx = task_sched_prio_idx(p, rq);					\
 \
-	p->sl_node.level = p->sl_level;						\
-	if (pds_skiplist_insert(&rq->sl_header, &p->sl_node) || b_first) {	\
+	list_del(&p->sq_node);							\
+	list_add_tail(&p->sq_node, &rq->queue.heads[idx]);			\
+	if (idx != p->sq_idx) {						\
+		if (list_empty(&rq->queue.heads[p->sq_idx]))			\
+			clear_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);		\
+		p->sq_idx = idx;						\
+		set_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);				\
 		func;								\
+		/*\
+		pr_info("<->: cpu%d(time_edge%llu) prio%lu idx%u\n",	\
+			rq->cpu, rq->time_edge, sched_idx2prio(p->sq_idx, rq), p->sq_idx);	\
+		*/\
 	}									\
 }
 
 static inline bool sched_task_need_requeue(struct task_struct *p, struct rq *rq)
 {
-	struct skiplist_node *node;
-
-	node = p->sl_node.prev[0];
-	if (node != &rq->sl_header &&
-	    skiplist_entry(node, struct task_struct, sl_node)->priodl > p->priodl)
-		return true;
-
-	node = p->sl_node.next[0];
-	if (node != &rq->sl_header &&
-	    skiplist_entry(node, struct task_struct, sl_node)->priodl < p->priodl)
-		return true;
-
-	return false;
-}
-
-/*
- * pds_skiplist_random_level -- Returns a pseudo-random level number for skip
- * list node which is used in PDS run queue.
- *
- * __ffs() is used to satisfy p = 0.5 between each levels, and there should be
- * platform instruction(known as ctz/clz) for acceleration.
- *
- * The skiplist level for a task is populated when task is created and doesn't
- * change in task's life time. When task is being inserted into run queue, this
- * skiplist level is set to task's sl_node->level, the skiplist insert function
- * may change it based on current level of the skip lsit.
- */
-static inline int pds_skiplist_random_level(const struct task_struct *p)
-{
-	/*
-	 * 1. Some architectures don't have better than microsecond resolution
-	 * so mask out ~microseconds as a factor of the random seed for skiplist
-	 * insertion.
-	 * 2. Use address of task structure pointer as another factor of the
-	 * random seed for task burst forking scenario.
-	 */
-	unsigned long randseed = (task_rq(p)->clock ^ (unsigned long)p) >> 10;
-
-	randseed &= __GENMASK(NUM_SKIPLIST_LEVEL - 1, 0);
-	if (randseed)
-		return __ffs(randseed);
-
-	return (NUM_SKIPLIST_LEVEL - 1);
+	return (task_sched_prio_idx(p, rq) != p->sq_idx);
 }
 
 static void sched_task_fork(struct task_struct *p, struct rq *rq)
 {
-	p->sl_level = pds_skiplist_random_level(p);
 	if (p->prio >= MAX_RT_PRIO)
 		p->deadline = rq->clock +
-			user_prio2deadline[p->static_prio - MAX_RT_PRIO];
+			SCHED_PRIO_SLOT * (p->static_prio - MAX_RT_PRIO + 1);
 	update_task_priodl(p);
 }
 
@@ -261,9 +305,10 @@ int task_prio(const struct task_struct *p)
 	if (p->prio < MAX_RT_PRIO)
 		return (p->prio - MAX_RT_PRIO);
 
-	preempt_disable();
-	ret = task_sched_prio(p, this_rq()) - MAX_RT_PRIO;
-	preempt_enable();
+	/*preempt_disable();
+	ret = task_sched_prio(p, task_rq(p)) - MAX_RT_PRIO;*/
+	ret = p->static_prio - MAX_RT_PRIO;
+	/*preempt_enable();*/
 
 	return ret;
 }
-- 
2.32.0.93.g670b81a890


From c7e57628841883139f17f29e118d73390ef5462e Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Mon, 17 May 2021 10:38:00 +0000
Subject: [PATCH 03/30] sched/pds: Remove unused priodl in task structure

---
 include/linux/sched.h          | 1 -
 include/linux/sched/deadline.h | 2 +-
 kernel/sched/alt_core.c        | 3 ---
 kernel/sched/pds_imp.h         | 8 --------
 4 files changed, 1 insertion(+), 13 deletions(-)

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 3217880daa71..91f32b48a839 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -722,7 +722,6 @@ struct task_struct {
 #endif /* CONFIG_SCHED_BMQ */
 #ifdef CONFIG_SCHED_PDS
 	u64				deadline;
-	u64				priodl;
 	int				sq_idx;
 	struct list_head		sq_node;
 #endif /* CONFIG_SCHED_PDS */
diff --git a/include/linux/sched/deadline.h b/include/linux/sched/deadline.h
index 179d77c8360e..3f208b842745 100644
--- a/include/linux/sched/deadline.h
+++ b/include/linux/sched/deadline.h
@@ -12,7 +12,7 @@ static inline int dl_task(struct task_struct *p)
 #endif
 
 #ifdef CONFIG_SCHED_PDS
-#define __tsk_deadline(p)	((p)->priodl)
+#define __tsk_deadline(p)	((((u64) ((p)->prio))<<56) | (((p)->deadline)>>8))
 #endif
 
 #else
diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 01abbf28670f..cbca3a54f912 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -4628,7 +4628,6 @@ void rt_mutex_setprio(struct task_struct *p, struct task_struct *pi_task)
 
 	trace_sched_pi_setprio(p, pi_task);
 	p->prio = prio;
-	update_task_priodl(p);
 
 	check_task_changed(rq, p);
 out_unlock:
@@ -4673,7 +4672,6 @@ void set_user_nice(struct task_struct *p, long nice)
 		goto out_unlock;
 
 	p->prio = effective_prio(p);
-	update_task_priodl(p);
 
 	check_task_changed(rq, p);
 out_unlock:
@@ -4823,7 +4821,6 @@ static void __setscheduler(struct rq *rq, struct task_struct *p,
 	p->prio = normal_prio(p);
 	if (keep_boost)
 		p->prio = rt_effective_prio(p, p->prio);
-	update_task_priodl(p);
 }
 
 /*
diff --git a/kernel/sched/pds_imp.h b/kernel/sched/pds_imp.h
index 35886852c71a..c9ab90f8d5c6 100644
--- a/kernel/sched/pds_imp.h
+++ b/kernel/sched/pds_imp.h
@@ -69,12 +69,6 @@ int task_running_nice(struct task_struct *p)
 	return task_sched_prio(p, task_rq(p)) > DEFAULT_SCHED_PRIO;
 }
 
-static inline void update_task_priodl(struct task_struct *p)
-{
-	p->priodl = (((u64) (p->prio))<<56) | ((p->deadline)>>8);
-}
-
-
 DECLARE_BITMAP(normal_mask, SCHED_BITS);
 
 static inline void sched_shift_normal_bitmap(unsigned long *mask, unsigned int shift)
@@ -131,7 +125,6 @@ static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
 	if (p->prio >= MAX_RT_PRIO)
 		p->deadline = rq->clock +
 			SCHED_PRIO_SLOT * (p->static_prio - MAX_RT_PRIO + 1);
-	update_task_priodl(p);
 
 	if (SCHED_FIFO != p->policy && task_on_rq_queued(p))
 		requeue_task(p, rq);
@@ -284,7 +277,6 @@ static void sched_task_fork(struct task_struct *p, struct rq *rq)
 	if (p->prio >= MAX_RT_PRIO)
 		p->deadline = rq->clock +
 			SCHED_PRIO_SLOT * (p->static_prio - MAX_RT_PRIO + 1);
-	update_task_priodl(p);
 }
 
 /**
-- 
2.32.0.93.g670b81a890


From 30709a2f933845602e766ed7ea92e68fe9680d1c Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Mon, 17 May 2021 12:49:04 +0000
Subject: [PATCH 04/30] sched/pds: Code clean up.

---
 kernel/sched/pds_imp.h | 42 +++++++++++++-----------------------------
 1 file changed, 13 insertions(+), 29 deletions(-)

diff --git a/kernel/sched/pds_imp.h b/kernel/sched/pds_imp.h
index c9ab90f8d5c6..8cc656a7cc48 100644
--- a/kernel/sched/pds_imp.h
+++ b/kernel/sched/pds_imp.h
@@ -88,11 +88,12 @@ static inline void update_rq_time_edge(struct rq *rq)
 	struct list_head head;
 	u64 old = rq->time_edge;
 	u64 now = rq->clock >> 23;
-	u64 prio, delta = min(20ULL, now - old);
+	u64 prio, delta;
 
 	if (now == old)
 		return;
 
+	delta = min(20ULL, now - old);
 	INIT_LIST_HEAD(&head);
 
 	prio = MAX_RT_PRIO;
@@ -115,17 +116,20 @@ static inline void update_rq_time_edge(struct rq *rq)
 	}
 }
 
+static inline void sched_renew_deadline(struct task_struct *p, const struct rq *rq)
+{
+	if (p->prio >= MAX_RT_PRIO)
+		p->deadline = rq->clock +
+			SCHED_PRIO_SLOT * (p->static_prio - MAX_RT_PRIO + 1);
+}
+
 static inline void requeue_task(struct task_struct *p, struct rq *rq);
 
 static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
 {
 	/*printk(KERN_INFO "sched: time_slice_expired(%d) - %px\n", cpu_of(rq), p);*/
 	p->time_slice = sched_timeslice_ns;
-
-	if (p->prio >= MAX_RT_PRIO)
-		p->deadline = rq->clock +
-			SCHED_PRIO_SLOT * (p->static_prio - MAX_RT_PRIO + 1);
-
+	sched_renew_deadline(p, rq);
 	if (SCHED_FIFO != p->policy && task_on_rq_queued(p))
 		requeue_task(p, rq);
 }
@@ -187,12 +191,6 @@ static inline struct task_struct *sched_rq_first_task(struct rq *rq)
 	unsigned long idx = find_first_bit(rq->queue.bitmap, SCHED_BITS);
 	const struct list_head *head = &rq->queue.heads[sched_prio2idx(idx, rq)];
 
-	/*
-	if (list_empty(head)) {
-		pr_err("BUG: cpu%d(time_edge%llu) prio%lu idx%lu mismatched\n",
-		       rq->cpu, rq->time_edge, idx, sched_prio2idx(idx, rq));
-		BUG();
-	}*/
 	return list_first_entry(head, struct task_struct, sq_node);
 }
 
@@ -227,11 +225,7 @@ static inline unsigned long sched_queue_watermark(struct rq *rq)
 		clear_bit(sched_idx2prio(p->sq_idx, rq),	\
 			  rq->queue.bitmap);			\
 		func;						\
-	}							\
-	/*\
-	pr_info("-->: cpu%d(time_edge%llu) prio%lu idx%u\n",	\
-		rq->cpu, rq->time_edge, sched_idx2prio(p->sq_idx, rq), p->sq_idx);	\
-	*/
+	}
 
 #define __SCHED_ENQUEUE_TASK(p, rq, flags)				\
 	sched_info_queued(rq, p);					\
@@ -239,11 +233,7 @@ static inline unsigned long sched_queue_watermark(struct rq *rq)
 									\
 	p->sq_idx = task_sched_prio_idx(p, rq);				\
 	list_add_tail(&p->sq_node, &rq->queue.heads[p->sq_idx]);	\
-	set_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);				\
-	/*\
-	pr_info("<--: cpu%d(time_edge%llu) prio%lu idx%u\n",	\
-		rq->cpu, rq->time_edge, sched_idx2prio(p->sq_idx, rq), p->sq_idx);	\
-	*/
+	set_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);
 
 /*
  * Requeue a task @p to @rq
@@ -260,10 +250,6 @@ static inline unsigned long sched_queue_watermark(struct rq *rq)
 		p->sq_idx = idx;						\
 		set_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);				\
 		func;								\
-		/*\
-		pr_info("<->: cpu%d(time_edge%llu) prio%lu idx%u\n",	\
-			rq->cpu, rq->time_edge, sched_idx2prio(p->sq_idx, rq), p->sq_idx);	\
-		*/\
 	}									\
 }
 
@@ -274,9 +260,7 @@ static inline bool sched_task_need_requeue(struct task_struct *p, struct rq *rq)
 
 static void sched_task_fork(struct task_struct *p, struct rq *rq)
 {
-	if (p->prio >= MAX_RT_PRIO)
-		p->deadline = rq->clock +
-			SCHED_PRIO_SLOT * (p->static_prio - MAX_RT_PRIO + 1);
+	sched_renew_deadline(p, rq);
 }
 
 /**
-- 
2.32.0.93.g670b81a890


From e26550a7f0b17733c22fb7931367b920ef236bfd Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Mon, 17 May 2021 13:48:11 +0000
Subject: [PATCH 05/30] sched/alt: BMQ&PDS share same name in data structure

sq_idx and sq_node are shared in task_struct.
queue is shared in rq.
---
 include/linux/sched.h    |  6 ++---
 init/init_task.c         |  5 ++--
 kernel/sched/alt_sched.h | 10 +++++---
 kernel/sched/bmq.h       |  5 ----
 kernel/sched/bmq_imp.h   | 54 ++++++++++++++++++++--------------------
 kernel/sched/pds.h       |  5 ----
 6 files changed, 37 insertions(+), 48 deletions(-)

diff --git a/include/linux/sched.h b/include/linux/sched.h
index 91f32b48a839..35f7cfe6539a 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -715,15 +715,13 @@ struct task_struct {
 #ifdef CONFIG_SCHED_ALT
 	u64				last_ran;
 	s64				time_slice;
+	int				sq_idx;
+	struct list_head		sq_node;
 #ifdef CONFIG_SCHED_BMQ
 	int				boost_prio;
-	int				bmq_idx;
-	struct list_head		bmq_node;
 #endif /* CONFIG_SCHED_BMQ */
 #ifdef CONFIG_SCHED_PDS
 	u64				deadline;
-	int				sq_idx;
-	struct list_head		sq_node;
 #endif /* CONFIG_SCHED_PDS */
 	/* sched_clock time spent running */
 	u64				sched_time;
diff --git a/init/init_task.c b/init/init_task.c
index 79c14fb5973c..89f1e6ace69a 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -99,14 +99,13 @@ struct task_struct init_task
 		.fn = do_no_restart_syscall,
 	},
 #ifdef CONFIG_SCHED_ALT
+	.sq_node	= LIST_HEAD_INIT(init_task.sq_node),
 #ifdef CONFIG_SCHED_BMQ
 	.boost_prio	= 0,
-	.bmq_idx	= 15,
-	.bmq_node	= LIST_HEAD_INIT(init_task.bmq_node),
+	.sq_idx		= 15,
 #endif
 #ifdef CONFIG_SCHED_PDS
 	.deadline	= 0,
-	.sq_node	= LIST_HEAD_INIT(init_task.sq_node),
 #endif
 	.time_slice	= HZ,
 #else
diff --git a/kernel/sched/alt_sched.h b/kernel/sched/alt_sched.h
index cfb4669dfbbf..21f359102fbc 100644
--- a/kernel/sched/alt_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -131,6 +131,11 @@ static inline int task_on_rq_migrating(struct task_struct *p)
 #define WF_MIGRATED	0x04		/* internal use, task got migrated */
 #define WF_ON_CPU	0x08		/* Wakee is on_rq */
 
+struct sched_queue {
+	DECLARE_BITMAP(bitmap, SCHED_BITS);
+	struct list_head heads[SCHED_BITS];
+};
+
 /*
  * This is the main, per-CPU runqueue data structure.
  * This data should only be modified by the local cpu.
@@ -143,11 +148,8 @@ struct rq {
 	struct task_struct *idle, *stop, *skip;
 	struct mm_struct *prev_mm;
 
-#ifdef CONFIG_SCHED_BMQ
-	struct bmq queue;
-#endif
-#ifdef CONFIG_SCHED_PDS
 	struct sched_queue	queue;
+#ifdef CONFIG_SCHED_PDS
 	u64			time_edge;
 #endif
 	unsigned long watermark;
diff --git a/kernel/sched/bmq.h b/kernel/sched/bmq.h
index aba3c98759f8..7f83b7c42619 100644
--- a/kernel/sched/bmq.h
+++ b/kernel/sched/bmq.h
@@ -6,9 +6,4 @@
 #define SCHED_BITS	(MAX_RT_PRIO + NICE_WIDTH / 2 + MAX_PRIORITY_ADJ + 1)
 #define IDLE_TASK_SCHED_PRIO	(SCHED_BITS - 1)
 
-struct bmq {
-	DECLARE_BITMAP(bitmap, SCHED_BITS);
-	struct list_head heads[SCHED_BITS];
-};
-
 #endif
diff --git a/kernel/sched/bmq_imp.h b/kernel/sched/bmq_imp.h
index 7c71f1141d00..f6bd3421b95c 100644
--- a/kernel/sched/bmq_imp.h
+++ b/kernel/sched/bmq_imp.h
@@ -67,8 +67,6 @@ inline int task_running_nice(struct task_struct *p)
 	return (p->prio + p->boost_prio > DEFAULT_PRIO + MAX_PRIORITY_ADJ);
 }
 
-static inline void update_task_priodl(struct task_struct *p) {}
-
 static inline unsigned long sched_queue_watermark(struct rq *rq)
 {
 	return find_first_bit(rq->queue.bitmap, SCHED_BITS);
@@ -76,7 +74,7 @@ static inline unsigned long sched_queue_watermark(struct rq *rq)
 
 static inline void sched_queue_init(struct rq *rq)
 {
-	struct bmq *q = &rq->queue;
+	struct sched_queue *q = &rq->queue;
 	int i;
 
 	bitmap_zero(q->bitmap, SCHED_BITS);
@@ -86,12 +84,12 @@ static inline void sched_queue_init(struct rq *rq)
 
 static inline void sched_queue_init_idle(struct rq *rq, struct task_struct *idle)
 {
-	struct bmq *q = &rq->queue;
+	struct sched_queue *q = &rq->queue;
 
-	idle->bmq_idx = IDLE_TASK_SCHED_PRIO;
-	INIT_LIST_HEAD(&q->heads[idle->bmq_idx]);
-	list_add(&idle->bmq_node, &q->heads[idle->bmq_idx]);
-	set_bit(idle->bmq_idx, q->bitmap);
+	idle->sq_idx = IDLE_TASK_SCHED_PRIO;
+	INIT_LIST_HEAD(&q->heads[idle->sq_idx]);
+	list_add(&idle->sq_node, &q->heads[idle->sq_idx]);
+	set_bit(idle->sq_idx, q->bitmap);
 }
 
 /*
@@ -102,32 +100,32 @@ static inline struct task_struct *sched_rq_first_task(struct rq *rq)
 	unsigned long idx = find_first_bit(rq->queue.bitmap, SCHED_BITS);
 	const struct list_head *head = &rq->queue.heads[idx];
 
-	return list_first_entry(head, struct task_struct, bmq_node);
+	return list_first_entry(head, struct task_struct, sq_node);
 }
 
 static inline struct task_struct *
 sched_rq_next_task(struct task_struct *p, struct rq *rq)
 {
-	unsigned long idx = p->bmq_idx;
+	unsigned long idx = p->sq_idx;
 	struct list_head *head = &rq->queue.heads[idx];
 
-	if (list_is_last(&p->bmq_node, head)) {
+	if (list_is_last(&p->sq_node, head)) {
 		idx = find_next_bit(rq->queue.bitmap, SCHED_BITS, idx + 1);
 		head = &rq->queue.heads[idx];
 
-		return list_first_entry(head, struct task_struct, bmq_node);
+		return list_first_entry(head, struct task_struct, sq_node);
 	}
 
-	return list_next_entry(p, bmq_node);
+	return list_next_entry(p, sq_node);
 }
 
 #define __SCHED_DEQUEUE_TASK(p, rq, flags, func)	\
 	psi_dequeue(p, flags & DEQUEUE_SLEEP);		\
 	sched_info_dequeued(rq, p);			\
 							\
-	list_del(&p->bmq_node);				\
-	if (list_empty(&rq->queue.heads[p->bmq_idx])) {	\
-		clear_bit(p->bmq_idx, rq->queue.bitmap);\
+	list_del(&p->sq_node);				\
+	if (list_empty(&rq->queue.heads[p->sq_idx])) {	\
+		clear_bit(p->sq_idx, rq->queue.bitmap);\
 		func;					\
 	}
 
@@ -135,28 +133,28 @@ sched_rq_next_task(struct task_struct *p, struct rq *rq)
 	sched_info_queued(rq, p);					\
 	psi_enqueue(p, flags);						\
 									\
-	p->bmq_idx = task_sched_prio(p, rq);				\
-	list_add_tail(&p->bmq_node, &rq->queue.heads[p->bmq_idx]);	\
-	set_bit(p->bmq_idx, rq->queue.bitmap)
+	p->sq_idx = task_sched_prio(p, rq);				\
+	list_add_tail(&p->sq_node, &rq->queue.heads[p->sq_idx]);	\
+	set_bit(p->sq_idx, rq->queue.bitmap)
 
 #define __SCHED_REQUEUE_TASK(p, rq, func)				\
 {									\
 	int idx = task_sched_prio(p, rq);				\
 \
-	list_del(&p->bmq_node);						\
-	list_add_tail(&p->bmq_node, &rq->queue.heads[idx]);		\
-	if (idx != p->bmq_idx) {					\
-		if (list_empty(&rq->queue.heads[p->bmq_idx]))		\
-			clear_bit(p->bmq_idx, rq->queue.bitmap);	\
-		p->bmq_idx = idx;					\
-		set_bit(p->bmq_idx, rq->queue.bitmap);			\
+	list_del(&p->sq_node);						\
+	list_add_tail(&p->sq_node, &rq->queue.heads[idx]);		\
+	if (idx != p->sq_idx) {					\
+		if (list_empty(&rq->queue.heads[p->sq_idx]))		\
+			clear_bit(p->sq_idx, rq->queue.bitmap);	\
+		p->sq_idx = idx;					\
+		set_bit(p->sq_idx, rq->queue.bitmap);			\
 		func;							\
 	}								\
 }
 
 static inline bool sched_task_need_requeue(struct task_struct *p, struct rq *rq)
 {
-	return (task_sched_prio(p, rq) != p->bmq_idx);
+	return (task_sched_prio(p, rq) != p->sq_idx);
 }
 
 static void sched_task_fork(struct task_struct *p, struct rq *rq)
@@ -201,3 +199,5 @@ static void sched_task_deactivate(struct task_struct *p, struct rq *rq)
 	if (rq_switch_time(rq) < boost_threshold(p))
 		boost_task(p);
 }
+
+static inline void update_rq_time_edge(struct rq *rq) {}
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index 3afc6fd7a27f..623908cf4380 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -6,9 +6,4 @@
 #define SCHED_BITS	(MAX_RT_PRIO + NICE_WIDTH / 2 + 1)
 #define IDLE_TASK_SCHED_PRIO	(SCHED_BITS - 1)
 
-struct sched_queue {
-	DECLARE_BITMAP(bitmap, SCHED_BITS);
-	struct list_head heads[SCHED_BITS];
-};
-
 #endif
-- 
2.32.0.93.g670b81a890


From cb2946d809ee3a54737b851dace953fca8c2f6e2 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Mon, 17 May 2021 14:17:37 +0000
Subject: [PATCH 06/30] sched/alt: [Sync] 163dd7fa459f kthread: Fix PF_KTHREAD
 vs to_kthread() race

---
 kernel/sched/alt_core.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index cbca3a54f912..b1c17ff1642c 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -6210,7 +6210,7 @@ static void balance_push(struct rq *rq)
 	 * histerical raisins.
 	 */
 	if (rq->idle == push_task ||
-	    ((push_task->flags & PF_KTHREAD) && kthread_is_per_cpu(push_task)) ||
+	    kthread_is_per_cpu(push_task) ||
 	    is_migration_disabled(push_task)) {
 
 		/*
-- 
2.32.0.93.g670b81a890


From a1db654c1c5fb93f954e82479f11c69b8e941277 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Mon, 17 May 2021 14:49:46 +0000
Subject: [PATCH 07/30] sched/alt: Rename BMQ&PDS implement files.

---
 kernel/sched/alt_core.c  |   4 +-
 kernel/sched/alt_sched.h |   9 +-
 kernel/sched/bmq.h       | 206 +++++++++++++++++++++++++-
 kernel/sched/bmq_imp.h   | 203 --------------------------
 kernel/sched/pds.h       | 303 ++++++++++++++++++++++++++++++++++++++-
 kernel/sched/pds_imp.h   | 300 --------------------------------------
 6 files changed, 506 insertions(+), 519 deletions(-)
 delete mode 100644 kernel/sched/bmq_imp.h
 delete mode 100644 kernel/sched/pds_imp.h

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index b1c17ff1642c..9ade1b64aa9c 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -143,10 +143,10 @@ static cpumask_t sched_sg_idle_mask ____cacheline_aligned_in_smp;
 static cpumask_t sched_rq_watermark[SCHED_BITS] ____cacheline_aligned_in_smp;
 
 #ifdef CONFIG_SCHED_BMQ
-#include "bmq_imp.h"
+#include "bmq.h"
 #endif
 #ifdef CONFIG_SCHED_PDS
-#include "pds_imp.h"
+#include "pds.h"
 #endif
 
 static inline void update_sched_rq_watermark(struct rq *rq)
diff --git a/kernel/sched/alt_sched.h b/kernel/sched/alt_sched.h
index 21f359102fbc..58ff6212b446 100644
--- a/kernel/sched/alt_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -50,12 +50,17 @@
 #include <trace/events/sched.h>
 
 #ifdef CONFIG_SCHED_BMQ
-#include "bmq.h"
+/* bits:
+ * RT(0-99), (Low prio adj range, nice width, high prio adj range) / 2, cpu idle task */
+#define SCHED_BITS	(MAX_RT_PRIO + NICE_WIDTH / 2 + MAX_PRIORITY_ADJ + 1)
 #endif
 #ifdef CONFIG_SCHED_PDS
-#include "pds.h"
+/* bits: RT(0-99), nice width / 2, cpu idle task */
+#define SCHED_BITS	(MAX_RT_PRIO + NICE_WIDTH / 2 + 1)
 #endif
 
+#define IDLE_TASK_SCHED_PRIO	(SCHED_BITS - 1)
+
 #ifdef CONFIG_SCHED_DEBUG
 # define SCHED_WARN_ON(x)	WARN_ONCE(x, #x)
 extern void resched_latency_warn(int cpu, u64 latency);
diff --git a/kernel/sched/bmq.h b/kernel/sched/bmq.h
index 7f83b7c42619..f6bd3421b95c 100644
--- a/kernel/sched/bmq.h
+++ b/kernel/sched/bmq.h
@@ -1,9 +1,203 @@
-#ifndef BMQ_H
-#define BMQ_H
+#define ALT_SCHED_VERSION_MSG "sched/bmq: BMQ CPU Scheduler "ALT_SCHED_VERSION" by Alfred Chen.\n"
 
-/* bits:
- * RT(0-99), (Low prio adj range, nice width, high prio adj range) / 2, cpu idle task */
-#define SCHED_BITS	(MAX_RT_PRIO + NICE_WIDTH / 2 + MAX_PRIORITY_ADJ + 1)
-#define IDLE_TASK_SCHED_PRIO	(SCHED_BITS - 1)
+/*
+ * BMQ only routines
+ */
+#define rq_switch_time(rq)	((rq)->clock - (rq)->last_ts_switch)
+#define boost_threshold(p)	(sched_timeslice_ns >>\
+				 (15 - MAX_PRIORITY_ADJ -  (p)->boost_prio))
 
+static inline void boost_task(struct task_struct *p)
+{
+	int limit;
+
+	switch (p->policy) {
+	case SCHED_NORMAL:
+		limit = -MAX_PRIORITY_ADJ;
+		break;
+	case SCHED_BATCH:
+	case SCHED_IDLE:
+		limit = 0;
+		break;
+	default:
+		return;
+	}
+
+	if (p->boost_prio > limit)
+		p->boost_prio--;
+}
+
+static inline void deboost_task(struct task_struct *p)
+{
+	if (p->boost_prio < MAX_PRIORITY_ADJ)
+		p->boost_prio++;
+}
+
+/*
+ * Common interfaces
+ */
+static inline int normal_prio(struct task_struct *p)
+{
+	if (task_has_rt_policy(p))
+		return MAX_RT_PRIO - 1 - p->rt_priority;
+
+	return p->static_prio + MAX_PRIORITY_ADJ;
+}
+
+static inline int task_sched_prio(struct task_struct *p, struct rq *rq)
+{
+	return (p->prio < MAX_RT_PRIO)? p->prio : MAX_RT_PRIO / 2 + (p->prio + p->boost_prio) / 2;
+}
+
+static inline void requeue_task(struct task_struct *p, struct rq *rq);
+
+static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
+{
+	p->time_slice = sched_timeslice_ns;
+
+	if (SCHED_FIFO != p->policy && task_on_rq_queued(p)) {
+		if (SCHED_RR != p->policy)
+			deboost_task(p);
+		requeue_task(p, rq);
+	}
+}
+
+inline int task_running_nice(struct task_struct *p)
+{
+	return (p->prio + p->boost_prio > DEFAULT_PRIO + MAX_PRIORITY_ADJ);
+}
+
+static inline unsigned long sched_queue_watermark(struct rq *rq)
+{
+	return find_first_bit(rq->queue.bitmap, SCHED_BITS);
+}
+
+static inline void sched_queue_init(struct rq *rq)
+{
+	struct sched_queue *q = &rq->queue;
+	int i;
+
+	bitmap_zero(q->bitmap, SCHED_BITS);
+	for(i = 0; i < SCHED_BITS; i++)
+		INIT_LIST_HEAD(&q->heads[i]);
+}
+
+static inline void sched_queue_init_idle(struct rq *rq, struct task_struct *idle)
+{
+	struct sched_queue *q = &rq->queue;
+
+	idle->sq_idx = IDLE_TASK_SCHED_PRIO;
+	INIT_LIST_HEAD(&q->heads[idle->sq_idx]);
+	list_add(&idle->sq_node, &q->heads[idle->sq_idx]);
+	set_bit(idle->sq_idx, q->bitmap);
+}
+
+/*
+ * This routine used in bmq scheduler only which assume the idle task in the bmq
+ */
+static inline struct task_struct *sched_rq_first_task(struct rq *rq)
+{
+	unsigned long idx = find_first_bit(rq->queue.bitmap, SCHED_BITS);
+	const struct list_head *head = &rq->queue.heads[idx];
+
+	return list_first_entry(head, struct task_struct, sq_node);
+}
+
+static inline struct task_struct *
+sched_rq_next_task(struct task_struct *p, struct rq *rq)
+{
+	unsigned long idx = p->sq_idx;
+	struct list_head *head = &rq->queue.heads[idx];
+
+	if (list_is_last(&p->sq_node, head)) {
+		idx = find_next_bit(rq->queue.bitmap, SCHED_BITS, idx + 1);
+		head = &rq->queue.heads[idx];
+
+		return list_first_entry(head, struct task_struct, sq_node);
+	}
+
+	return list_next_entry(p, sq_node);
+}
+
+#define __SCHED_DEQUEUE_TASK(p, rq, flags, func)	\
+	psi_dequeue(p, flags & DEQUEUE_SLEEP);		\
+	sched_info_dequeued(rq, p);			\
+							\
+	list_del(&p->sq_node);				\
+	if (list_empty(&rq->queue.heads[p->sq_idx])) {	\
+		clear_bit(p->sq_idx, rq->queue.bitmap);\
+		func;					\
+	}
+
+#define __SCHED_ENQUEUE_TASK(p, rq, flags)				\
+	sched_info_queued(rq, p);					\
+	psi_enqueue(p, flags);						\
+									\
+	p->sq_idx = task_sched_prio(p, rq);				\
+	list_add_tail(&p->sq_node, &rq->queue.heads[p->sq_idx]);	\
+	set_bit(p->sq_idx, rq->queue.bitmap)
+
+#define __SCHED_REQUEUE_TASK(p, rq, func)				\
+{									\
+	int idx = task_sched_prio(p, rq);				\
+\
+	list_del(&p->sq_node);						\
+	list_add_tail(&p->sq_node, &rq->queue.heads[idx]);		\
+	if (idx != p->sq_idx) {					\
+		if (list_empty(&rq->queue.heads[p->sq_idx]))		\
+			clear_bit(p->sq_idx, rq->queue.bitmap);	\
+		p->sq_idx = idx;					\
+		set_bit(p->sq_idx, rq->queue.bitmap);			\
+		func;							\
+	}								\
+}
+
+static inline bool sched_task_need_requeue(struct task_struct *p, struct rq *rq)
+{
+	return (task_sched_prio(p, rq) != p->sq_idx);
+}
+
+static void sched_task_fork(struct task_struct *p, struct rq *rq)
+{
+	p->boost_prio = (p->boost_prio < 0) ?
+		p->boost_prio + MAX_PRIORITY_ADJ : MAX_PRIORITY_ADJ;
+}
+
+/**
+ * task_prio - return the priority value of a given task.
+ * @p: the task in question.
+ *
+ * Return: The priority value as seen by users in /proc.
+ *
+ * sched policy         return value   kernel prio    user prio/nice/boost
+ *
+ * normal, batch, idle     [0 ... 53]  [100 ... 139]          0/[-20 ... 19]/[-7 ... 7]
+ * fifo, rr             [-1 ... -100]     [99 ... 0]  [0 ... 99]
+ */
+int task_prio(const struct task_struct *p)
+{
+	if (p->prio < MAX_RT_PRIO)
+		return (p->prio - MAX_RT_PRIO);
+	return (p->prio - MAX_RT_PRIO + p->boost_prio);
+}
+
+static void do_sched_yield_type_1(struct task_struct *p, struct rq *rq)
+{
+	p->boost_prio = MAX_PRIORITY_ADJ;
+}
+
+#ifdef CONFIG_SMP
+static void sched_task_ttwu(struct task_struct *p)
+{
+	if(this_rq()->clock_task - p->last_ran > sched_timeslice_ns)
+		boost_task(p);
+}
 #endif
+
+static void sched_task_deactivate(struct task_struct *p, struct rq *rq)
+{
+	if (rq_switch_time(rq) < boost_threshold(p))
+		boost_task(p);
+}
+
+static inline void update_rq_time_edge(struct rq *rq) {}
diff --git a/kernel/sched/bmq_imp.h b/kernel/sched/bmq_imp.h
deleted file mode 100644
index f6bd3421b95c..000000000000
--- a/kernel/sched/bmq_imp.h
+++ /dev/null
@@ -1,203 +0,0 @@
-#define ALT_SCHED_VERSION_MSG "sched/bmq: BMQ CPU Scheduler "ALT_SCHED_VERSION" by Alfred Chen.\n"
-
-/*
- * BMQ only routines
- */
-#define rq_switch_time(rq)	((rq)->clock - (rq)->last_ts_switch)
-#define boost_threshold(p)	(sched_timeslice_ns >>\
-				 (15 - MAX_PRIORITY_ADJ -  (p)->boost_prio))
-
-static inline void boost_task(struct task_struct *p)
-{
-	int limit;
-
-	switch (p->policy) {
-	case SCHED_NORMAL:
-		limit = -MAX_PRIORITY_ADJ;
-		break;
-	case SCHED_BATCH:
-	case SCHED_IDLE:
-		limit = 0;
-		break;
-	default:
-		return;
-	}
-
-	if (p->boost_prio > limit)
-		p->boost_prio--;
-}
-
-static inline void deboost_task(struct task_struct *p)
-{
-	if (p->boost_prio < MAX_PRIORITY_ADJ)
-		p->boost_prio++;
-}
-
-/*
- * Common interfaces
- */
-static inline int normal_prio(struct task_struct *p)
-{
-	if (task_has_rt_policy(p))
-		return MAX_RT_PRIO - 1 - p->rt_priority;
-
-	return p->static_prio + MAX_PRIORITY_ADJ;
-}
-
-static inline int task_sched_prio(struct task_struct *p, struct rq *rq)
-{
-	return (p->prio < MAX_RT_PRIO)? p->prio : MAX_RT_PRIO / 2 + (p->prio + p->boost_prio) / 2;
-}
-
-static inline void requeue_task(struct task_struct *p, struct rq *rq);
-
-static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
-{
-	p->time_slice = sched_timeslice_ns;
-
-	if (SCHED_FIFO != p->policy && task_on_rq_queued(p)) {
-		if (SCHED_RR != p->policy)
-			deboost_task(p);
-		requeue_task(p, rq);
-	}
-}
-
-inline int task_running_nice(struct task_struct *p)
-{
-	return (p->prio + p->boost_prio > DEFAULT_PRIO + MAX_PRIORITY_ADJ);
-}
-
-static inline unsigned long sched_queue_watermark(struct rq *rq)
-{
-	return find_first_bit(rq->queue.bitmap, SCHED_BITS);
-}
-
-static inline void sched_queue_init(struct rq *rq)
-{
-	struct sched_queue *q = &rq->queue;
-	int i;
-
-	bitmap_zero(q->bitmap, SCHED_BITS);
-	for(i = 0; i < SCHED_BITS; i++)
-		INIT_LIST_HEAD(&q->heads[i]);
-}
-
-static inline void sched_queue_init_idle(struct rq *rq, struct task_struct *idle)
-{
-	struct sched_queue *q = &rq->queue;
-
-	idle->sq_idx = IDLE_TASK_SCHED_PRIO;
-	INIT_LIST_HEAD(&q->heads[idle->sq_idx]);
-	list_add(&idle->sq_node, &q->heads[idle->sq_idx]);
-	set_bit(idle->sq_idx, q->bitmap);
-}
-
-/*
- * This routine used in bmq scheduler only which assume the idle task in the bmq
- */
-static inline struct task_struct *sched_rq_first_task(struct rq *rq)
-{
-	unsigned long idx = find_first_bit(rq->queue.bitmap, SCHED_BITS);
-	const struct list_head *head = &rq->queue.heads[idx];
-
-	return list_first_entry(head, struct task_struct, sq_node);
-}
-
-static inline struct task_struct *
-sched_rq_next_task(struct task_struct *p, struct rq *rq)
-{
-	unsigned long idx = p->sq_idx;
-	struct list_head *head = &rq->queue.heads[idx];
-
-	if (list_is_last(&p->sq_node, head)) {
-		idx = find_next_bit(rq->queue.bitmap, SCHED_BITS, idx + 1);
-		head = &rq->queue.heads[idx];
-
-		return list_first_entry(head, struct task_struct, sq_node);
-	}
-
-	return list_next_entry(p, sq_node);
-}
-
-#define __SCHED_DEQUEUE_TASK(p, rq, flags, func)	\
-	psi_dequeue(p, flags & DEQUEUE_SLEEP);		\
-	sched_info_dequeued(rq, p);			\
-							\
-	list_del(&p->sq_node);				\
-	if (list_empty(&rq->queue.heads[p->sq_idx])) {	\
-		clear_bit(p->sq_idx, rq->queue.bitmap);\
-		func;					\
-	}
-
-#define __SCHED_ENQUEUE_TASK(p, rq, flags)				\
-	sched_info_queued(rq, p);					\
-	psi_enqueue(p, flags);						\
-									\
-	p->sq_idx = task_sched_prio(p, rq);				\
-	list_add_tail(&p->sq_node, &rq->queue.heads[p->sq_idx]);	\
-	set_bit(p->sq_idx, rq->queue.bitmap)
-
-#define __SCHED_REQUEUE_TASK(p, rq, func)				\
-{									\
-	int idx = task_sched_prio(p, rq);				\
-\
-	list_del(&p->sq_node);						\
-	list_add_tail(&p->sq_node, &rq->queue.heads[idx]);		\
-	if (idx != p->sq_idx) {					\
-		if (list_empty(&rq->queue.heads[p->sq_idx]))		\
-			clear_bit(p->sq_idx, rq->queue.bitmap);	\
-		p->sq_idx = idx;					\
-		set_bit(p->sq_idx, rq->queue.bitmap);			\
-		func;							\
-	}								\
-}
-
-static inline bool sched_task_need_requeue(struct task_struct *p, struct rq *rq)
-{
-	return (task_sched_prio(p, rq) != p->sq_idx);
-}
-
-static void sched_task_fork(struct task_struct *p, struct rq *rq)
-{
-	p->boost_prio = (p->boost_prio < 0) ?
-		p->boost_prio + MAX_PRIORITY_ADJ : MAX_PRIORITY_ADJ;
-}
-
-/**
- * task_prio - return the priority value of a given task.
- * @p: the task in question.
- *
- * Return: The priority value as seen by users in /proc.
- *
- * sched policy         return value   kernel prio    user prio/nice/boost
- *
- * normal, batch, idle     [0 ... 53]  [100 ... 139]          0/[-20 ... 19]/[-7 ... 7]
- * fifo, rr             [-1 ... -100]     [99 ... 0]  [0 ... 99]
- */
-int task_prio(const struct task_struct *p)
-{
-	if (p->prio < MAX_RT_PRIO)
-		return (p->prio - MAX_RT_PRIO);
-	return (p->prio - MAX_RT_PRIO + p->boost_prio);
-}
-
-static void do_sched_yield_type_1(struct task_struct *p, struct rq *rq)
-{
-	p->boost_prio = MAX_PRIORITY_ADJ;
-}
-
-#ifdef CONFIG_SMP
-static void sched_task_ttwu(struct task_struct *p)
-{
-	if(this_rq()->clock_task - p->last_ran > sched_timeslice_ns)
-		boost_task(p);
-}
-#endif
-
-static void sched_task_deactivate(struct task_struct *p, struct rq *rq)
-{
-	if (rq_switch_time(rq) < boost_threshold(p))
-		boost_task(p);
-}
-
-static inline void update_rq_time_edge(struct rq *rq) {}
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index 623908cf4380..8cc656a7cc48 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -1,9 +1,300 @@
-#ifndef PDS_H
-#define PDS_H
+#define ALT_SCHED_VERSION_MSG "sched/pds: PDS CPU Scheduler "ALT_SCHED_VERSION" by Alfred Chen.\n"
 
-/* bits:
- * RT(0-99), (Low prio adj range, nice width, high prio adj range) / 2, cpu idle task */
-#define SCHED_BITS	(MAX_RT_PRIO + NICE_WIDTH / 2 + 1)
-#define IDLE_TASK_SCHED_PRIO	(SCHED_BITS - 1)
+static const u64 user_prio2deadline[NICE_WIDTH] = {
+/* -20 */	  4194304,   4613734,   5075107,   5582617,   6140878,
+/* -15 */	  6754965,   7430461,   8173507,   8990857,   9889942,
+/* -10 */	 10878936,  11966829,  13163511,  14479862,  15927848,
+/*  -5 */	 17520632,  19272695,  21199964,  23319960,  25651956,
+/*   0 */	 28217151,  31038866,  34142752,  37557027,  41312729,
+/*   5 */	 45444001,  49988401,  54987241,  60485965,  66534561,
+/*  10 */	 73188017,  80506818,  88557499,  97413248, 107154572,
+/*  15 */	117870029, 129657031, 142622734, 156885007, 172573507
+};
 
+#define SCHED_PRIO_SLOT		(4ULL << 20)
+#define DEFAULT_SCHED_PRIO (MAX_RT_PRIO + 10)
+
+static inline int normal_prio(struct task_struct *p)
+{
+	if (task_has_rt_policy(p))
+		return MAX_RT_PRIO - 1 - p->rt_priority;
+
+	return MAX_RT_PRIO;
+}
+
+extern int alt_debug[20];
+
+static inline int
+task_sched_prio_normal(const struct task_struct *p, const struct rq *rq)
+{
+	int delta;
+
+	delta = rq->time_edge + 20 - (p->deadline >> 23);
+	if (delta < 0) {
+		delta = 0;
+		alt_debug[0]++;
+	}
+	delta = 19 - min(delta, 19);
+
+	return delta;
+}
+
+static inline int
+task_sched_prio(const struct task_struct *p, const struct rq *rq)
+{
+	if (p == rq->idle)
+		return IDLE_TASK_SCHED_PRIO;
+
+	if (p->prio < MAX_RT_PRIO)
+		return p->prio;
+
+	return MAX_RT_PRIO + task_sched_prio_normal(p, rq);
+}
+
+static inline int
+task_sched_prio_idx(const struct task_struct *p, const struct rq *rq)
+{
+	if (p == rq->idle)
+		return IDLE_TASK_SCHED_PRIO;
+
+	if (p->prio < MAX_RT_PRIO)
+		return p->prio;
+
+	return MAX_RT_PRIO +
+		(task_sched_prio_normal(p, rq) + rq->time_edge) % 20;
+}
+
+int task_running_nice(struct task_struct *p)
+{
+	return task_sched_prio(p, task_rq(p)) > DEFAULT_SCHED_PRIO;
+}
+
+DECLARE_BITMAP(normal_mask, SCHED_BITS);
+
+static inline void sched_shift_normal_bitmap(unsigned long *mask, unsigned int shift)
+{
+	DECLARE_BITMAP(normal, SCHED_BITS);
+
+	bitmap_and(normal, mask, normal_mask, SCHED_BITS);
+	bitmap_shift_right(normal, normal, shift, SCHED_BITS);
+	bitmap_and(normal, normal, normal_mask, SCHED_BITS);
+
+	bitmap_andnot(mask, mask, normal_mask, SCHED_BITS);
+	bitmap_or(mask, mask, normal, SCHED_BITS);
+}
+
+static inline void update_rq_time_edge(struct rq *rq)
+{
+	struct list_head head;
+	u64 old = rq->time_edge;
+	u64 now = rq->clock >> 23;
+	u64 prio, delta;
+
+	if (now == old)
+		return;
+
+	delta = min(20ULL, now - old);
+	INIT_LIST_HEAD(&head);
+
+	prio = MAX_RT_PRIO;
+	for_each_set_bit_from(prio, rq->queue.bitmap, MAX_RT_PRIO + delta) {
+		u64 idx;
+
+		idx = MAX_RT_PRIO + ((prio - MAX_RT_PRIO) + rq->time_edge) % 20;
+		list_splice_tail_init(rq->queue.heads + idx, &head);
+	}
+	sched_shift_normal_bitmap(rq->queue.bitmap, delta);
+	rq->time_edge = now;
+	if (!list_empty(&head)) {
+		struct task_struct *p;
+
+		list_for_each_entry(p, &head, sq_node)
+			p->sq_idx = MAX_RT_PRIO + now % 20;
+
+		list_splice(&head, rq->queue.heads + MAX_RT_PRIO + now % 20);
+		set_bit(MAX_RT_PRIO, rq->queue.bitmap);
+	}
+}
+
+static inline void sched_renew_deadline(struct task_struct *p, const struct rq *rq)
+{
+	if (p->prio >= MAX_RT_PRIO)
+		p->deadline = rq->clock +
+			SCHED_PRIO_SLOT * (p->static_prio - MAX_RT_PRIO + 1);
+}
+
+static inline void requeue_task(struct task_struct *p, struct rq *rq);
+
+static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
+{
+	/*printk(KERN_INFO "sched: time_slice_expired(%d) - %px\n", cpu_of(rq), p);*/
+	p->time_slice = sched_timeslice_ns;
+	sched_renew_deadline(p, rq);
+	if (SCHED_FIFO != p->policy && task_on_rq_queued(p))
+		requeue_task(p, rq);
+}
+
+/*
+ * Init the queue structure in rq
+ */
+static inline void sched_queue_init(struct rq *rq)
+{
+	struct sched_queue *q = &rq->queue;
+	int i;
+
+	bitmap_set(normal_mask, MAX_RT_PRIO, 20);
+	bitmap_zero(q->bitmap, SCHED_BITS);
+	for(i = 0; i < SCHED_BITS; i++)
+		INIT_LIST_HEAD(&q->heads[i]);
+}
+
+/*
+ * Init idle task and put into queue structure of rq
+ * IMPORTANT: may be called multiple times for a single cpu
+ */
+static inline void sched_queue_init_idle(struct rq *rq, struct task_struct *idle)
+{
+	struct sched_queue *q = &rq->queue;
+	/*printk(KERN_INFO "sched: init(%d) - %px\n", cpu_of(rq), idle);*/
+
+	idle->sq_idx = IDLE_TASK_SCHED_PRIO;
+	INIT_LIST_HEAD(&q->heads[idle->sq_idx]);
+	list_add(&idle->sq_node, &q->heads[idle->sq_idx]);
+	set_bit(idle->sq_idx, q->bitmap);
+}
+
+static inline unsigned long sched_prio2idx(unsigned long idx, struct rq *rq)
+{
+	if (IDLE_TASK_SCHED_PRIO == idx ||
+	    idx < MAX_RT_PRIO)
+		return idx;
+
+	return MAX_RT_PRIO +
+		((idx - MAX_RT_PRIO) + rq->time_edge) % 20;
+}
+
+static inline unsigned long sched_idx2prio(unsigned long idx, struct rq *rq)
+{
+	if (IDLE_TASK_SCHED_PRIO == idx ||
+	    idx < MAX_RT_PRIO)
+		return idx;
+
+	return MAX_RT_PRIO +
+		((idx - MAX_RT_PRIO) + 20 -  rq->time_edge % 20) % 20;
+}
+
+/*
+ * This routine assume that the idle task always in queue
+ */
+static inline struct task_struct *sched_rq_first_task(struct rq *rq)
+{
+	unsigned long idx = find_first_bit(rq->queue.bitmap, SCHED_BITS);
+	const struct list_head *head = &rq->queue.heads[sched_prio2idx(idx, rq)];
+
+	return list_first_entry(head, struct task_struct, sq_node);
+}
+
+static inline struct task_struct *
+sched_rq_next_task(struct task_struct *p, struct rq *rq)
+{
+	unsigned long idx = p->sq_idx;
+	struct list_head *head = &rq->queue.heads[idx];
+
+	if (list_is_last(&p->sq_node, head)) {
+		idx = find_next_bit(rq->queue.bitmap, SCHED_BITS,
+				    sched_idx2prio(idx, rq) + 1);
+		head = &rq->queue.heads[sched_prio2idx(idx, rq)];
+
+		return list_first_entry(head, struct task_struct, sq_node);
+	}
+
+	return list_next_entry(p, sq_node);
+}
+
+static inline unsigned long sched_queue_watermark(struct rq *rq)
+{
+	return find_first_bit(rq->queue.bitmap, SCHED_BITS);
+}
+
+#define __SCHED_DEQUEUE_TASK(p, rq, flags, func)		\
+	psi_dequeue(p, flags & DEQUEUE_SLEEP);			\
+	sched_info_dequeued(rq, p);				\
+								\
+	list_del(&p->sq_node);					\
+	if (list_empty(&rq->queue.heads[p->sq_idx])) {		\
+		clear_bit(sched_idx2prio(p->sq_idx, rq),	\
+			  rq->queue.bitmap);			\
+		func;						\
+	}
+
+#define __SCHED_ENQUEUE_TASK(p, rq, flags)				\
+	sched_info_queued(rq, p);					\
+	psi_enqueue(p, flags);						\
+									\
+	p->sq_idx = task_sched_prio_idx(p, rq);				\
+	list_add_tail(&p->sq_node, &rq->queue.heads[p->sq_idx]);	\
+	set_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);
+
+/*
+ * Requeue a task @p to @rq
+ */
+#define __SCHED_REQUEUE_TASK(p, rq, func)					\
+{\
+	int idx = task_sched_prio_idx(p, rq);					\
+\
+	list_del(&p->sq_node);							\
+	list_add_tail(&p->sq_node, &rq->queue.heads[idx]);			\
+	if (idx != p->sq_idx) {						\
+		if (list_empty(&rq->queue.heads[p->sq_idx]))			\
+			clear_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);		\
+		p->sq_idx = idx;						\
+		set_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);				\
+		func;								\
+	}									\
+}
+
+static inline bool sched_task_need_requeue(struct task_struct *p, struct rq *rq)
+{
+	return (task_sched_prio_idx(p, rq) != p->sq_idx);
+}
+
+static void sched_task_fork(struct task_struct *p, struct rq *rq)
+{
+	sched_renew_deadline(p, rq);
+}
+
+/**
+ * task_prio - return the priority value of a given task.
+ * @p: the task in question.
+ *
+ * Return: The priority value as seen by users in /proc.
+ *
+ * sched policy         return value   kernel prio    user prio/nice
+ *
+ * normal, batch, idle     [0 ... 39]            100          0/[-20 ... 19]
+ * fifo, rr             [-1 ... -100]     [99 ... 0]  [0 ... 99]
+ */
+int task_prio(const struct task_struct *p)
+{
+	int ret;
+
+	if (p->prio < MAX_RT_PRIO)
+		return (p->prio - MAX_RT_PRIO);
+
+	/*preempt_disable();
+	ret = task_sched_prio(p, task_rq(p)) - MAX_RT_PRIO;*/
+	ret = p->static_prio - MAX_RT_PRIO;
+	/*preempt_enable();*/
+
+	return ret;
+}
+
+static void do_sched_yield_type_1(struct task_struct *p, struct rq *rq)
+{
+	time_slice_expired(p, rq);
+}
+
+#ifdef CONFIG_SMP
+static void sched_task_ttwu(struct task_struct *p) {}
 #endif
+static void sched_task_deactivate(struct task_struct *p, struct rq *rq) {}
diff --git a/kernel/sched/pds_imp.h b/kernel/sched/pds_imp.h
deleted file mode 100644
index 8cc656a7cc48..000000000000
--- a/kernel/sched/pds_imp.h
+++ /dev/null
@@ -1,300 +0,0 @@
-#define ALT_SCHED_VERSION_MSG "sched/pds: PDS CPU Scheduler "ALT_SCHED_VERSION" by Alfred Chen.\n"
-
-static const u64 user_prio2deadline[NICE_WIDTH] = {
-/* -20 */	  4194304,   4613734,   5075107,   5582617,   6140878,
-/* -15 */	  6754965,   7430461,   8173507,   8990857,   9889942,
-/* -10 */	 10878936,  11966829,  13163511,  14479862,  15927848,
-/*  -5 */	 17520632,  19272695,  21199964,  23319960,  25651956,
-/*   0 */	 28217151,  31038866,  34142752,  37557027,  41312729,
-/*   5 */	 45444001,  49988401,  54987241,  60485965,  66534561,
-/*  10 */	 73188017,  80506818,  88557499,  97413248, 107154572,
-/*  15 */	117870029, 129657031, 142622734, 156885007, 172573507
-};
-
-#define SCHED_PRIO_SLOT		(4ULL << 20)
-#define DEFAULT_SCHED_PRIO (MAX_RT_PRIO + 10)
-
-static inline int normal_prio(struct task_struct *p)
-{
-	if (task_has_rt_policy(p))
-		return MAX_RT_PRIO - 1 - p->rt_priority;
-
-	return MAX_RT_PRIO;
-}
-
-extern int alt_debug[20];
-
-static inline int
-task_sched_prio_normal(const struct task_struct *p, const struct rq *rq)
-{
-	int delta;
-
-	delta = rq->time_edge + 20 - (p->deadline >> 23);
-	if (delta < 0) {
-		delta = 0;
-		alt_debug[0]++;
-	}
-	delta = 19 - min(delta, 19);
-
-	return delta;
-}
-
-static inline int
-task_sched_prio(const struct task_struct *p, const struct rq *rq)
-{
-	if (p == rq->idle)
-		return IDLE_TASK_SCHED_PRIO;
-
-	if (p->prio < MAX_RT_PRIO)
-		return p->prio;
-
-	return MAX_RT_PRIO + task_sched_prio_normal(p, rq);
-}
-
-static inline int
-task_sched_prio_idx(const struct task_struct *p, const struct rq *rq)
-{
-	if (p == rq->idle)
-		return IDLE_TASK_SCHED_PRIO;
-
-	if (p->prio < MAX_RT_PRIO)
-		return p->prio;
-
-	return MAX_RT_PRIO +
-		(task_sched_prio_normal(p, rq) + rq->time_edge) % 20;
-}
-
-int task_running_nice(struct task_struct *p)
-{
-	return task_sched_prio(p, task_rq(p)) > DEFAULT_SCHED_PRIO;
-}
-
-DECLARE_BITMAP(normal_mask, SCHED_BITS);
-
-static inline void sched_shift_normal_bitmap(unsigned long *mask, unsigned int shift)
-{
-	DECLARE_BITMAP(normal, SCHED_BITS);
-
-	bitmap_and(normal, mask, normal_mask, SCHED_BITS);
-	bitmap_shift_right(normal, normal, shift, SCHED_BITS);
-	bitmap_and(normal, normal, normal_mask, SCHED_BITS);
-
-	bitmap_andnot(mask, mask, normal_mask, SCHED_BITS);
-	bitmap_or(mask, mask, normal, SCHED_BITS);
-}
-
-static inline void update_rq_time_edge(struct rq *rq)
-{
-	struct list_head head;
-	u64 old = rq->time_edge;
-	u64 now = rq->clock >> 23;
-	u64 prio, delta;
-
-	if (now == old)
-		return;
-
-	delta = min(20ULL, now - old);
-	INIT_LIST_HEAD(&head);
-
-	prio = MAX_RT_PRIO;
-	for_each_set_bit_from(prio, rq->queue.bitmap, MAX_RT_PRIO + delta) {
-		u64 idx;
-
-		idx = MAX_RT_PRIO + ((prio - MAX_RT_PRIO) + rq->time_edge) % 20;
-		list_splice_tail_init(rq->queue.heads + idx, &head);
-	}
-	sched_shift_normal_bitmap(rq->queue.bitmap, delta);
-	rq->time_edge = now;
-	if (!list_empty(&head)) {
-		struct task_struct *p;
-
-		list_for_each_entry(p, &head, sq_node)
-			p->sq_idx = MAX_RT_PRIO + now % 20;
-
-		list_splice(&head, rq->queue.heads + MAX_RT_PRIO + now % 20);
-		set_bit(MAX_RT_PRIO, rq->queue.bitmap);
-	}
-}
-
-static inline void sched_renew_deadline(struct task_struct *p, const struct rq *rq)
-{
-	if (p->prio >= MAX_RT_PRIO)
-		p->deadline = rq->clock +
-			SCHED_PRIO_SLOT * (p->static_prio - MAX_RT_PRIO + 1);
-}
-
-static inline void requeue_task(struct task_struct *p, struct rq *rq);
-
-static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
-{
-	/*printk(KERN_INFO "sched: time_slice_expired(%d) - %px\n", cpu_of(rq), p);*/
-	p->time_slice = sched_timeslice_ns;
-	sched_renew_deadline(p, rq);
-	if (SCHED_FIFO != p->policy && task_on_rq_queued(p))
-		requeue_task(p, rq);
-}
-
-/*
- * Init the queue structure in rq
- */
-static inline void sched_queue_init(struct rq *rq)
-{
-	struct sched_queue *q = &rq->queue;
-	int i;
-
-	bitmap_set(normal_mask, MAX_RT_PRIO, 20);
-	bitmap_zero(q->bitmap, SCHED_BITS);
-	for(i = 0; i < SCHED_BITS; i++)
-		INIT_LIST_HEAD(&q->heads[i]);
-}
-
-/*
- * Init idle task and put into queue structure of rq
- * IMPORTANT: may be called multiple times for a single cpu
- */
-static inline void sched_queue_init_idle(struct rq *rq, struct task_struct *idle)
-{
-	struct sched_queue *q = &rq->queue;
-	/*printk(KERN_INFO "sched: init(%d) - %px\n", cpu_of(rq), idle);*/
-
-	idle->sq_idx = IDLE_TASK_SCHED_PRIO;
-	INIT_LIST_HEAD(&q->heads[idle->sq_idx]);
-	list_add(&idle->sq_node, &q->heads[idle->sq_idx]);
-	set_bit(idle->sq_idx, q->bitmap);
-}
-
-static inline unsigned long sched_prio2idx(unsigned long idx, struct rq *rq)
-{
-	if (IDLE_TASK_SCHED_PRIO == idx ||
-	    idx < MAX_RT_PRIO)
-		return idx;
-
-	return MAX_RT_PRIO +
-		((idx - MAX_RT_PRIO) + rq->time_edge) % 20;
-}
-
-static inline unsigned long sched_idx2prio(unsigned long idx, struct rq *rq)
-{
-	if (IDLE_TASK_SCHED_PRIO == idx ||
-	    idx < MAX_RT_PRIO)
-		return idx;
-
-	return MAX_RT_PRIO +
-		((idx - MAX_RT_PRIO) + 20 -  rq->time_edge % 20) % 20;
-}
-
-/*
- * This routine assume that the idle task always in queue
- */
-static inline struct task_struct *sched_rq_first_task(struct rq *rq)
-{
-	unsigned long idx = find_first_bit(rq->queue.bitmap, SCHED_BITS);
-	const struct list_head *head = &rq->queue.heads[sched_prio2idx(idx, rq)];
-
-	return list_first_entry(head, struct task_struct, sq_node);
-}
-
-static inline struct task_struct *
-sched_rq_next_task(struct task_struct *p, struct rq *rq)
-{
-	unsigned long idx = p->sq_idx;
-	struct list_head *head = &rq->queue.heads[idx];
-
-	if (list_is_last(&p->sq_node, head)) {
-		idx = find_next_bit(rq->queue.bitmap, SCHED_BITS,
-				    sched_idx2prio(idx, rq) + 1);
-		head = &rq->queue.heads[sched_prio2idx(idx, rq)];
-
-		return list_first_entry(head, struct task_struct, sq_node);
-	}
-
-	return list_next_entry(p, sq_node);
-}
-
-static inline unsigned long sched_queue_watermark(struct rq *rq)
-{
-	return find_first_bit(rq->queue.bitmap, SCHED_BITS);
-}
-
-#define __SCHED_DEQUEUE_TASK(p, rq, flags, func)		\
-	psi_dequeue(p, flags & DEQUEUE_SLEEP);			\
-	sched_info_dequeued(rq, p);				\
-								\
-	list_del(&p->sq_node);					\
-	if (list_empty(&rq->queue.heads[p->sq_idx])) {		\
-		clear_bit(sched_idx2prio(p->sq_idx, rq),	\
-			  rq->queue.bitmap);			\
-		func;						\
-	}
-
-#define __SCHED_ENQUEUE_TASK(p, rq, flags)				\
-	sched_info_queued(rq, p);					\
-	psi_enqueue(p, flags);						\
-									\
-	p->sq_idx = task_sched_prio_idx(p, rq);				\
-	list_add_tail(&p->sq_node, &rq->queue.heads[p->sq_idx]);	\
-	set_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);
-
-/*
- * Requeue a task @p to @rq
- */
-#define __SCHED_REQUEUE_TASK(p, rq, func)					\
-{\
-	int idx = task_sched_prio_idx(p, rq);					\
-\
-	list_del(&p->sq_node);							\
-	list_add_tail(&p->sq_node, &rq->queue.heads[idx]);			\
-	if (idx != p->sq_idx) {						\
-		if (list_empty(&rq->queue.heads[p->sq_idx]))			\
-			clear_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);		\
-		p->sq_idx = idx;						\
-		set_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);				\
-		func;								\
-	}									\
-}
-
-static inline bool sched_task_need_requeue(struct task_struct *p, struct rq *rq)
-{
-	return (task_sched_prio_idx(p, rq) != p->sq_idx);
-}
-
-static void sched_task_fork(struct task_struct *p, struct rq *rq)
-{
-	sched_renew_deadline(p, rq);
-}
-
-/**
- * task_prio - return the priority value of a given task.
- * @p: the task in question.
- *
- * Return: The priority value as seen by users in /proc.
- *
- * sched policy         return value   kernel prio    user prio/nice
- *
- * normal, batch, idle     [0 ... 39]            100          0/[-20 ... 19]
- * fifo, rr             [-1 ... -100]     [99 ... 0]  [0 ... 99]
- */
-int task_prio(const struct task_struct *p)
-{
-	int ret;
-
-	if (p->prio < MAX_RT_PRIO)
-		return (p->prio - MAX_RT_PRIO);
-
-	/*preempt_disable();
-	ret = task_sched_prio(p, task_rq(p)) - MAX_RT_PRIO;*/
-	ret = p->static_prio - MAX_RT_PRIO;
-	/*preempt_enable();*/
-
-	return ret;
-}
-
-static void do_sched_yield_type_1(struct task_struct *p, struct rq *rq)
-{
-	time_slice_expired(p, rq);
-}
-
-#ifdef CONFIG_SMP
-static void sched_task_ttwu(struct task_struct *p) {}
-#endif
-static void sched_task_deactivate(struct task_struct *p, struct rq *rq) {}
-- 
2.32.0.93.g670b81a890


From d7def68e17e5b719b5d489a07c8934b55d48bf87 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Mon, 17 May 2021 16:55:30 +0000
Subject: [PATCH 08/30] sched/alt: sched_queue_init_idle() share common code

---
 kernel/sched/alt_core.c | 15 ++++++++++++++-
 kernel/sched/bmq.h      | 10 ----------
 kernel/sched/pds.h      | 15 ---------------
 3 files changed, 14 insertions(+), 26 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 9ade1b64aa9c..407d5d441298 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -6045,6 +6045,19 @@ void dump_cpu_task(int cpu)
 	sched_show_task(cpu_curr(cpu));
 }
 
+/*
+ * Init idle task and put into queue structure of rq
+ * IMPORTANT: may be called multiple times for a single cpu
+ */
+static inline void sched_queue_init_idle(struct sched_queue *q,
+					 struct task_struct *idle)
+{
+	idle->sq_idx = IDLE_TASK_SCHED_PRIO;
+	INIT_LIST_HEAD(&q->heads[idle->sq_idx]);
+	list_add(&idle->sq_node, &q->heads[idle->sq_idx]);
+	set_bit(idle->sq_idx, q->bitmap);
+}
+
 /**
  * init_idle - set up an idle thread for a given CPU
  * @idle: task in question
@@ -6067,7 +6080,7 @@ void init_idle(struct task_struct *idle, int cpu)
 	idle->last_ran = rq->clock_task;
 	idle->state = TASK_RUNNING;
 	idle->flags |= PF_IDLE;
-	sched_queue_init_idle(rq, idle);
+	sched_queue_init_idle(&rq->queue, idle);
 
 	scs_task_reset(idle);
 	kasan_unpoison_task_stack(idle);
diff --git a/kernel/sched/bmq.h b/kernel/sched/bmq.h
index f6bd3421b95c..f5bd651a7666 100644
--- a/kernel/sched/bmq.h
+++ b/kernel/sched/bmq.h
@@ -82,16 +82,6 @@ static inline void sched_queue_init(struct rq *rq)
 		INIT_LIST_HEAD(&q->heads[i]);
 }
 
-static inline void sched_queue_init_idle(struct rq *rq, struct task_struct *idle)
-{
-	struct sched_queue *q = &rq->queue;
-
-	idle->sq_idx = IDLE_TASK_SCHED_PRIO;
-	INIT_LIST_HEAD(&q->heads[idle->sq_idx]);
-	list_add(&idle->sq_node, &q->heads[idle->sq_idx]);
-	set_bit(idle->sq_idx, q->bitmap);
-}
-
 /*
  * This routine used in bmq scheduler only which assume the idle task in the bmq
  */
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index 8cc656a7cc48..c29122334bda 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -148,21 +148,6 @@ static inline void sched_queue_init(struct rq *rq)
 		INIT_LIST_HEAD(&q->heads[i]);
 }
 
-/*
- * Init idle task and put into queue structure of rq
- * IMPORTANT: may be called multiple times for a single cpu
- */
-static inline void sched_queue_init_idle(struct rq *rq, struct task_struct *idle)
-{
-	struct sched_queue *q = &rq->queue;
-	/*printk(KERN_INFO "sched: init(%d) - %px\n", cpu_of(rq), idle);*/
-
-	idle->sq_idx = IDLE_TASK_SCHED_PRIO;
-	INIT_LIST_HEAD(&q->heads[idle->sq_idx]);
-	list_add(&idle->sq_node, &q->heads[idle->sq_idx]);
-	set_bit(idle->sq_idx, q->bitmap);
-}
-
 static inline unsigned long sched_prio2idx(unsigned long idx, struct rq *rq)
 {
 	if (IDLE_TASK_SCHED_PRIO == idx ||
-- 
2.32.0.93.g670b81a890


From 90174a907e95399a86300b51afe31bdd7e80b412 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Tue, 18 May 2021 10:40:43 +0000
Subject: [PATCH 09/30] sched/alt: Merge BMQ&PDS common code.

---
 kernel/sched/alt_core.c | 43 ++++++++++++-------
 kernel/sched/bmq.h      | 17 +-------
 kernel/sched/pds.h      | 91 ++++++++++++++++++-----------------------
 3 files changed, 70 insertions(+), 81 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 407d5d441298..c81a9fc6a140 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -149,9 +149,34 @@ static cpumask_t sched_rq_watermark[SCHED_BITS] ____cacheline_aligned_in_smp;
 #include "pds.h"
 #endif
 
+/* sched_queue related functions */
+static inline void sched_queue_init(struct sched_queue *q)
+{
+	int i;
+
+	bitmap_zero(q->bitmap, SCHED_BITS);
+	for(i = 0; i < SCHED_BITS; i++)
+		INIT_LIST_HEAD(&q->heads[i]);
+}
+
+/*
+ * Init idle task and put into queue structure of rq
+ * IMPORTANT: may be called multiple times for a single cpu
+ */
+static inline void sched_queue_init_idle(struct sched_queue *q,
+					 struct task_struct *idle)
+{
+	idle->sq_idx = IDLE_TASK_SCHED_PRIO;
+	INIT_LIST_HEAD(&q->heads[idle->sq_idx]);
+	list_add(&idle->sq_node, &q->heads[idle->sq_idx]);
+	set_bit(idle->sq_idx, q->bitmap);
+}
+
+
+/* water mark related functions*/
 static inline void update_sched_rq_watermark(struct rq *rq)
 {
-	unsigned long watermark = sched_queue_watermark(rq);
+	unsigned long watermark = find_first_bit(rq->queue.bitmap, SCHED_BITS);
 	unsigned long last_wm = rq->watermark;
 	unsigned long i;
 	int cpu;
@@ -6045,19 +6070,6 @@ void dump_cpu_task(int cpu)
 	sched_show_task(cpu_curr(cpu));
 }
 
-/*
- * Init idle task and put into queue structure of rq
- * IMPORTANT: may be called multiple times for a single cpu
- */
-static inline void sched_queue_init_idle(struct sched_queue *q,
-					 struct task_struct *idle)
-{
-	idle->sq_idx = IDLE_TASK_SCHED_PRIO;
-	INIT_LIST_HEAD(&q->heads[idle->sq_idx]);
-	list_add(&idle->sq_node, &q->heads[idle->sq_idx]);
-	set_bit(idle->sq_idx, q->bitmap);
-}
-
 /**
  * init_idle - set up an idle thread for a given CPU
  * @idle: task in question
@@ -6677,6 +6689,7 @@ void __init sched_init(void)
 	struct rq *rq;
 
 	printk(KERN_INFO ALT_SCHED_VERSION_MSG);
+	sched_imp_init();
 
 	wait_bit_init();
 
@@ -6695,7 +6708,7 @@ void __init sched_init(void)
 	for_each_possible_cpu(i) {
 		rq = cpu_rq(i);
 
-		sched_queue_init(rq);
+		sched_queue_init(&rq->queue);
 		rq->watermark = IDLE_WM;
 		rq->skip = NULL;
 
diff --git a/kernel/sched/bmq.h b/kernel/sched/bmq.h
index f5bd651a7666..7858ac1185ce 100644
--- a/kernel/sched/bmq.h
+++ b/kernel/sched/bmq.h
@@ -62,26 +62,13 @@ static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
 	}
 }
 
+static inline void sched_imp_init(void) {}
+
 inline int task_running_nice(struct task_struct *p)
 {
 	return (p->prio + p->boost_prio > DEFAULT_PRIO + MAX_PRIORITY_ADJ);
 }
 
-static inline unsigned long sched_queue_watermark(struct rq *rq)
-{
-	return find_first_bit(rq->queue.bitmap, SCHED_BITS);
-}
-
-static inline void sched_queue_init(struct rq *rq)
-{
-	struct sched_queue *q = &rq->queue;
-	int i;
-
-	bitmap_zero(q->bitmap, SCHED_BITS);
-	for(i = 0; i < SCHED_BITS; i++)
-		INIT_LIST_HEAD(&q->heads[i]);
-}
-
 /*
  * This routine used in bmq scheduler only which assume the idle task in the bmq
  */
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index c29122334bda..64631b2770fe 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -14,13 +14,7 @@ static const u64 user_prio2deadline[NICE_WIDTH] = {
 #define SCHED_PRIO_SLOT		(4ULL << 20)
 #define DEFAULT_SCHED_PRIO (MAX_RT_PRIO + 10)
 
-static inline int normal_prio(struct task_struct *p)
-{
-	if (task_has_rt_policy(p))
-		return MAX_RT_PRIO - 1 - p->rt_priority;
-
-	return MAX_RT_PRIO;
-}
+DECLARE_BITMAP(normal_mask, SCHED_BITS);
 
 extern int alt_debug[20];
 
@@ -64,13 +58,49 @@ task_sched_prio_idx(const struct task_struct *p, const struct rq *rq)
 		(task_sched_prio_normal(p, rq) + rq->time_edge) % 20;
 }
 
+static inline unsigned long sched_prio2idx(unsigned long idx, struct rq *rq)
+{
+	if (IDLE_TASK_SCHED_PRIO == idx ||
+	    idx < MAX_RT_PRIO)
+		return idx;
+
+	return MAX_RT_PRIO +
+		((idx - MAX_RT_PRIO) + rq->time_edge) % 20;
+}
+
+static inline unsigned long sched_idx2prio(unsigned long idx, struct rq *rq)
+{
+	if (IDLE_TASK_SCHED_PRIO == idx ||
+	    idx < MAX_RT_PRIO)
+		return idx;
+
+	return MAX_RT_PRIO +
+		((idx - MAX_RT_PRIO) + 20 -  rq->time_edge % 20) % 20;
+}
+
+static inline void sched_renew_deadline(struct task_struct *p, const struct rq *rq)
+{
+	if (p->prio >= MAX_RT_PRIO)
+		p->deadline = rq->clock +
+			SCHED_PRIO_SLOT * (p->static_prio - MAX_RT_PRIO + 1);
+}
+
+/*
+ * Common interfaces
+ */
+static inline int normal_prio(struct task_struct *p)
+{
+	if (task_has_rt_policy(p))
+		return MAX_RT_PRIO - 1 - p->rt_priority;
+
+	return MAX_RT_PRIO;
+}
+
 int task_running_nice(struct task_struct *p)
 {
 	return task_sched_prio(p, task_rq(p)) > DEFAULT_SCHED_PRIO;
 }
 
-DECLARE_BITMAP(normal_mask, SCHED_BITS);
-
 static inline void sched_shift_normal_bitmap(unsigned long *mask, unsigned int shift)
 {
 	DECLARE_BITMAP(normal, SCHED_BITS);
@@ -116,13 +146,6 @@ static inline void update_rq_time_edge(struct rq *rq)
 	}
 }
 
-static inline void sched_renew_deadline(struct task_struct *p, const struct rq *rq)
-{
-	if (p->prio >= MAX_RT_PRIO)
-		p->deadline = rq->clock +
-			SCHED_PRIO_SLOT * (p->static_prio - MAX_RT_PRIO + 1);
-}
-
 static inline void requeue_task(struct task_struct *p, struct rq *rq);
 
 static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
@@ -134,38 +157,9 @@ static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
 		requeue_task(p, rq);
 }
 
-/*
- * Init the queue structure in rq
- */
-static inline void sched_queue_init(struct rq *rq)
+static inline void sched_imp_init(void)
 {
-	struct sched_queue *q = &rq->queue;
-	int i;
-
 	bitmap_set(normal_mask, MAX_RT_PRIO, 20);
-	bitmap_zero(q->bitmap, SCHED_BITS);
-	for(i = 0; i < SCHED_BITS; i++)
-		INIT_LIST_HEAD(&q->heads[i]);
-}
-
-static inline unsigned long sched_prio2idx(unsigned long idx, struct rq *rq)
-{
-	if (IDLE_TASK_SCHED_PRIO == idx ||
-	    idx < MAX_RT_PRIO)
-		return idx;
-
-	return MAX_RT_PRIO +
-		((idx - MAX_RT_PRIO) + rq->time_edge) % 20;
-}
-
-static inline unsigned long sched_idx2prio(unsigned long idx, struct rq *rq)
-{
-	if (IDLE_TASK_SCHED_PRIO == idx ||
-	    idx < MAX_RT_PRIO)
-		return idx;
-
-	return MAX_RT_PRIO +
-		((idx - MAX_RT_PRIO) + 20 -  rq->time_edge % 20) % 20;
 }
 
 /*
@@ -196,11 +190,6 @@ sched_rq_next_task(struct task_struct *p, struct rq *rq)
 	return list_next_entry(p, sq_node);
 }
 
-static inline unsigned long sched_queue_watermark(struct rq *rq)
-{
-	return find_first_bit(rq->queue.bitmap, SCHED_BITS);
-}
-
 #define __SCHED_DEQUEUE_TASK(p, rq, flags, func)		\
 	psi_dequeue(p, flags & DEQUEUE_SLEEP);			\
 	sched_info_dequeued(rq, p);				\
-- 
2.32.0.93.g670b81a890


From fed43ee8ba3e20a4a2f2938dd1d78fcdb8184dee Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Wed, 19 May 2021 10:56:37 +0000
Subject: [PATCH 10/30] sched/pds: Refine task_sched_prio() and
 task_sched_prio_idx()

idle task should never be queued/dequued/requeued or be woken.
---
 kernel/sched/pds.h | 20 +++++++-------------
 1 file changed, 7 insertions(+), 13 deletions(-)

diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index 64631b2770fe..62b5ab738876 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -36,26 +36,20 @@ task_sched_prio_normal(const struct task_struct *p, const struct rq *rq)
 static inline int
 task_sched_prio(const struct task_struct *p, const struct rq *rq)
 {
-	if (p == rq->idle)
-		return IDLE_TASK_SCHED_PRIO;
-
-	if (p->prio < MAX_RT_PRIO)
-		return p->prio;
+	if (p->prio >= MAX_RT_PRIO)
+		return MAX_RT_PRIO + task_sched_prio_normal(p, rq);
 
-	return MAX_RT_PRIO + task_sched_prio_normal(p, rq);
+	return p->prio;
 }
 
 static inline int
 task_sched_prio_idx(const struct task_struct *p, const struct rq *rq)
 {
-	if (p == rq->idle)
-		return IDLE_TASK_SCHED_PRIO;
-
-	if (p->prio < MAX_RT_PRIO)
-		return p->prio;
+	if (p->prio >= MAX_RT_PRIO)
+		return MAX_RT_PRIO +
+			(task_sched_prio_normal(p, rq) + rq->time_edge) % 20;
 
-	return MAX_RT_PRIO +
-		(task_sched_prio_normal(p, rq) + rq->time_edge) % 20;
+	return p->prio;
 }
 
 static inline unsigned long sched_prio2idx(unsigned long idx, struct rq *rq)
-- 
2.32.0.93.g670b81a890


From d71607f359ceb721f570164479fb843a2e730858 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Mon, 24 May 2021 21:46:42 +0000
Subject: [PATCH 11/30] sched/pds: Fix unexpected larger delta in
 task_sched_prio_normal()

---
 kernel/sched/alt_core.c | 23 ++++++++++++-----------
 kernel/sched/bmq.h      | 10 ++++++----
 kernel/sched/pds.h      | 39 +++++++++++++++++++++------------------
 3 files changed, 39 insertions(+), 33 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index c81a9fc6a140..21dc24e855eb 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -1328,6 +1328,7 @@ static struct rq *move_queued_task(struct rq *rq, struct task_struct *p, int
 
 	raw_spin_lock(&rq->lock);
 	BUG_ON(task_cpu(p) != new_cpu);
+	sched_task_sanity_check(p, rq);
 	enqueue_task(p, rq, 0);
 	p->on_rq = TASK_ON_RQ_QUEUED;
 	check_preempt_curr(rq);
@@ -1656,7 +1657,7 @@ static int select_fallback_rq(int cpu, struct task_struct *p)
 	return dest_cpu;
 }
 
-static inline int select_task_rq(struct task_struct *p, struct rq *rq)
+static inline int select_task_rq(struct task_struct *p)
 {
 	cpumask_t chk_mask, tmp;
 
@@ -1669,7 +1670,7 @@ static inline int select_task_rq(struct task_struct *p, struct rq *rq)
 #endif
 	    cpumask_and(&tmp, &chk_mask, &sched_rq_watermark[IDLE_WM]) ||
 	    cpumask_and(&tmp, &chk_mask,
-			&sched_rq_watermark[task_sched_prio(p, rq) + 1]))
+			&sched_rq_watermark[task_sched_prio(p) + 1]))
 		return best_mask_cpu(task_cpu(p), &tmp);
 
 	return best_mask_cpu(task_cpu(p), &chk_mask);
@@ -1823,7 +1824,7 @@ EXPORT_SYMBOL_GPL(set_cpus_allowed_ptr);
 
 #else /* CONFIG_SMP */
 
-static inline int select_task_rq(struct task_struct *p, struct rq *rq)
+static inline int select_task_rq(struct task_struct *p)
 {
 	return 0;
 }
@@ -2360,7 +2361,7 @@ static int try_to_wake_up(struct task_struct *p, unsigned int state,
 
 	sched_task_ttwu(p);
 
-	cpu = select_task_rq(p, this_rq());
+	cpu = select_task_rq(p);
 
 	if (cpu != task_cpu(p)) {
 		if (p->in_iowait) {
@@ -2662,7 +2663,7 @@ void wake_up_new_task(struct task_struct *p)
 
 	p->state = TASK_RUNNING;
 
-	rq = cpu_rq(select_task_rq(p, this_rq()));
+	rq = cpu_rq(select_task_rq(p));
 #ifdef CONFIG_SMP
 	rseq_migrate(p);
 	/*
@@ -3265,7 +3266,7 @@ void sched_exec(void)
 	if (rq != task_rq(p) || rq->nr_running < 2)
 		goto unlock;
 
-	dest_cpu = select_task_rq(p, task_rq(p));
+	dest_cpu = select_task_rq(p);
 	if (dest_cpu == smp_processor_id())
 		goto unlock;
 
@@ -3847,7 +3848,7 @@ inline void alt_sched_debug(void)
 {
 	int i;
 
-	for (i = 0; i < 3; i++)
+	for (i = 0; i < 6; i++)
 		printk(KERN_INFO "sched: %d\n", alt_debug[i]);
 }
 #endif
@@ -4562,7 +4563,7 @@ int default_wake_function(wait_queue_entry_t *curr, unsigned mode, int wake_flag
 }
 EXPORT_SYMBOL(default_wake_function);
 
-static inline void check_task_changed(struct rq *rq, struct task_struct *p)
+static inline void check_task_changed(struct task_struct *p, struct rq *rq)
 {
 	/* Trigger resched if task sched_prio has been modified. */
 	if (task_on_rq_queued(p) && sched_task_need_requeue(p, rq)) {
@@ -4654,7 +4655,7 @@ void rt_mutex_setprio(struct task_struct *p, struct task_struct *pi_task)
 	trace_sched_pi_setprio(p, pi_task);
 	p->prio = prio;
 
-	check_task_changed(rq, p);
+	check_task_changed(p, rq);
 out_unlock:
 	/* Avoid rq from going away on us: */
 	preempt_disable();
@@ -4698,7 +4699,7 @@ void set_user_nice(struct task_struct *p, long nice)
 
 	p->prio = effective_prio(p);
 
-	check_task_changed(rq, p);
+	check_task_changed(p, rq);
 out_unlock:
 	__task_access_unlock(p, lock);
 	raw_spin_unlock_irqrestore(&p->pi_lock, flags);
@@ -5027,7 +5028,7 @@ static int __sched_setscheduler(struct task_struct *p,
 
 	__setscheduler(rq, p, attr, pi);
 
-	check_task_changed(rq, p);
+	check_task_changed(p, rq);
 
 	/* Avoid rq from going away on us: */
 	preempt_disable();
diff --git a/kernel/sched/bmq.h b/kernel/sched/bmq.h
index 7858ac1185ce..eea8cb31ca1a 100644
--- a/kernel/sched/bmq.h
+++ b/kernel/sched/bmq.h
@@ -44,7 +44,7 @@ static inline int normal_prio(struct task_struct *p)
 	return p->static_prio + MAX_PRIORITY_ADJ;
 }
 
-static inline int task_sched_prio(struct task_struct *p, struct rq *rq)
+static inline int task_sched_prio(struct task_struct *p)
 {
 	return (p->prio < MAX_RT_PRIO)? p->prio : MAX_RT_PRIO / 2 + (p->prio + p->boost_prio) / 2;
 }
@@ -62,6 +62,8 @@ static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
 	}
 }
 
+static inline void sched_task_sanity_check(struct task_struct *p, struct rq *rq) {}
+
 static inline void sched_imp_init(void) {}
 
 inline int task_running_nice(struct task_struct *p)
@@ -110,13 +112,13 @@ sched_rq_next_task(struct task_struct *p, struct rq *rq)
 	sched_info_queued(rq, p);					\
 	psi_enqueue(p, flags);						\
 									\
-	p->sq_idx = task_sched_prio(p, rq);				\
+	p->sq_idx = task_sched_prio(p);					\
 	list_add_tail(&p->sq_node, &rq->queue.heads[p->sq_idx]);	\
 	set_bit(p->sq_idx, rq->queue.bitmap)
 
 #define __SCHED_REQUEUE_TASK(p, rq, func)				\
 {									\
-	int idx = task_sched_prio(p, rq);				\
+	int idx = task_sched_prio(p);					\
 \
 	list_del(&p->sq_node);						\
 	list_add_tail(&p->sq_node, &rq->queue.heads[idx]);		\
@@ -131,7 +133,7 @@ sched_rq_next_task(struct task_struct *p, struct rq *rq)
 
 static inline bool sched_task_need_requeue(struct task_struct *p, struct rq *rq)
 {
-	return (task_sched_prio(p, rq) != p->sq_idx);
+	return (task_sched_prio(p) != p->sq_idx);
 }
 
 static void sched_task_fork(struct task_struct *p, struct rq *rq)
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index 62b5ab738876..7eac80b83fb3 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -21,23 +21,22 @@ extern int alt_debug[20];
 static inline int
 task_sched_prio_normal(const struct task_struct *p, const struct rq *rq)
 {
-	int delta;
+	int delta = (p->deadline >> 23) - rq->time_edge  - 1;
 
-	delta = rq->time_edge + 20 - (p->deadline >> 23);
-	if (delta < 0) {
-		delta = 0;
-		alt_debug[0]++;
+	if (unlikely(delta > 19)) {
+		pr_info("pds: task_sched_prio_normal delta %d, deadline %llu(%llu), time_edge %llu\n",
+			delta, p->deadline, p->deadline >> 23, rq->time_edge);
+		delta = 19;
 	}
-	delta = 19 - min(delta, 19);
 
-	return delta;
+	return (delta < 0)? 0:delta;
 }
 
 static inline int
-task_sched_prio(const struct task_struct *p, const struct rq *rq)
+task_sched_prio(const struct task_struct *p)
 {
 	if (p->prio >= MAX_RT_PRIO)
-		return MAX_RT_PRIO + task_sched_prio_normal(p, rq);
+		return MAX_RT_PRIO + task_sched_prio_normal(p, task_rq(p));
 
 	return p->prio;
 }
@@ -92,7 +91,7 @@ static inline int normal_prio(struct task_struct *p)
 
 int task_running_nice(struct task_struct *p)
 {
-	return task_sched_prio(p, task_rq(p)) > DEFAULT_SCHED_PRIO;
+	return task_sched_prio(p) > DEFAULT_SCHED_PRIO;
 }
 
 static inline void sched_shift_normal_bitmap(unsigned long *mask, unsigned int shift)
@@ -117,7 +116,7 @@ static inline void update_rq_time_edge(struct rq *rq)
 	if (now == old)
 		return;
 
-	delta = min(20ULL, now - old);
+	delta = min_t(u64, 20, now - old);
 	INIT_LIST_HEAD(&head);
 
 	prio = MAX_RT_PRIO;
@@ -151,6 +150,12 @@ static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
 		requeue_task(p, rq);
 }
 
+static inline void sched_task_sanity_check(struct task_struct *p, struct rq *rq)
+{
+	if (unlikely(p->deadline > rq->clock + 40 * SCHED_PRIO_SLOT))
+		p->deadline = rq->clock + 40 * SCHED_PRIO_SLOT;
+}
+
 static inline void sched_imp_init(void)
 {
 	bitmap_set(normal_mask, MAX_RT_PRIO, 20);
@@ -212,11 +217,12 @@ sched_rq_next_task(struct task_struct *p, struct rq *rq)
 \
 	list_del(&p->sq_node);							\
 	list_add_tail(&p->sq_node, &rq->queue.heads[idx]);			\
-	if (idx != p->sq_idx) {						\
+	if (idx != p->sq_idx) {							\
 		if (list_empty(&rq->queue.heads[p->sq_idx]))			\
-			clear_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);		\
+			clear_bit(sched_idx2prio(p->sq_idx, rq),		\
+				  rq->queue.bitmap);				\
 		p->sq_idx = idx;						\
-		set_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);				\
+		set_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);	\
 		func;								\
 	}									\
 }
@@ -249,10 +255,7 @@ int task_prio(const struct task_struct *p)
 	if (p->prio < MAX_RT_PRIO)
 		return (p->prio - MAX_RT_PRIO);
 
-	/*preempt_disable();
-	ret = task_sched_prio(p, task_rq(p)) - MAX_RT_PRIO;*/
-	ret = p->static_prio - MAX_RT_PRIO;
-	/*preempt_enable();*/
+	ret = task_sched_prio(p) - MAX_RT_PRIO;
 
 	return ret;
 }
-- 
2.32.0.93.g670b81a890


From 41898967b95066954163d72824972fc127c656cd Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Tue, 25 May 2021 09:56:33 +0000
Subject: [PATCH 12/30] sched/pds: Rewrite
 task_sched_prio/task_sched_prio_idx/sched_prio2idx/sched_idx2prio

---
 kernel/sched/pds.h | 27 +++++++--------------------
 1 file changed, 7 insertions(+), 20 deletions(-)

diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index 7eac80b83fb3..8a1841e52e91 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -35,39 +35,26 @@ task_sched_prio_normal(const struct task_struct *p, const struct rq *rq)
 static inline int
 task_sched_prio(const struct task_struct *p)
 {
-	if (p->prio >= MAX_RT_PRIO)
-		return MAX_RT_PRIO + task_sched_prio_normal(p, task_rq(p));
-
-	return p->prio;
+	return (p->prio < MAX_RT_PRIO) ? p->prio :
+		MAX_RT_PRIO + task_sched_prio_normal(p, task_rq(p));
 }
 
 static inline int
 task_sched_prio_idx(const struct task_struct *p, const struct rq *rq)
 {
-	if (p->prio >= MAX_RT_PRIO)
-		return MAX_RT_PRIO +
-			(task_sched_prio_normal(p, rq) + rq->time_edge) % 20;
-
-	return p->prio;
+	return (p->prio < MAX_RT_PRIO) ? p->prio : MAX_RT_PRIO +
+		(task_sched_prio_normal(p, rq) + rq->time_edge) % 20;
 }
 
 static inline unsigned long sched_prio2idx(unsigned long idx, struct rq *rq)
 {
-	if (IDLE_TASK_SCHED_PRIO == idx ||
-	    idx < MAX_RT_PRIO)
-		return idx;
-
-	return MAX_RT_PRIO +
-		((idx - MAX_RT_PRIO) + rq->time_edge) % 20;
+	return (IDLE_TASK_SCHED_PRIO == idx || idx < MAX_RT_PRIO) ? idx :
+		MAX_RT_PRIO + ((idx - MAX_RT_PRIO) + rq->time_edge) % 20;
 }
 
 static inline unsigned long sched_idx2prio(unsigned long idx, struct rq *rq)
 {
-	if (IDLE_TASK_SCHED_PRIO == idx ||
-	    idx < MAX_RT_PRIO)
-		return idx;
-
-	return MAX_RT_PRIO +
+	return (idx < MAX_RT_PRIO) ? idx : MAX_RT_PRIO +
 		((idx - MAX_RT_PRIO) + 20 -  rq->time_edge % 20) % 20;
 }
 
-- 
2.32.0.93.g670b81a890


From bce08b8d4bb46a3164869da2ea0b353bc2c51a5f Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Tue, 25 May 2021 10:51:11 +0000
Subject: [PATCH 13/30] sched/alt: One less bit for sched_queue.bitmap

---
 kernel/sched/alt_core.c  |  3 +--
 kernel/sched/alt_sched.h |  4 +++-
 kernel/sched/bmq.h       | 10 +++++-----
 kernel/sched/pds.h       |  4 ++--
 4 files changed, 11 insertions(+), 10 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 21dc24e855eb..8fd6fd9ec2ea 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -169,14 +169,13 @@ static inline void sched_queue_init_idle(struct sched_queue *q,
 	idle->sq_idx = IDLE_TASK_SCHED_PRIO;
 	INIT_LIST_HEAD(&q->heads[idle->sq_idx]);
 	list_add(&idle->sq_node, &q->heads[idle->sq_idx]);
-	set_bit(idle->sq_idx, q->bitmap);
 }
 
 
 /* water mark related functions*/
 static inline void update_sched_rq_watermark(struct rq *rq)
 {
-	unsigned long watermark = find_first_bit(rq->queue.bitmap, SCHED_BITS);
+	unsigned long watermark = find_first_bit(rq->queue.bitmap, SCHED_QUEUE_BITS);
 	unsigned long last_wm = rq->watermark;
 	unsigned long i;
 	int cpu;
diff --git a/kernel/sched/alt_sched.h b/kernel/sched/alt_sched.h
index 58ff6212b446..76ec6f9c737b 100644
--- a/kernel/sched/alt_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -136,8 +136,10 @@ static inline int task_on_rq_migrating(struct task_struct *p)
 #define WF_MIGRATED	0x04		/* internal use, task got migrated */
 #define WF_ON_CPU	0x08		/* Wakee is on_rq */
 
+#define SCHED_QUEUE_BITS	(SCHED_BITS - 1)
+
 struct sched_queue {
-	DECLARE_BITMAP(bitmap, SCHED_BITS);
+	DECLARE_BITMAP(bitmap, SCHED_QUEUE_BITS);
 	struct list_head heads[SCHED_BITS];
 };
 
diff --git a/kernel/sched/bmq.h b/kernel/sched/bmq.h
index eea8cb31ca1a..85e4c477eda8 100644
--- a/kernel/sched/bmq.h
+++ b/kernel/sched/bmq.h
@@ -76,7 +76,7 @@ inline int task_running_nice(struct task_struct *p)
  */
 static inline struct task_struct *sched_rq_first_task(struct rq *rq)
 {
-	unsigned long idx = find_first_bit(rq->queue.bitmap, SCHED_BITS);
+	unsigned long idx = find_first_bit(rq->queue.bitmap, SCHED_QUEUE_BITS);
 	const struct list_head *head = &rq->queue.heads[idx];
 
 	return list_first_entry(head, struct task_struct, sq_node);
@@ -89,7 +89,7 @@ sched_rq_next_task(struct task_struct *p, struct rq *rq)
 	struct list_head *head = &rq->queue.heads[idx];
 
 	if (list_is_last(&p->sq_node, head)) {
-		idx = find_next_bit(rq->queue.bitmap, SCHED_BITS, idx + 1);
+		idx = find_next_bit(rq->queue.bitmap, SCHED_QUEUE_BITS, idx + 1);
 		head = &rq->queue.heads[idx];
 
 		return list_first_entry(head, struct task_struct, sq_node);
@@ -104,7 +104,7 @@ sched_rq_next_task(struct task_struct *p, struct rq *rq)
 							\
 	list_del(&p->sq_node);				\
 	if (list_empty(&rq->queue.heads[p->sq_idx])) {	\
-		clear_bit(p->sq_idx, rq->queue.bitmap);\
+		clear_bit(p->sq_idx, rq->queue.bitmap);	\
 		func;					\
 	}
 
@@ -122,9 +122,9 @@ sched_rq_next_task(struct task_struct *p, struct rq *rq)
 \
 	list_del(&p->sq_node);						\
 	list_add_tail(&p->sq_node, &rq->queue.heads[idx]);		\
-	if (idx != p->sq_idx) {					\
+	if (idx != p->sq_idx) {						\
 		if (list_empty(&rq->queue.heads[p->sq_idx]))		\
-			clear_bit(p->sq_idx, rq->queue.bitmap);	\
+			clear_bit(p->sq_idx, rq->queue.bitmap);		\
 		p->sq_idx = idx;					\
 		set_bit(p->sq_idx, rq->queue.bitmap);			\
 		func;							\
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index 8a1841e52e91..ee3d5cfac781 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -153,7 +153,7 @@ static inline void sched_imp_init(void)
  */
 static inline struct task_struct *sched_rq_first_task(struct rq *rq)
 {
-	unsigned long idx = find_first_bit(rq->queue.bitmap, SCHED_BITS);
+	unsigned long idx = find_first_bit(rq->queue.bitmap, SCHED_QUEUE_BITS);
 	const struct list_head *head = &rq->queue.heads[sched_prio2idx(idx, rq)];
 
 	return list_first_entry(head, struct task_struct, sq_node);
@@ -166,7 +166,7 @@ sched_rq_next_task(struct task_struct *p, struct rq *rq)
 	struct list_head *head = &rq->queue.heads[idx];
 
 	if (list_is_last(&p->sq_node, head)) {
-		idx = find_next_bit(rq->queue.bitmap, SCHED_BITS,
+		idx = find_next_bit(rq->queue.bitmap, SCHED_QUEUE_BITS,
 				    sched_idx2prio(idx, rq) + 1);
 		head = &rq->queue.heads[sched_prio2idx(idx, rq)];
 
-- 
2.32.0.93.g670b81a890


From 0c494fafa968eca97daf6a70237d5eb1ff78cbd5 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Tue, 25 May 2021 14:08:11 +0000
Subject: [PATCH 14/30] sched/pds: Introduce SCHED_NORMAL_PRIO_NUM

---
 kernel/sched/alt_sched.h |  3 ++-
 kernel/sched/pds.h       | 30 ++++++++++++++++++------------
 2 files changed, 20 insertions(+), 13 deletions(-)

diff --git a/kernel/sched/alt_sched.h b/kernel/sched/alt_sched.h
index 76ec6f9c737b..1a579536fd30 100644
--- a/kernel/sched/alt_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -55,8 +55,9 @@
 #define SCHED_BITS	(MAX_RT_PRIO + NICE_WIDTH / 2 + MAX_PRIORITY_ADJ + 1)
 #endif
 #ifdef CONFIG_SCHED_PDS
+#define SCHED_NORMAL_PRIO_NUM	(NICE_WIDTH / 2)
 /* bits: RT(0-99), nice width / 2, cpu idle task */
-#define SCHED_BITS	(MAX_RT_PRIO + NICE_WIDTH / 2 + 1)
+#define SCHED_BITS	(MAX_RT_PRIO + SCHED_NORMAL_PRIO_NUM + 1)
 #endif
 
 #define IDLE_TASK_SCHED_PRIO	(SCHED_BITS - 1)
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index ee3d5cfac781..effd38a024d1 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -21,12 +21,12 @@ extern int alt_debug[20];
 static inline int
 task_sched_prio_normal(const struct task_struct *p, const struct rq *rq)
 {
-	int delta = (p->deadline >> 23) - rq->time_edge  - 1;
+	s64 delta = (p->deadline >> 23) - rq->time_edge  - 1;
 
-	if (unlikely(delta > 19)) {
-		pr_info("pds: task_sched_prio_normal delta %d, deadline %llu(%llu), time_edge %llu\n",
+	if (unlikely(delta > SCHED_NORMAL_PRIO_NUM - 1)) {
+		pr_info("pds: task_sched_prio_normal delta %lld, deadline %llu(%llu), time_edge %llu\n",
 			delta, p->deadline, p->deadline >> 23, rq->time_edge);
-		delta = 19;
+		delta = SCHED_NORMAL_PRIO_NUM - 1ULL;
 	}
 
 	return (delta < 0)? 0:delta;
@@ -43,19 +43,23 @@ static inline int
 task_sched_prio_idx(const struct task_struct *p, const struct rq *rq)
 {
 	return (p->prio < MAX_RT_PRIO) ? p->prio : MAX_RT_PRIO +
-		(task_sched_prio_normal(p, rq) + rq->time_edge) % 20;
+		(task_sched_prio_normal(p, rq) + rq->time_edge) %
+		SCHED_NORMAL_PRIO_NUM;
 }
 
 static inline unsigned long sched_prio2idx(unsigned long idx, struct rq *rq)
 {
 	return (IDLE_TASK_SCHED_PRIO == idx || idx < MAX_RT_PRIO) ? idx :
-		MAX_RT_PRIO + ((idx - MAX_RT_PRIO) + rq->time_edge) % 20;
+		MAX_RT_PRIO + ((idx - MAX_RT_PRIO) + rq->time_edge) %
+		SCHED_NORMAL_PRIO_NUM;
 }
 
 static inline unsigned long sched_idx2prio(unsigned long idx, struct rq *rq)
 {
 	return (idx < MAX_RT_PRIO) ? idx : MAX_RT_PRIO +
-		((idx - MAX_RT_PRIO) + 20 -  rq->time_edge % 20) % 20;
+		((idx - MAX_RT_PRIO) + SCHED_NORMAL_PRIO_NUM -
+		 rq->time_edge % SCHED_NORMAL_PRIO_NUM) %
+		SCHED_NORMAL_PRIO_NUM;
 }
 
 static inline void sched_renew_deadline(struct task_struct *p, const struct rq *rq)
@@ -103,25 +107,27 @@ static inline void update_rq_time_edge(struct rq *rq)
 	if (now == old)
 		return;
 
-	delta = min_t(u64, 20, now - old);
+	delta = min_t(u64, SCHED_NORMAL_PRIO_NUM, now - old);
 	INIT_LIST_HEAD(&head);
 
 	prio = MAX_RT_PRIO;
 	for_each_set_bit_from(prio, rq->queue.bitmap, MAX_RT_PRIO + delta) {
 		u64 idx;
 
-		idx = MAX_RT_PRIO + ((prio - MAX_RT_PRIO) + rq->time_edge) % 20;
+		idx = MAX_RT_PRIO + ((prio - MAX_RT_PRIO) + rq->time_edge) %
+			SCHED_NORMAL_PRIO_NUM;
 		list_splice_tail_init(rq->queue.heads + idx, &head);
 	}
 	sched_shift_normal_bitmap(rq->queue.bitmap, delta);
 	rq->time_edge = now;
 	if (!list_empty(&head)) {
 		struct task_struct *p;
+		u64 new_idx = MAX_RT_PRIO + now % SCHED_NORMAL_PRIO_NUM;
 
 		list_for_each_entry(p, &head, sq_node)
-			p->sq_idx = MAX_RT_PRIO + now % 20;
+			p->sq_idx = new_idx;
 
-		list_splice(&head, rq->queue.heads + MAX_RT_PRIO + now % 20);
+		list_splice(&head, rq->queue.heads + new_idx);
 		set_bit(MAX_RT_PRIO, rq->queue.bitmap);
 	}
 }
@@ -145,7 +151,7 @@ static inline void sched_task_sanity_check(struct task_struct *p, struct rq *rq)
 
 static inline void sched_imp_init(void)
 {
-	bitmap_set(normal_mask, MAX_RT_PRIO, 20);
+	bitmap_set(normal_mask, MAX_RT_PRIO, SCHED_NORMAL_PRIO_NUM);
 }
 
 /*
-- 
2.32.0.93.g670b81a890


From 8bb84fb5ebced13aa62b089a9187b758bdfb27b7 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Tue, 25 May 2021 15:00:58 +0000
Subject: [PATCH 15/30] sched/pds: SCHED_NORMAL_PRIO_NUM to 40

---
 kernel/sched/alt_sched.h | 2 +-
 kernel/sched/pds.h       | 8 ++++----
 2 files changed, 5 insertions(+), 5 deletions(-)

diff --git a/kernel/sched/alt_sched.h b/kernel/sched/alt_sched.h
index 1a579536fd30..eb5e8d31686c 100644
--- a/kernel/sched/alt_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -55,7 +55,7 @@
 #define SCHED_BITS	(MAX_RT_PRIO + NICE_WIDTH / 2 + MAX_PRIORITY_ADJ + 1)
 #endif
 #ifdef CONFIG_SCHED_PDS
-#define SCHED_NORMAL_PRIO_NUM	(NICE_WIDTH / 2)
+#define SCHED_NORMAL_PRIO_NUM	(NICE_WIDTH)
 /* bits: RT(0-99), nice width / 2, cpu idle task */
 #define SCHED_BITS	(MAX_RT_PRIO + SCHED_NORMAL_PRIO_NUM + 1)
 #endif
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index effd38a024d1..b1ea68e43ba7 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -12,7 +12,7 @@ static const u64 user_prio2deadline[NICE_WIDTH] = {
 };
 
 #define SCHED_PRIO_SLOT		(4ULL << 20)
-#define DEFAULT_SCHED_PRIO (MAX_RT_PRIO + 10)
+#define DEFAULT_SCHED_PRIO (MAX_RT_PRIO + SCHED_NORMAL_PRIO_NUM / 2)
 
 DECLARE_BITMAP(normal_mask, SCHED_BITS);
 
@@ -21,11 +21,11 @@ extern int alt_debug[20];
 static inline int
 task_sched_prio_normal(const struct task_struct *p, const struct rq *rq)
 {
-	s64 delta = (p->deadline >> 23) - rq->time_edge  - 1;
+	s64 delta = (p->deadline >> 22) - rq->time_edge  - 1;
 
 	if (unlikely(delta > SCHED_NORMAL_PRIO_NUM - 1)) {
 		pr_info("pds: task_sched_prio_normal delta %lld, deadline %llu(%llu), time_edge %llu\n",
-			delta, p->deadline, p->deadline >> 23, rq->time_edge);
+			delta, p->deadline, p->deadline >> 22, rq->time_edge);
 		delta = SCHED_NORMAL_PRIO_NUM - 1ULL;
 	}
 
@@ -101,7 +101,7 @@ static inline void update_rq_time_edge(struct rq *rq)
 {
 	struct list_head head;
 	u64 old = rq->time_edge;
-	u64 now = rq->clock >> 23;
+	u64 now = rq->clock >> 22;
 	u64 prio, delta;
 
 	if (now == old)
-- 
2.32.0.93.g670b81a890


From 34e4ebdbaba5e1f30349524a9f1438b96f0ff305 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Thu, 27 May 2021 14:34:44 +0000
Subject: [PATCH 16/30] sched/pds: Change MIN_NORMAL_PRIO to 128

---
 include/linux/sched/prio.h | 17 +++++++++++++++++
 kernel/sched/alt_sched.h   |  7 ++++---
 kernel/sched/pds.h         | 31 +++++++++++++++++--------------
 3 files changed, 38 insertions(+), 17 deletions(-)

diff --git a/include/linux/sched/prio.h b/include/linux/sched/prio.h
index 4d4f92bffeea..a191f253771b 100644
--- a/include/linux/sched/prio.h
+++ b/include/linux/sched/prio.h
@@ -18,14 +18,31 @@
 #define MAX_PRIO		(MAX_RT_PRIO + NICE_WIDTH)
 #define DEFAULT_PRIO		(MAX_RT_PRIO + NICE_WIDTH / 2)
 
+#ifdef CONFIG_SCHED_ALT
+
+/* Undefine MAX_PRIO and DEFAULT_PRIO */
+#undef MAX_PRIO
+#undef DEFAULT_PRIO
+
 /* +/- priority levels from the base priority */
 #ifdef CONFIG_SCHED_BMQ
 #define MAX_PRIORITY_ADJ	7
+
+#define MIN_NORMAL_PRIO		(MAX_RT_PRIO)
+#define MAX_PRIO		(MIN_NORMAL_PRIO + NICE_WIDTH)
+#define DEFAULT_PRIO		(MIN_NORMAL_PRIO + NICE_WIDTH / 2)
 #endif
+
 #ifdef CONFIG_SCHED_PDS
 #define MAX_PRIORITY_ADJ	0
+
+#define MIN_NORMAL_PRIO		(128)
+#define MAX_PRIO		(MIN_NORMAL_PRIO + NICE_WIDTH)
+#define DEFAULT_PRIO		(MIN_NORMAL_PRIO + NICE_WIDTH / 2)
 #endif
 
+#endif /* CONFIG_SCHED_ALT */
+
 /*
  * Convert user-nice values [ -20 ... 0 ... 19 ]
  * to static priority [ MAX_RT_PRIO..MAX_PRIO-1 ],
diff --git a/kernel/sched/alt_sched.h b/kernel/sched/alt_sched.h
index eb5e8d31686c..52e1baa4f5da 100644
--- a/kernel/sched/alt_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -54,11 +54,12 @@
  * RT(0-99), (Low prio adj range, nice width, high prio adj range) / 2, cpu idle task */
 #define SCHED_BITS	(MAX_RT_PRIO + NICE_WIDTH / 2 + MAX_PRIORITY_ADJ + 1)
 #endif
+
 #ifdef CONFIG_SCHED_PDS
 #define SCHED_NORMAL_PRIO_NUM	(NICE_WIDTH)
-/* bits: RT(0-99), nice width / 2, cpu idle task */
-#define SCHED_BITS	(MAX_RT_PRIO + SCHED_NORMAL_PRIO_NUM + 1)
-#endif
+/* bits: RT(0-99), reserved(100-127), SCHED_NORMAL_PRIO_NUM, cpu idle task */
+#define SCHED_BITS	(MIN_NORMAL_PRIO + SCHED_NORMAL_PRIO_NUM + 1)
+#endif /* CONFIG_SCHED_PDS */
 
 #define IDLE_TASK_SCHED_PRIO	(SCHED_BITS - 1)
 
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index b1ea68e43ba7..4a181e6ed52f 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -12,7 +12,7 @@ static const u64 user_prio2deadline[NICE_WIDTH] = {
 };
 
 #define SCHED_PRIO_SLOT		(4ULL << 20)
-#define DEFAULT_SCHED_PRIO (MAX_RT_PRIO + SCHED_NORMAL_PRIO_NUM / 2)
+#define DEFAULT_SCHED_PRIO (MIN_NORMAL_PRIO + SCHED_NORMAL_PRIO_NUM / 2)
 
 DECLARE_BITMAP(normal_mask, SCHED_BITS);
 
@@ -36,13 +36,13 @@ static inline int
 task_sched_prio(const struct task_struct *p)
 {
 	return (p->prio < MAX_RT_PRIO) ? p->prio :
-		MAX_RT_PRIO + task_sched_prio_normal(p, task_rq(p));
+		MIN_NORMAL_PRIO + task_sched_prio_normal(p, task_rq(p));
 }
 
 static inline int
 task_sched_prio_idx(const struct task_struct *p, const struct rq *rq)
 {
-	return (p->prio < MAX_RT_PRIO) ? p->prio : MAX_RT_PRIO +
+	return (p->prio < MAX_RT_PRIO) ? p->prio : MIN_NORMAL_PRIO +
 		(task_sched_prio_normal(p, rq) + rq->time_edge) %
 		SCHED_NORMAL_PRIO_NUM;
 }
@@ -50,14 +50,15 @@ task_sched_prio_idx(const struct task_struct *p, const struct rq *rq)
 static inline unsigned long sched_prio2idx(unsigned long idx, struct rq *rq)
 {
 	return (IDLE_TASK_SCHED_PRIO == idx || idx < MAX_RT_PRIO) ? idx :
-		MAX_RT_PRIO + ((idx - MAX_RT_PRIO) + rq->time_edge) %
+		MIN_NORMAL_PRIO +
+		((idx - MIN_NORMAL_PRIO) + rq->time_edge) %
 		SCHED_NORMAL_PRIO_NUM;
 }
 
 static inline unsigned long sched_idx2prio(unsigned long idx, struct rq *rq)
 {
-	return (idx < MAX_RT_PRIO) ? idx : MAX_RT_PRIO +
-		((idx - MAX_RT_PRIO) + SCHED_NORMAL_PRIO_NUM -
+	return (idx < MAX_RT_PRIO) ? idx : MIN_NORMAL_PRIO +
+		((idx - MIN_NORMAL_PRIO) + SCHED_NORMAL_PRIO_NUM -
 		 rq->time_edge % SCHED_NORMAL_PRIO_NUM) %
 		SCHED_NORMAL_PRIO_NUM;
 }
@@ -66,7 +67,8 @@ static inline void sched_renew_deadline(struct task_struct *p, const struct rq *
 {
 	if (p->prio >= MAX_RT_PRIO)
 		p->deadline = rq->clock +
-			SCHED_PRIO_SLOT * (p->static_prio - MAX_RT_PRIO + 1);
+			SCHED_PRIO_SLOT *
+			(p->static_prio - MIN_NORMAL_PRIO + 1);
 }
 
 /*
@@ -110,11 +112,12 @@ static inline void update_rq_time_edge(struct rq *rq)
 	delta = min_t(u64, SCHED_NORMAL_PRIO_NUM, now - old);
 	INIT_LIST_HEAD(&head);
 
-	prio = MAX_RT_PRIO;
-	for_each_set_bit_from(prio, rq->queue.bitmap, MAX_RT_PRIO + delta) {
+	prio = MIN_NORMAL_PRIO;
+	for_each_set_bit_from(prio, rq->queue.bitmap, MIN_NORMAL_PRIO + delta) {
 		u64 idx;
 
-		idx = MAX_RT_PRIO + ((prio - MAX_RT_PRIO) + rq->time_edge) %
+		idx = MIN_NORMAL_PRIO +
+			((prio - MIN_NORMAL_PRIO) + rq->time_edge) %
 			SCHED_NORMAL_PRIO_NUM;
 		list_splice_tail_init(rq->queue.heads + idx, &head);
 	}
@@ -122,13 +125,13 @@ static inline void update_rq_time_edge(struct rq *rq)
 	rq->time_edge = now;
 	if (!list_empty(&head)) {
 		struct task_struct *p;
-		u64 new_idx = MAX_RT_PRIO + now % SCHED_NORMAL_PRIO_NUM;
+		u64 new_idx = MIN_NORMAL_PRIO + now % SCHED_NORMAL_PRIO_NUM;
 
 		list_for_each_entry(p, &head, sq_node)
 			p->sq_idx = new_idx;
 
 		list_splice(&head, rq->queue.heads + new_idx);
-		set_bit(MAX_RT_PRIO, rq->queue.bitmap);
+		set_bit(MIN_NORMAL_PRIO, rq->queue.bitmap);
 	}
 }
 
@@ -151,7 +154,7 @@ static inline void sched_task_sanity_check(struct task_struct *p, struct rq *rq)
 
 static inline void sched_imp_init(void)
 {
-	bitmap_set(normal_mask, MAX_RT_PRIO, SCHED_NORMAL_PRIO_NUM);
+	bitmap_set(normal_mask, MIN_NORMAL_PRIO, SCHED_NORMAL_PRIO_NUM);
 }
 
 /*
@@ -248,7 +251,7 @@ int task_prio(const struct task_struct *p)
 	if (p->prio < MAX_RT_PRIO)
 		return (p->prio - MAX_RT_PRIO);
 
-	ret = task_sched_prio(p) - MAX_RT_PRIO;
+	ret = task_sched_prio(p) - MIN_NORMAL_PRIO;
 
 	return ret;
 }
-- 
2.32.0.93.g670b81a890


From 830f7b82d9dd12cd4acfd2aa6020965168e2f78d Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Thu, 27 May 2021 15:19:51 +0000
Subject: [PATCH 17/30] sched/pds: Optimization for MIN_NORMAL_PRIO=128

---
 kernel/sched/alt_core.c |  1 -
 kernel/sched/bmq.h      |  2 --
 kernel/sched/pds.h      | 34 ++++++----------------------------
 3 files changed, 6 insertions(+), 31 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 8fd6fd9ec2ea..56c527cbcff5 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -6689,7 +6689,6 @@ void __init sched_init(void)
 	struct rq *rq;
 
 	printk(KERN_INFO ALT_SCHED_VERSION_MSG);
-	sched_imp_init();
 
 	wait_bit_init();
 
diff --git a/kernel/sched/bmq.h b/kernel/sched/bmq.h
index 85e4c477eda8..ed6995865d81 100644
--- a/kernel/sched/bmq.h
+++ b/kernel/sched/bmq.h
@@ -64,8 +64,6 @@ static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
 
 static inline void sched_task_sanity_check(struct task_struct *p, struct rq *rq) {}
 
-static inline void sched_imp_init(void) {}
-
 inline int task_running_nice(struct task_struct *p)
 {
 	return (p->prio + p->boost_prio > DEFAULT_PRIO + MAX_PRIORITY_ADJ);
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index 4a181e6ed52f..79121046e892 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -14,8 +14,6 @@ static const u64 user_prio2deadline[NICE_WIDTH] = {
 #define SCHED_PRIO_SLOT		(4ULL << 20)
 #define DEFAULT_SCHED_PRIO (MIN_NORMAL_PRIO + SCHED_NORMAL_PRIO_NUM / 2)
 
-DECLARE_BITMAP(normal_mask, SCHED_BITS);
-
 extern int alt_debug[20];
 
 static inline int
@@ -66,8 +64,7 @@ static inline unsigned long sched_idx2prio(unsigned long idx, struct rq *rq)
 static inline void sched_renew_deadline(struct task_struct *p, const struct rq *rq)
 {
 	if (p->prio >= MAX_RT_PRIO)
-		p->deadline = rq->clock +
-			SCHED_PRIO_SLOT *
+		p->deadline = rq->clock + SCHED_PRIO_SLOT *
 			(p->static_prio - MIN_NORMAL_PRIO + 1);
 }
 
@@ -87,18 +84,6 @@ int task_running_nice(struct task_struct *p)
 	return task_sched_prio(p) > DEFAULT_SCHED_PRIO;
 }
 
-static inline void sched_shift_normal_bitmap(unsigned long *mask, unsigned int shift)
-{
-	DECLARE_BITMAP(normal, SCHED_BITS);
-
-	bitmap_and(normal, mask, normal_mask, SCHED_BITS);
-	bitmap_shift_right(normal, normal, shift, SCHED_BITS);
-	bitmap_and(normal, normal, normal_mask, SCHED_BITS);
-
-	bitmap_andnot(mask, mask, normal_mask, SCHED_BITS);
-	bitmap_or(mask, mask, normal, SCHED_BITS);
-}
-
 static inline void update_rq_time_edge(struct rq *rq)
 {
 	struct list_head head;
@@ -112,26 +97,24 @@ static inline void update_rq_time_edge(struct rq *rq)
 	delta = min_t(u64, SCHED_NORMAL_PRIO_NUM, now - old);
 	INIT_LIST_HEAD(&head);
 
-	prio = MIN_NORMAL_PRIO;
-	for_each_set_bit_from(prio, rq->queue.bitmap, MIN_NORMAL_PRIO + delta) {
+	for_each_set_bit(prio, &rq->queue.bitmap[2], delta) {
 		u64 idx;
 
 		idx = MIN_NORMAL_PRIO +
-			((prio - MIN_NORMAL_PRIO) + rq->time_edge) %
-			SCHED_NORMAL_PRIO_NUM;
+			(prio + rq->time_edge) % SCHED_NORMAL_PRIO_NUM;
 		list_splice_tail_init(rq->queue.heads + idx, &head);
 	}
-	sched_shift_normal_bitmap(rq->queue.bitmap, delta);
+	rq->queue.bitmap[2] >>= delta;
 	rq->time_edge = now;
 	if (!list_empty(&head)) {
-		struct task_struct *p;
 		u64 new_idx = MIN_NORMAL_PRIO + now % SCHED_NORMAL_PRIO_NUM;
+		struct task_struct *p;
 
 		list_for_each_entry(p, &head, sq_node)
 			p->sq_idx = new_idx;
 
 		list_splice(&head, rq->queue.heads + new_idx);
-		set_bit(MIN_NORMAL_PRIO, rq->queue.bitmap);
+		rq->queue.bitmap[2] |= 1UL;
 	}
 }
 
@@ -152,11 +135,6 @@ static inline void sched_task_sanity_check(struct task_struct *p, struct rq *rq)
 		p->deadline = rq->clock + 40 * SCHED_PRIO_SLOT;
 }
 
-static inline void sched_imp_init(void)
-{
-	bitmap_set(normal_mask, MIN_NORMAL_PRIO, SCHED_NORMAL_PRIO_NUM);
-}
-
 /*
  * This routine assume that the idle task always in queue
  */
-- 
2.32.0.93.g670b81a890


From 636a21edeef2d5e9abce08fed137b68718e6d3c8 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Fri, 28 May 2021 10:13:57 +0000
Subject: [PATCH 18/30] sched/alt: Machine friendly time slice value

---
 kernel/sched/alt_core.c | 6 +++---
 1 file changed, 3 insertions(+), 3 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 56c527cbcff5..b553f5fa60dd 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -77,7 +77,7 @@ __read_mostly int sysctl_resched_latency_warn_once = 1;
 #define STOP_PRIO		(MAX_RT_PRIO - 1)
 
 /* Default time slice is 4 in ms, can be set via kernel parameter "sched_timeslice" */
-u64 sched_timeslice_ns __read_mostly = (4 * 1000 * 1000);
+u64 sched_timeslice_ns __read_mostly = (4 << 20);
 
 static int __init sched_timeslice(char *str)
 {
@@ -85,14 +85,14 @@ static int __init sched_timeslice(char *str)
 
 	get_option(&str, &timeslice_us);
 	if (timeslice_us >= 1000)
-		sched_timeslice_ns = timeslice_us * 1000;
+		sched_timeslice_ns = (timeslice_us / 1000) << 20;
 
 	return 0;
 }
 early_param("sched_timeslice", sched_timeslice);
 
 /* Reschedule if less than this many s left */
-#define RESCHED_NS		(100 * 1000)
+#define RESCHED_NS		(100 << 10)
 
 /**
  * sched_yield_type - Choose what sort of yield sched_yield will perform.
-- 
2.32.0.93.g670b81a890


From 249ce7672cbc819132a30b235a1cb228ae630c52 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Fri, 28 May 2021 14:47:49 +0000
Subject: [PATCH 19/30] sched/pds: Default 2ms time slice

---
 kernel/sched/alt_core.c |  3 ++-
 kernel/sched/bmq.h      |  2 ++
 kernel/sched/pds.h      | 51 ++++++++++++++++++-----------------------
 3 files changed, 26 insertions(+), 30 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index b553f5fa60dd..db8f5b24089d 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -77,7 +77,7 @@ __read_mostly int sysctl_resched_latency_warn_once = 1;
 #define STOP_PRIO		(MAX_RT_PRIO - 1)
 
 /* Default time slice is 4 in ms, can be set via kernel parameter "sched_timeslice" */
-u64 sched_timeslice_ns __read_mostly = (4 << 20);
+u64 sched_timeslice_ns __read_mostly = (2 << 20);
 
 static int __init sched_timeslice(char *str)
 {
@@ -6689,6 +6689,7 @@ void __init sched_init(void)
 	struct rq *rq;
 
 	printk(KERN_INFO ALT_SCHED_VERSION_MSG);
+	sched_imp_init();
 
 	wait_bit_init();
 
diff --git a/kernel/sched/bmq.h b/kernel/sched/bmq.h
index ed6995865d81..7299b5cc9a87 100644
--- a/kernel/sched/bmq.h
+++ b/kernel/sched/bmq.h
@@ -36,6 +36,8 @@ static inline void deboost_task(struct task_struct *p)
 /*
  * Common interfaces
  */
+static inline void sched_imp_init(void) {}
+
 static inline int normal_prio(struct task_struct *p)
 {
 	if (task_has_rt_policy(p))
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index 79121046e892..6bba054465d3 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -1,17 +1,7 @@
 #define ALT_SCHED_VERSION_MSG "sched/pds: PDS CPU Scheduler "ALT_SCHED_VERSION" by Alfred Chen.\n"
 
-static const u64 user_prio2deadline[NICE_WIDTH] = {
-/* -20 */	  4194304,   4613734,   5075107,   5582617,   6140878,
-/* -15 */	  6754965,   7430461,   8173507,   8990857,   9889942,
-/* -10 */	 10878936,  11966829,  13163511,  14479862,  15927848,
-/*  -5 */	 17520632,  19272695,  21199964,  23319960,  25651956,
-/*   0 */	 28217151,  31038866,  34142752,  37557027,  41312729,
-/*   5 */	 45444001,  49988401,  54987241,  60485965,  66534561,
-/*  10 */	 73188017,  80506818,  88557499,  97413248, 107154572,
-/*  15 */	117870029, 129657031, 142622734, 156885007, 172573507
-};
-
-#define SCHED_PRIO_SLOT		(4ULL << 20)
+static u64 user_prio2deadline[NICE_WIDTH];
+
 #define DEFAULT_SCHED_PRIO (MIN_NORMAL_PRIO + SCHED_NORMAL_PRIO_NUM / 2)
 
 extern int alt_debug[20];
@@ -19,11 +9,11 @@ extern int alt_debug[20];
 static inline int
 task_sched_prio_normal(const struct task_struct *p, const struct rq *rq)
 {
-	s64 delta = (p->deadline >> 22) - rq->time_edge  - 1;
+	s64 delta = (p->deadline >> 21) - rq->time_edge  - 1;
 
 	if (unlikely(delta > SCHED_NORMAL_PRIO_NUM - 1)) {
 		pr_info("pds: task_sched_prio_normal delta %lld, deadline %llu(%llu), time_edge %llu\n",
-			delta, p->deadline, p->deadline >> 22, rq->time_edge);
+			delta, p->deadline, p->deadline >> 21, rq->time_edge);
 		delta = SCHED_NORMAL_PRIO_NUM - 1ULL;
 	}
 
@@ -48,8 +38,7 @@ task_sched_prio_idx(const struct task_struct *p, const struct rq *rq)
 static inline unsigned long sched_prio2idx(unsigned long idx, struct rq *rq)
 {
 	return (IDLE_TASK_SCHED_PRIO == idx || idx < MAX_RT_PRIO) ? idx :
-		MIN_NORMAL_PRIO +
-		((idx - MIN_NORMAL_PRIO) + rq->time_edge) %
+		MIN_NORMAL_PRIO + ((idx - MIN_NORMAL_PRIO) + rq->time_edge) %
 		SCHED_NORMAL_PRIO_NUM;
 }
 
@@ -64,13 +53,23 @@ static inline unsigned long sched_idx2prio(unsigned long idx, struct rq *rq)
 static inline void sched_renew_deadline(struct task_struct *p, const struct rq *rq)
 {
 	if (p->prio >= MAX_RT_PRIO)
-		p->deadline = rq->clock + SCHED_PRIO_SLOT *
-			(p->static_prio - MIN_NORMAL_PRIO + 1);
+		p->deadline = rq->clock +
+			user_prio2deadline[p->static_prio - MIN_NORMAL_PRIO];
 }
 
 /*
  * Common interfaces
  */
+static inline void sched_imp_init(void)
+{
+	int i;
+
+	user_prio2deadline[0] = sched_timeslice_ns;
+	for (i = 1; i < NICE_WIDTH; i++)
+		user_prio2deadline[i] =
+			user_prio2deadline[i - 1] + sched_timeslice_ns;
+}
+
 static inline int normal_prio(struct task_struct *p)
 {
 	if (task_has_rt_policy(p))
@@ -88,7 +87,7 @@ static inline void update_rq_time_edge(struct rq *rq)
 {
 	struct list_head head;
 	u64 old = rq->time_edge;
-	u64 now = rq->clock >> 22;
+	u64 now = rq->clock >> 21;
 	u64 prio, delta;
 
 	if (now == old)
@@ -131,8 +130,8 @@ static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
 
 static inline void sched_task_sanity_check(struct task_struct *p, struct rq *rq)
 {
-	if (unlikely(p->deadline > rq->clock + 40 * SCHED_PRIO_SLOT))
-		p->deadline = rq->clock + 40 * SCHED_PRIO_SLOT;
+	if (unlikely(p->deadline > rq->clock + user_prio2deadline[NICE_WIDTH - 1]))
+		p->deadline = rq->clock + user_prio2deadline[NICE_WIDTH - 1];
 }
 
 /*
@@ -224,14 +223,8 @@ static void sched_task_fork(struct task_struct *p, struct rq *rq)
  */
 int task_prio(const struct task_struct *p)
 {
-	int ret;
-
-	if (p->prio < MAX_RT_PRIO)
-		return (p->prio - MAX_RT_PRIO);
-
-	ret = task_sched_prio(p) - MIN_NORMAL_PRIO;
-
-	return ret;
+	return (p->prio < MAX_RT_PRIO) ? p->prio - MAX_RT_PRIO :
+		task_sched_prio(p) - MIN_NORMAL_PRIO;
 }
 
 static void do_sched_yield_type_1(struct task_struct *p, struct rq *rq)
-- 
2.32.0.93.g670b81a890


From d902192677fa2093e232f44cd1751d2de96a0667 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Tue, 1 Jun 2021 11:29:16 +0000
Subject: [PATCH 20/30] sched/pds: Code clean up

---
 kernel/sched/pds.h | 14 +++++---------
 1 file changed, 5 insertions(+), 9 deletions(-)

diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index 6bba054465d3..d7f772401b3e 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -2,8 +2,6 @@
 
 static u64 user_prio2deadline[NICE_WIDTH];
 
-#define DEFAULT_SCHED_PRIO (MIN_NORMAL_PRIO + SCHED_NORMAL_PRIO_NUM / 2)
-
 extern int alt_debug[20];
 
 static inline int
@@ -14,10 +12,10 @@ task_sched_prio_normal(const struct task_struct *p, const struct rq *rq)
 	if (unlikely(delta > SCHED_NORMAL_PRIO_NUM - 1)) {
 		pr_info("pds: task_sched_prio_normal delta %lld, deadline %llu(%llu), time_edge %llu\n",
 			delta, p->deadline, p->deadline >> 21, rq->time_edge);
-		delta = SCHED_NORMAL_PRIO_NUM - 1ULL;
+		return SCHED_NORMAL_PRIO_NUM - 1ULL;
 	}
 
-	return (delta < 0)? 0:delta;
+	return (delta < 0) ? 0 : delta;
 }
 
 static inline int
@@ -72,15 +70,13 @@ static inline void sched_imp_init(void)
 
 static inline int normal_prio(struct task_struct *p)
 {
-	if (task_has_rt_policy(p))
-		return MAX_RT_PRIO - 1 - p->rt_priority;
-
-	return MAX_RT_PRIO;
+	return task_has_rt_policy(p) ? (MAX_RT_PRIO - 1 - p->rt_priority) :
+		MAX_RT_PRIO;
 }
 
 int task_running_nice(struct task_struct *p)
 {
-	return task_sched_prio(p) > DEFAULT_SCHED_PRIO;
+	return task_sched_prio(p) > DEFAULT_PRIO;
 }
 
 static inline void update_rq_time_edge(struct rq *rq)
-- 
2.32.0.93.g670b81a890


From 7d456d6d1618b2563c6e3edd975ce2db1d56fa66 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Wed, 2 Jun 2021 10:25:26 +0000
Subject: [PATCH 21/30] sched/pds: SCHED_NORMAL_PRIO_NUM to 64

---
 include/linux/sched/prio.h |  9 +++++----
 kernel/sched/alt_sched.h   |  2 +-
 kernel/sched/pds.h         | 12 ++++++------
 3 files changed, 12 insertions(+), 11 deletions(-)

diff --git a/include/linux/sched/prio.h b/include/linux/sched/prio.h
index a191f253771b..6af9ae681116 100644
--- a/include/linux/sched/prio.h
+++ b/include/linux/sched/prio.h
@@ -26,7 +26,7 @@
 
 /* +/- priority levels from the base priority */
 #ifdef CONFIG_SCHED_BMQ
-#define MAX_PRIORITY_ADJ	7
+#define MAX_PRIORITY_ADJ	(7)
 
 #define MIN_NORMAL_PRIO		(MAX_RT_PRIO)
 #define MAX_PRIO		(MIN_NORMAL_PRIO + NICE_WIDTH)
@@ -34,11 +34,12 @@
 #endif
 
 #ifdef CONFIG_SCHED_PDS
-#define MAX_PRIORITY_ADJ	0
+#define MAX_PRIORITY_ADJ	(0)
 
 #define MIN_NORMAL_PRIO		(128)
-#define MAX_PRIO		(MIN_NORMAL_PRIO + NICE_WIDTH)
-#define DEFAULT_PRIO		(MIN_NORMAL_PRIO + NICE_WIDTH / 2)
+#define NORMAL_PRIO_NUM		(64)
+#define MAX_PRIO		(MIN_NORMAL_PRIO + NORMAL_PRIO_NUM)
+#define DEFAULT_PRIO		(MAX_PRIO - NICE_WIDTH / 2)
 #endif
 
 #endif /* CONFIG_SCHED_ALT */
diff --git a/kernel/sched/alt_sched.h b/kernel/sched/alt_sched.h
index 52e1baa4f5da..db89d3d3be63 100644
--- a/kernel/sched/alt_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -56,7 +56,7 @@
 #endif
 
 #ifdef CONFIG_SCHED_PDS
-#define SCHED_NORMAL_PRIO_NUM	(NICE_WIDTH)
+#define SCHED_NORMAL_PRIO_NUM	(NORMAL_PRIO_NUM)
 /* bits: RT(0-99), reserved(100-127), SCHED_NORMAL_PRIO_NUM, cpu idle task */
 #define SCHED_BITS	(MIN_NORMAL_PRIO + SCHED_NORMAL_PRIO_NUM + 1)
 #endif /* CONFIG_SCHED_PDS */
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index d7f772401b3e..5abc6a9d0b9b 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -7,7 +7,8 @@ extern int alt_debug[20];
 static inline int
 task_sched_prio_normal(const struct task_struct *p, const struct rq *rq)
 {
-	s64 delta = (p->deadline >> 21) - rq->time_edge  - 1;
+	s64 delta = (p->deadline >> 21) - rq->time_edge +
+		SCHED_NORMAL_PRIO_NUM - NICE_WIDTH - 1;
 
 	if (unlikely(delta > SCHED_NORMAL_PRIO_NUM - 1)) {
 		pr_info("pds: task_sched_prio_normal delta %lld, deadline %llu(%llu), time_edge %llu\n",
@@ -51,8 +52,8 @@ static inline unsigned long sched_idx2prio(unsigned long idx, struct rq *rq)
 static inline void sched_renew_deadline(struct task_struct *p, const struct rq *rq)
 {
 	if (p->prio >= MAX_RT_PRIO)
-		p->deadline = rq->clock +
-			user_prio2deadline[p->static_prio - MIN_NORMAL_PRIO];
+		p->deadline = rq->clock + user_prio2deadline[p->static_prio -
+			(MAX_PRIO - NICE_WIDTH)];
 }
 
 /*
@@ -95,8 +96,7 @@ static inline void update_rq_time_edge(struct rq *rq)
 	for_each_set_bit(prio, &rq->queue.bitmap[2], delta) {
 		u64 idx;
 
-		idx = MIN_NORMAL_PRIO +
-			(prio + rq->time_edge) % SCHED_NORMAL_PRIO_NUM;
+		idx = MIN_NORMAL_PRIO + (prio + old) % SCHED_NORMAL_PRIO_NUM;
 		list_splice_tail_init(rq->queue.heads + idx, &head);
 	}
 	rq->queue.bitmap[2] >>= delta;
@@ -220,7 +220,7 @@ static void sched_task_fork(struct task_struct *p, struct rq *rq)
 int task_prio(const struct task_struct *p)
 {
 	return (p->prio < MAX_RT_PRIO) ? p->prio - MAX_RT_PRIO :
-		task_sched_prio(p) - MIN_NORMAL_PRIO;
+		task_sched_prio_normal(p, task_rq(p));
 }
 
 static void do_sched_yield_type_1(struct task_struct *p, struct rq *rq)
-- 
2.32.0.93.g670b81a890


From 911fa09b36b8367a622d6614ad209f43458545e5 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Thu, 3 Jun 2021 09:31:01 +0000
Subject: [PATCH 22/30] sched/pds: Optimize MOD operation when
 NORMAL_PRIO_NUM==64

---
 kernel/sched/alt_sched.h |  5 ++---
 kernel/sched/pds.h       | 33 +++++++++++++++++----------------
 2 files changed, 19 insertions(+), 19 deletions(-)

diff --git a/kernel/sched/alt_sched.h b/kernel/sched/alt_sched.h
index db89d3d3be63..f9f79422bf0e 100644
--- a/kernel/sched/alt_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -56,9 +56,8 @@
 #endif
 
 #ifdef CONFIG_SCHED_PDS
-#define SCHED_NORMAL_PRIO_NUM	(NORMAL_PRIO_NUM)
-/* bits: RT(0-99), reserved(100-127), SCHED_NORMAL_PRIO_NUM, cpu idle task */
-#define SCHED_BITS	(MIN_NORMAL_PRIO + SCHED_NORMAL_PRIO_NUM + 1)
+/* bits: RT(0-99), reserved(100-127), NORMAL_PRIO_NUM, cpu idle task */
+#define SCHED_BITS	(MIN_NORMAL_PRIO + NORMAL_PRIO_NUM + 1)
 #endif /* CONFIG_SCHED_PDS */
 
 #define IDLE_TASK_SCHED_PRIO	(SCHED_BITS - 1)
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index 5abc6a9d0b9b..41e9873d8cd7 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -4,16 +4,18 @@ static u64 user_prio2deadline[NICE_WIDTH];
 
 extern int alt_debug[20];
 
+#define NORMAL_PRIO_MOD(x)	((x) & (NORMAL_PRIO_NUM - 1))
+
 static inline int
 task_sched_prio_normal(const struct task_struct *p, const struct rq *rq)
 {
 	s64 delta = (p->deadline >> 21) - rq->time_edge +
-		SCHED_NORMAL_PRIO_NUM - NICE_WIDTH - 1;
+		NORMAL_PRIO_NUM - NICE_WIDTH - 1;
 
-	if (unlikely(delta > SCHED_NORMAL_PRIO_NUM - 1)) {
+	if (unlikely(delta > NORMAL_PRIO_NUM - 1)) {
 		pr_info("pds: task_sched_prio_normal delta %lld, deadline %llu(%llu), time_edge %llu\n",
 			delta, p->deadline, p->deadline >> 21, rq->time_edge);
-		return SCHED_NORMAL_PRIO_NUM - 1ULL;
+		return NORMAL_PRIO_NUM - 1;
 	}
 
 	return (delta < 0) ? 0 : delta;
@@ -30,23 +32,21 @@ static inline int
 task_sched_prio_idx(const struct task_struct *p, const struct rq *rq)
 {
 	return (p->prio < MAX_RT_PRIO) ? p->prio : MIN_NORMAL_PRIO +
-		(task_sched_prio_normal(p, rq) + rq->time_edge) %
-		SCHED_NORMAL_PRIO_NUM;
+		NORMAL_PRIO_MOD(task_sched_prio_normal(p, rq) + rq->time_edge);
 }
 
-static inline unsigned long sched_prio2idx(unsigned long idx, struct rq *rq)
+static inline unsigned long sched_prio2idx(unsigned long prio, struct rq *rq)
 {
-	return (IDLE_TASK_SCHED_PRIO == idx || idx < MAX_RT_PRIO) ? idx :
-		MIN_NORMAL_PRIO + ((idx - MIN_NORMAL_PRIO) + rq->time_edge) %
-		SCHED_NORMAL_PRIO_NUM;
+	return (IDLE_TASK_SCHED_PRIO == prio || prio < MAX_RT_PRIO) ? prio :
+		MIN_NORMAL_PRIO + NORMAL_PRIO_MOD((prio - MIN_NORMAL_PRIO) +
+						  rq->time_edge);
 }
 
 static inline unsigned long sched_idx2prio(unsigned long idx, struct rq *rq)
 {
 	return (idx < MAX_RT_PRIO) ? idx : MIN_NORMAL_PRIO +
-		((idx - MIN_NORMAL_PRIO) + SCHED_NORMAL_PRIO_NUM -
-		 rq->time_edge % SCHED_NORMAL_PRIO_NUM) %
-		SCHED_NORMAL_PRIO_NUM;
+		NORMAL_PRIO_MOD((idx - MIN_NORMAL_PRIO) + NORMAL_PRIO_NUM -
+				NORMAL_PRIO_MOD(rq->time_edge));
 }
 
 static inline void sched_renew_deadline(struct task_struct *p, const struct rq *rq)
@@ -90,19 +90,20 @@ static inline void update_rq_time_edge(struct rq *rq)
 	if (now == old)
 		return;
 
-	delta = min_t(u64, SCHED_NORMAL_PRIO_NUM, now - old);
+	delta = min_t(u64, NORMAL_PRIO_NUM, now - old);
 	INIT_LIST_HEAD(&head);
 
 	for_each_set_bit(prio, &rq->queue.bitmap[2], delta) {
 		u64 idx;
 
-		idx = MIN_NORMAL_PRIO + (prio + old) % SCHED_NORMAL_PRIO_NUM;
+		idx = MIN_NORMAL_PRIO + NORMAL_PRIO_MOD(prio + old);
 		list_splice_tail_init(rq->queue.heads + idx, &head);
 	}
-	rq->queue.bitmap[2] >>= delta;
+	rq->queue.bitmap[2] = (NORMAL_PRIO_NUM == delta) ? 0UL :
+		rq->queue.bitmap[2] >> delta;
 	rq->time_edge = now;
 	if (!list_empty(&head)) {
-		u64 new_idx = MIN_NORMAL_PRIO + now % SCHED_NORMAL_PRIO_NUM;
+		u64 new_idx = MIN_NORMAL_PRIO + NORMAL_PRIO_MOD(now);
 		struct task_struct *p;
 
 		list_for_each_entry(p, &head, sq_node)
-- 
2.32.0.93.g670b81a890


From b80fa0c382d31aa8fff73ef6e44c57dee58b319c Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Sat, 5 Jun 2021 21:28:50 +0000
Subject: [PATCH 23/30] sched/pds: Code clean up

---
 kernel/sched/pds.h | 18 +++++++-----------
 1 file changed, 7 insertions(+), 11 deletions(-)

diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index 41e9873d8cd7..5ce0a16eb454 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -21,8 +21,7 @@ task_sched_prio_normal(const struct task_struct *p, const struct rq *rq)
 	return (delta < 0) ? 0 : delta;
 }
 
-static inline int
-task_sched_prio(const struct task_struct *p)
+static inline int task_sched_prio(const struct task_struct *p)
 {
 	return (p->prio < MAX_RT_PRIO) ? p->prio :
 		MIN_NORMAL_PRIO + task_sched_prio_normal(p, task_rq(p));
@@ -93,23 +92,21 @@ static inline void update_rq_time_edge(struct rq *rq)
 	delta = min_t(u64, NORMAL_PRIO_NUM, now - old);
 	INIT_LIST_HEAD(&head);
 
-	for_each_set_bit(prio, &rq->queue.bitmap[2], delta) {
-		u64 idx;
+	for_each_set_bit(prio, &rq->queue.bitmap[2], delta)
+		list_splice_tail_init(rq->queue.heads + MIN_NORMAL_PRIO +
+				      NORMAL_PRIO_MOD(prio + old), &head);
 
-		idx = MIN_NORMAL_PRIO + NORMAL_PRIO_MOD(prio + old);
-		list_splice_tail_init(rq->queue.heads + idx, &head);
-	}
 	rq->queue.bitmap[2] = (NORMAL_PRIO_NUM == delta) ? 0UL :
 		rq->queue.bitmap[2] >> delta;
 	rq->time_edge = now;
 	if (!list_empty(&head)) {
-		u64 new_idx = MIN_NORMAL_PRIO + NORMAL_PRIO_MOD(now);
+		u64 idx = MIN_NORMAL_PRIO + NORMAL_PRIO_MOD(now);
 		struct task_struct *p;
 
 		list_for_each_entry(p, &head, sq_node)
-			p->sq_idx = new_idx;
+			p->sq_idx = idx;
 
-		list_splice(&head, rq->queue.heads + new_idx);
+		list_splice(&head, rq->queue.heads + idx);
 		rq->queue.bitmap[2] |= 1UL;
 	}
 }
@@ -118,7 +115,6 @@ static inline void requeue_task(struct task_struct *p, struct rq *rq);
 
 static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
 {
-	/*printk(KERN_INFO "sched: time_slice_expired(%d) - %px\n", cpu_of(rq), p);*/
 	p->time_slice = sched_timeslice_ns;
 	sched_renew_deadline(p, rq);
 	if (SCHED_FIFO != p->policy && task_on_rq_queued(p))
-- 
2.32.0.93.g670b81a890


From 88b9f849dddfeb2b7ab88afaf225badc4ab33e4d Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Sun, 6 Jun 2021 09:32:26 +0000
Subject: [PATCH 24/30] sched/alt: Merge BMQ&PDS common code (II)

---
 kernel/sched/alt_core.c |  89 +++++++++++++++++++++++++++---
 kernel/sched/bmq.h      | 117 ++++++++++------------------------------
 kernel/sched/pds.h      |  96 ++-------------------------------
 3 files changed, 113 insertions(+), 189 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index db8f5b24089d..626bd8d20c4f 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -142,6 +142,8 @@ static cpumask_t sched_sg_idle_mask ____cacheline_aligned_in_smp;
 #endif
 static cpumask_t sched_rq_watermark[SCHED_BITS] ____cacheline_aligned_in_smp;
 
+static inline void requeue_task(struct task_struct *p, struct rq *rq);
+
 #ifdef CONFIG_SCHED_BMQ
 #include "bmq.h"
 #endif
@@ -171,8 +173,7 @@ static inline void sched_queue_init_idle(struct sched_queue *q,
 	list_add(&idle->sq_node, &q->heads[idle->sq_idx]);
 }
 
-
-/* water mark related functions*/
+/* water mark related functions */
 static inline void update_sched_rq_watermark(struct rq *rq)
 {
 	unsigned long watermark = find_first_bit(rq->queue.bitmap, SCHED_QUEUE_BITS);
@@ -180,8 +181,6 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 	unsigned long i;
 	int cpu;
 
-	/*printk(KERN_INFO "sched: watermark(%d) %d, last %d\n",
-	       cpu_of(rq), watermark, last_wm);*/
 	if (watermark == last_wm)
 		return;
 
@@ -216,6 +215,34 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 #endif
 }
 
+/*
+ * This routine assume that the idle task always in queue
+ */
+static inline struct task_struct *sched_rq_first_task(struct rq *rq)
+{
+	unsigned long idx = find_first_bit(rq->queue.bitmap, SCHED_QUEUE_BITS);
+	const struct list_head *head = &rq->queue.heads[sched_prio2idx(idx, rq)];
+
+	return list_first_entry(head, struct task_struct, sq_node);
+}
+
+static inline struct task_struct *
+sched_rq_next_task(struct task_struct *p, struct rq *rq)
+{
+	unsigned long idx = p->sq_idx;
+	struct list_head *head = &rq->queue.heads[idx];
+
+	if (list_is_last(&p->sq_node, head)) {
+		idx = find_next_bit(rq->queue.bitmap, SCHED_QUEUE_BITS,
+				    sched_idx2prio(idx, rq) + 1);
+		head = &rq->queue.heads[sched_prio2idx(idx, rq)];
+
+		return list_first_entry(head, struct task_struct, sq_node);
+	}
+
+	return list_next_entry(p, sq_node);
+}
+
 static inline struct task_struct *rq_runnable_task(struct rq *rq)
 {
 	struct task_struct *next = sched_rq_first_task(rq);
@@ -563,6 +590,25 @@ static inline void sched_update_tick_dependency(struct rq *rq) { }
  * Add/Remove/Requeue task to/from the runqueue routines
  * Context: rq->lock
  */
+#define __SCHED_DEQUEUE_TASK(p, rq, flags, func)		\
+	psi_dequeue(p, flags & DEQUEUE_SLEEP);			\
+	sched_info_dequeued(rq, p);				\
+								\
+	list_del(&p->sq_node);					\
+	if (list_empty(&rq->queue.heads[p->sq_idx])) {		\
+		clear_bit(sched_idx2prio(p->sq_idx, rq),	\
+			  rq->queue.bitmap);			\
+		func;						\
+	}
+
+#define __SCHED_ENQUEUE_TASK(p, rq, flags)				\
+	sched_info_queued(rq, p);					\
+	psi_enqueue(p, flags);						\
+									\
+	p->sq_idx = task_sched_prio_idx(p, rq);				\
+	list_add_tail(&p->sq_node, &rq->queue.heads[p->sq_idx]);	\
+	set_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);
+
 static inline void dequeue_task(struct task_struct *p, struct rq *rq, int flags)
 {
 	lockdep_assert_held(&rq->lock);
@@ -602,12 +648,25 @@ static inline void enqueue_task(struct task_struct *p, struct rq *rq, int flags)
 
 static inline void requeue_task(struct task_struct *p, struct rq *rq)
 {
+	int idx;
+
 	lockdep_assert_held(&rq->lock);
 	/*printk(KERN_INFO "sched: requeue(%d) %px %016llx\n", cpu_of(rq), p, p->priodl);*/
 	WARN_ONCE(task_rq(p) != rq, "sched: cpu[%d] requeue task reside on cpu%d\n",
 		  cpu_of(rq), task_cpu(p));
 
-	__SCHED_REQUEUE_TASK(p, rq, update_sched_rq_watermark(rq));
+	idx = task_sched_prio_idx(p, rq);
+
+	list_del(&p->sq_node);
+	list_add_tail(&p->sq_node, &rq->queue.heads[idx]);
+	if (idx != p->sq_idx) {
+		if (list_empty(&rq->queue.heads[p->sq_idx]))
+			clear_bit(sched_idx2prio(p->sq_idx, rq),
+				  rq->queue.bitmap);
+		p->sq_idx = idx;
+		set_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);
+		update_sched_rq_watermark(rq);
+	}
 }
 
 /*
@@ -4565,7 +4624,7 @@ EXPORT_SYMBOL(default_wake_function);
 static inline void check_task_changed(struct task_struct *p, struct rq *rq)
 {
 	/* Trigger resched if task sched_prio has been modified. */
-	if (task_on_rq_queued(p) && sched_task_need_requeue(p, rq)) {
+	if (task_on_rq_queued(p) && task_sched_prio_idx(p, rq) != p->sq_idx) {
 		requeue_task(p, rq);
 		check_preempt_curr(rq);
 	}
@@ -4755,6 +4814,24 @@ SYSCALL_DEFINE1(nice, int, increment)
 
 #endif
 
+/**
+ * task_prio - return the priority value of a given task.
+ * @p: the task in question.
+ *
+ * Return: The priority value as seen by users in /proc.
+ *
+ * sched policy         return value   kernel prio    user prio/nice
+ *
+ * (BMQ)normal, batch, idle[0 ... 53]  [100 ... 139]          0/[-20 ... 19]/[-7 ... 7]
+ * (PDS)normal, batch, idle[0 ... 39]            100          0/[-20 ... 19]
+ * fifo, rr             [-1 ... -100]     [99 ... 0]  [0 ... 99]
+ */
+int task_prio(const struct task_struct *p)
+{
+	return (p->prio < MAX_RT_PRIO) ? p->prio - MAX_RT_PRIO :
+		task_sched_prio_normal(p, task_rq(p));
+}
+
 /**
  * idle_cpu - is a given CPU idle currently?
  * @cpu: the processor in question.
diff --git a/kernel/sched/bmq.h b/kernel/sched/bmq.h
index 7299b5cc9a87..840173f29e42 100644
--- a/kernel/sched/bmq.h
+++ b/kernel/sched/bmq.h
@@ -36,6 +36,33 @@ static inline void deboost_task(struct task_struct *p)
 /*
  * Common interfaces
  */
+static inline int
+task_sched_prio_normal(const struct task_struct *p, const struct rq *rq)
+{
+	return p->prio + p->boost_prio - MAX_RT_PRIO;
+}
+
+static inline int task_sched_prio(const struct task_struct *p)
+{
+	return (p->prio < MAX_RT_PRIO)? p->prio : MAX_RT_PRIO / 2 + (p->prio + p->boost_prio) / 2;
+}
+
+static inline int
+task_sched_prio_idx(const struct task_struct *p, const struct rq *rq)
+{
+	return task_sched_prio(p);
+}
+
+static inline unsigned long sched_prio2idx(unsigned long prio, struct rq *rq)
+{
+	return prio;
+}
+
+static inline unsigned long sched_idx2prio(unsigned long idx, struct rq *rq)
+{
+	return idx;
+}
+
 static inline void sched_imp_init(void) {}
 
 static inline int normal_prio(struct task_struct *p)
@@ -46,13 +73,6 @@ static inline int normal_prio(struct task_struct *p)
 	return p->static_prio + MAX_PRIORITY_ADJ;
 }
 
-static inline int task_sched_prio(struct task_struct *p)
-{
-	return (p->prio < MAX_RT_PRIO)? p->prio : MAX_RT_PRIO / 2 + (p->prio + p->boost_prio) / 2;
-}
-
-static inline void requeue_task(struct task_struct *p, struct rq *rq);
-
 static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
 {
 	p->time_slice = sched_timeslice_ns;
@@ -71,95 +91,12 @@ inline int task_running_nice(struct task_struct *p)
 	return (p->prio + p->boost_prio > DEFAULT_PRIO + MAX_PRIORITY_ADJ);
 }
 
-/*
- * This routine used in bmq scheduler only which assume the idle task in the bmq
- */
-static inline struct task_struct *sched_rq_first_task(struct rq *rq)
-{
-	unsigned long idx = find_first_bit(rq->queue.bitmap, SCHED_QUEUE_BITS);
-	const struct list_head *head = &rq->queue.heads[idx];
-
-	return list_first_entry(head, struct task_struct, sq_node);
-}
-
-static inline struct task_struct *
-sched_rq_next_task(struct task_struct *p, struct rq *rq)
-{
-	unsigned long idx = p->sq_idx;
-	struct list_head *head = &rq->queue.heads[idx];
-
-	if (list_is_last(&p->sq_node, head)) {
-		idx = find_next_bit(rq->queue.bitmap, SCHED_QUEUE_BITS, idx + 1);
-		head = &rq->queue.heads[idx];
-
-		return list_first_entry(head, struct task_struct, sq_node);
-	}
-
-	return list_next_entry(p, sq_node);
-}
-
-#define __SCHED_DEQUEUE_TASK(p, rq, flags, func)	\
-	psi_dequeue(p, flags & DEQUEUE_SLEEP);		\
-	sched_info_dequeued(rq, p);			\
-							\
-	list_del(&p->sq_node);				\
-	if (list_empty(&rq->queue.heads[p->sq_idx])) {	\
-		clear_bit(p->sq_idx, rq->queue.bitmap);	\
-		func;					\
-	}
-
-#define __SCHED_ENQUEUE_TASK(p, rq, flags)				\
-	sched_info_queued(rq, p);					\
-	psi_enqueue(p, flags);						\
-									\
-	p->sq_idx = task_sched_prio(p);					\
-	list_add_tail(&p->sq_node, &rq->queue.heads[p->sq_idx]);	\
-	set_bit(p->sq_idx, rq->queue.bitmap)
-
-#define __SCHED_REQUEUE_TASK(p, rq, func)				\
-{									\
-	int idx = task_sched_prio(p);					\
-\
-	list_del(&p->sq_node);						\
-	list_add_tail(&p->sq_node, &rq->queue.heads[idx]);		\
-	if (idx != p->sq_idx) {						\
-		if (list_empty(&rq->queue.heads[p->sq_idx]))		\
-			clear_bit(p->sq_idx, rq->queue.bitmap);		\
-		p->sq_idx = idx;					\
-		set_bit(p->sq_idx, rq->queue.bitmap);			\
-		func;							\
-	}								\
-}
-
-static inline bool sched_task_need_requeue(struct task_struct *p, struct rq *rq)
-{
-	return (task_sched_prio(p) != p->sq_idx);
-}
-
 static void sched_task_fork(struct task_struct *p, struct rq *rq)
 {
 	p->boost_prio = (p->boost_prio < 0) ?
 		p->boost_prio + MAX_PRIORITY_ADJ : MAX_PRIORITY_ADJ;
 }
 
-/**
- * task_prio - return the priority value of a given task.
- * @p: the task in question.
- *
- * Return: The priority value as seen by users in /proc.
- *
- * sched policy         return value   kernel prio    user prio/nice/boost
- *
- * normal, batch, idle     [0 ... 53]  [100 ... 139]          0/[-20 ... 19]/[-7 ... 7]
- * fifo, rr             [-1 ... -100]     [99 ... 0]  [0 ... 99]
- */
-int task_prio(const struct task_struct *p)
-{
-	if (p->prio < MAX_RT_PRIO)
-		return (p->prio - MAX_RT_PRIO);
-	return (p->prio - MAX_RT_PRIO + p->boost_prio);
-}
-
 static void do_sched_yield_type_1(struct task_struct *p, struct rq *rq)
 {
 	p->boost_prio = MAX_PRIORITY_ADJ;
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index 5ce0a16eb454..31c6bd4d29c8 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -6,6 +6,9 @@ extern int alt_debug[20];
 
 #define NORMAL_PRIO_MOD(x)	((x) & (NORMAL_PRIO_NUM - 1))
 
+/*
+ * Common interfaces
+ */
 static inline int
 task_sched_prio_normal(const struct task_struct *p, const struct rq *rq)
 {
@@ -55,9 +58,6 @@ static inline void sched_renew_deadline(struct task_struct *p, const struct rq *
 			(MAX_PRIO - NICE_WIDTH)];
 }
 
-/*
- * Common interfaces
- */
 static inline void sched_imp_init(void)
 {
 	int i;
@@ -111,8 +111,6 @@ static inline void update_rq_time_edge(struct rq *rq)
 	}
 }
 
-static inline void requeue_task(struct task_struct *p, struct rq *rq);
-
 static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
 {
 	p->time_slice = sched_timeslice_ns;
@@ -127,99 +125,11 @@ static inline void sched_task_sanity_check(struct task_struct *p, struct rq *rq)
 		p->deadline = rq->clock + user_prio2deadline[NICE_WIDTH - 1];
 }
 
-/*
- * This routine assume that the idle task always in queue
- */
-static inline struct task_struct *sched_rq_first_task(struct rq *rq)
-{
-	unsigned long idx = find_first_bit(rq->queue.bitmap, SCHED_QUEUE_BITS);
-	const struct list_head *head = &rq->queue.heads[sched_prio2idx(idx, rq)];
-
-	return list_first_entry(head, struct task_struct, sq_node);
-}
-
-static inline struct task_struct *
-sched_rq_next_task(struct task_struct *p, struct rq *rq)
-{
-	unsigned long idx = p->sq_idx;
-	struct list_head *head = &rq->queue.heads[idx];
-
-	if (list_is_last(&p->sq_node, head)) {
-		idx = find_next_bit(rq->queue.bitmap, SCHED_QUEUE_BITS,
-				    sched_idx2prio(idx, rq) + 1);
-		head = &rq->queue.heads[sched_prio2idx(idx, rq)];
-
-		return list_first_entry(head, struct task_struct, sq_node);
-	}
-
-	return list_next_entry(p, sq_node);
-}
-
-#define __SCHED_DEQUEUE_TASK(p, rq, flags, func)		\
-	psi_dequeue(p, flags & DEQUEUE_SLEEP);			\
-	sched_info_dequeued(rq, p);				\
-								\
-	list_del(&p->sq_node);					\
-	if (list_empty(&rq->queue.heads[p->sq_idx])) {		\
-		clear_bit(sched_idx2prio(p->sq_idx, rq),	\
-			  rq->queue.bitmap);			\
-		func;						\
-	}
-
-#define __SCHED_ENQUEUE_TASK(p, rq, flags)				\
-	sched_info_queued(rq, p);					\
-	psi_enqueue(p, flags);						\
-									\
-	p->sq_idx = task_sched_prio_idx(p, rq);				\
-	list_add_tail(&p->sq_node, &rq->queue.heads[p->sq_idx]);	\
-	set_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);
-
-/*
- * Requeue a task @p to @rq
- */
-#define __SCHED_REQUEUE_TASK(p, rq, func)					\
-{\
-	int idx = task_sched_prio_idx(p, rq);					\
-\
-	list_del(&p->sq_node);							\
-	list_add_tail(&p->sq_node, &rq->queue.heads[idx]);			\
-	if (idx != p->sq_idx) {							\
-		if (list_empty(&rq->queue.heads[p->sq_idx]))			\
-			clear_bit(sched_idx2prio(p->sq_idx, rq),		\
-				  rq->queue.bitmap);				\
-		p->sq_idx = idx;						\
-		set_bit(sched_idx2prio(p->sq_idx, rq), rq->queue.bitmap);	\
-		func;								\
-	}									\
-}
-
-static inline bool sched_task_need_requeue(struct task_struct *p, struct rq *rq)
-{
-	return (task_sched_prio_idx(p, rq) != p->sq_idx);
-}
-
 static void sched_task_fork(struct task_struct *p, struct rq *rq)
 {
 	sched_renew_deadline(p, rq);
 }
 
-/**
- * task_prio - return the priority value of a given task.
- * @p: the task in question.
- *
- * Return: The priority value as seen by users in /proc.
- *
- * sched policy         return value   kernel prio    user prio/nice
- *
- * normal, batch, idle     [0 ... 39]            100          0/[-20 ... 19]
- * fifo, rr             [-1 ... -100]     [99 ... 0]  [0 ... 99]
- */
-int task_prio(const struct task_struct *p)
-{
-	return (p->prio < MAX_RT_PRIO) ? p->prio - MAX_RT_PRIO :
-		task_sched_prio_normal(p, task_rq(p));
-}
-
 static void do_sched_yield_type_1(struct task_struct *p, struct rq *rq)
 {
 	time_slice_expired(p, rq);
-- 
2.32.0.93.g670b81a890


From 0fed282e992a1a759254ee6fd9ca3c35b02e06a1 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Sun, 6 Jun 2021 18:04:37 +0000
Subject: [PATCH 25/30] sched/pds: Introduce sched_timeslice_shift

---
 kernel/sched/alt_core.c | 28 +++++++++++++++-------------
 kernel/sched/bmq.h      |  2 ++
 kernel/sched/pds.h      | 13 ++++++++++---
 3 files changed, 27 insertions(+), 16 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 626bd8d20c4f..799605256a19 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -79,13 +79,24 @@ __read_mostly int sysctl_resched_latency_warn_once = 1;
 /* Default time slice is 4 in ms, can be set via kernel parameter "sched_timeslice" */
 u64 sched_timeslice_ns __read_mostly = (2 << 20);
 
+static inline void requeue_task(struct task_struct *p, struct rq *rq);
+
+#ifdef CONFIG_SCHED_BMQ
+#include "bmq.h"
+#endif
+#ifdef CONFIG_SCHED_PDS
+#include "pds.h"
+#endif
+
 static int __init sched_timeslice(char *str)
 {
-	int timeslice_us;
+	int timeslice_ms;
 
-	get_option(&str, &timeslice_us);
-	if (timeslice_us >= 1000)
-		sched_timeslice_ns = (timeslice_us / 1000) << 20;
+	get_option(&str, &timeslice_ms);
+	if (2 != timeslice_ms)
+		timeslice_ms = 4;
+	sched_timeslice_ns = timeslice_ms << 20;
+	sched_timeslice_imp(timeslice_ms);
 
 	return 0;
 }
@@ -142,15 +153,6 @@ static cpumask_t sched_sg_idle_mask ____cacheline_aligned_in_smp;
 #endif
 static cpumask_t sched_rq_watermark[SCHED_BITS] ____cacheline_aligned_in_smp;
 
-static inline void requeue_task(struct task_struct *p, struct rq *rq);
-
-#ifdef CONFIG_SCHED_BMQ
-#include "bmq.h"
-#endif
-#ifdef CONFIG_SCHED_PDS
-#include "pds.h"
-#endif
-
 /* sched_queue related functions */
 static inline void sched_queue_init(struct sched_queue *q)
 {
diff --git a/kernel/sched/bmq.h b/kernel/sched/bmq.h
index 840173f29e42..f9f58c21c1e4 100644
--- a/kernel/sched/bmq.h
+++ b/kernel/sched/bmq.h
@@ -36,6 +36,8 @@ static inline void deboost_task(struct task_struct *p)
 /*
  * Common interfaces
  */
+static inline void sched_timeslice_imp(const int timeslice_ms) {}
+
 static inline int
 task_sched_prio_normal(const struct task_struct *p, const struct rq *rq)
 {
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index 31c6bd4d29c8..b9b19c6a7622 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -1,6 +1,7 @@
 #define ALT_SCHED_VERSION_MSG "sched/pds: PDS CPU Scheduler "ALT_SCHED_VERSION" by Alfred Chen.\n"
 
 static u64 user_prio2deadline[NICE_WIDTH];
+static int sched_timeslice_shift = 22;
 
 extern int alt_debug[20];
 
@@ -9,15 +10,21 @@ extern int alt_debug[20];
 /*
  * Common interfaces
  */
+static inline void sched_timeslice_imp(const int timeslice_ms)
+{
+	if (2 == timeslice_ms)
+		sched_timeslice_shift = 21;
+}
+
 static inline int
 task_sched_prio_normal(const struct task_struct *p, const struct rq *rq)
 {
-	s64 delta = (p->deadline >> 21) - rq->time_edge +
+	s64 delta = (p->deadline >> sched_timeslice_shift) - rq->time_edge +
 		NORMAL_PRIO_NUM - NICE_WIDTH - 1;
 
 	if (unlikely(delta > NORMAL_PRIO_NUM - 1)) {
 		pr_info("pds: task_sched_prio_normal delta %lld, deadline %llu(%llu), time_edge %llu\n",
-			delta, p->deadline, p->deadline >> 21, rq->time_edge);
+			delta, p->deadline, p->deadline >> sched_timeslice_shift, rq->time_edge);
 		return NORMAL_PRIO_NUM - 1;
 	}
 
@@ -83,7 +90,7 @@ static inline void update_rq_time_edge(struct rq *rq)
 {
 	struct list_head head;
 	u64 old = rq->time_edge;
-	u64 now = rq->clock >> 21;
+	u64 now = rq->clock >> sched_timeslice_shift;
 	u64 prio, delta;
 
 	if (now == old)
-- 
2.32.0.93.g670b81a890


From 9df27b555be282c1b97fc50a6df5247d1b1b0b64 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Mon, 7 Jun 2021 09:31:11 +0000
Subject: [PATCH 26/30] sched/pds: Optimize task deadline

---
 include/linux/sched/deadline.h |  2 +-
 kernel/sched/alt_core.c        | 11 +----------
 kernel/sched/bmq.h             |  2 --
 kernel/sched/pds.h             | 29 ++++++++---------------------
 4 files changed, 10 insertions(+), 34 deletions(-)

diff --git a/include/linux/sched/deadline.h b/include/linux/sched/deadline.h
index 3f208b842745..216fdf2fe90c 100644
--- a/include/linux/sched/deadline.h
+++ b/include/linux/sched/deadline.h
@@ -12,7 +12,7 @@ static inline int dl_task(struct task_struct *p)
 #endif
 
 #ifdef CONFIG_SCHED_PDS
-#define __tsk_deadline(p)	((((u64) ((p)->prio))<<56) | (((p)->deadline)>>8))
+#define __tsk_deadline(p)	((((u64) ((p)->prio))<<56) | (p)->deadline)
 #endif
 
 #else
diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 799605256a19..946983ca5763 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -3902,15 +3902,7 @@ void alt_sched_debug(void)
 	       sched_sg_idle_mask.bits[0]);
 }
 #else
-int alt_debug[20];
-
-inline void alt_sched_debug(void)
-{
-	int i;
-
-	for (i = 0; i < 6; i++)
-		printk(KERN_INFO "sched: %d\n", alt_debug[i]);
-}
+inline void alt_sched_debug(void) {}
 #endif
 
 #ifdef	CONFIG_SMP
@@ -6768,7 +6760,6 @@ void __init sched_init(void)
 	struct rq *rq;
 
 	printk(KERN_INFO ALT_SCHED_VERSION_MSG);
-	sched_imp_init();
 
 	wait_bit_init();
 
diff --git a/kernel/sched/bmq.h b/kernel/sched/bmq.h
index f9f58c21c1e4..b425f8979b6f 100644
--- a/kernel/sched/bmq.h
+++ b/kernel/sched/bmq.h
@@ -65,8 +65,6 @@ static inline unsigned long sched_idx2prio(unsigned long idx, struct rq *rq)
 	return idx;
 }
 
-static inline void sched_imp_init(void) {}
-
 static inline int normal_prio(struct task_struct *p)
 {
 	if (task_has_rt_policy(p))
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index b9b19c6a7622..4898b3ae8e41 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -1,10 +1,7 @@
 #define ALT_SCHED_VERSION_MSG "sched/pds: PDS CPU Scheduler "ALT_SCHED_VERSION" by Alfred Chen.\n"
 
-static u64 user_prio2deadline[NICE_WIDTH];
 static int sched_timeslice_shift = 22;
 
-extern int alt_debug[20];
-
 #define NORMAL_PRIO_MOD(x)	((x) & (NORMAL_PRIO_NUM - 1))
 
 /*
@@ -19,12 +16,11 @@ static inline void sched_timeslice_imp(const int timeslice_ms)
 static inline int
 task_sched_prio_normal(const struct task_struct *p, const struct rq *rq)
 {
-	s64 delta = (p->deadline >> sched_timeslice_shift) - rq->time_edge +
-		NORMAL_PRIO_NUM - NICE_WIDTH - 1;
+	s64 delta = p->deadline - rq->time_edge + NORMAL_PRIO_NUM - NICE_WIDTH;
 
 	if (unlikely(delta > NORMAL_PRIO_NUM - 1)) {
-		pr_info("pds: task_sched_prio_normal delta %lld, deadline %llu(%llu), time_edge %llu\n",
-			delta, p->deadline, p->deadline >> sched_timeslice_shift, rq->time_edge);
+		pr_info("pds: task_sched_prio_normal delta %lld, deadline %llu, time_edge %llu\n",
+			delta, p->deadline, rq->time_edge);
 		return NORMAL_PRIO_NUM - 1;
 	}
 
@@ -61,18 +57,8 @@ static inline unsigned long sched_idx2prio(unsigned long idx, struct rq *rq)
 static inline void sched_renew_deadline(struct task_struct *p, const struct rq *rq)
 {
 	if (p->prio >= MAX_RT_PRIO)
-		p->deadline = rq->clock + user_prio2deadline[p->static_prio -
-			(MAX_PRIO - NICE_WIDTH)];
-}
-
-static inline void sched_imp_init(void)
-{
-	int i;
-
-	user_prio2deadline[0] = sched_timeslice_ns;
-	for (i = 1; i < NICE_WIDTH; i++)
-		user_prio2deadline[i] =
-			user_prio2deadline[i - 1] + sched_timeslice_ns;
+		p->deadline = (rq->clock >> sched_timeslice_shift) +
+			p->static_prio - (MAX_PRIO - NICE_WIDTH);
 }
 
 static inline int normal_prio(struct task_struct *p)
@@ -128,8 +114,9 @@ static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
 
 static inline void sched_task_sanity_check(struct task_struct *p, struct rq *rq)
 {
-	if (unlikely(p->deadline > rq->clock + user_prio2deadline[NICE_WIDTH - 1]))
-		p->deadline = rq->clock + user_prio2deadline[NICE_WIDTH - 1];
+	u64 max_dl = rq->time_edge + NICE_WIDTH - 1;
+	if (unlikely(p->deadline > max_dl))
+		p->deadline = max_dl;
 }
 
 static void sched_task_fork(struct task_struct *p, struct rq *rq)
-- 
2.32.0.93.g670b81a890


From 17cd077951b74c244ef2297c5326c91584060736 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Fri, 11 Jun 2021 18:19:19 +0000
Subject: [PATCH 27/30] sched/pds: Optimize parameter and return types

---
 kernel/sched/bmq.h | 4 ++--
 kernel/sched/pds.h | 4 ++--
 2 files changed, 4 insertions(+), 4 deletions(-)

diff --git a/kernel/sched/bmq.h b/kernel/sched/bmq.h
index b425f8979b6f..76db5eb21a01 100644
--- a/kernel/sched/bmq.h
+++ b/kernel/sched/bmq.h
@@ -55,12 +55,12 @@ task_sched_prio_idx(const struct task_struct *p, const struct rq *rq)
 	return task_sched_prio(p);
 }
 
-static inline unsigned long sched_prio2idx(unsigned long prio, struct rq *rq)
+static inline int sched_prio2idx(int prio, struct rq *rq)
 {
 	return prio;
 }
 
-static inline unsigned long sched_idx2prio(unsigned long idx, struct rq *rq)
+static inline int sched_idx2prio(int idx, struct rq *rq)
 {
 	return idx;
 }
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index 4898b3ae8e41..ed336dd35809 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -40,14 +40,14 @@ task_sched_prio_idx(const struct task_struct *p, const struct rq *rq)
 		NORMAL_PRIO_MOD(task_sched_prio_normal(p, rq) + rq->time_edge);
 }
 
-static inline unsigned long sched_prio2idx(unsigned long prio, struct rq *rq)
+static inline int sched_prio2idx(int prio, struct rq *rq)
 {
 	return (IDLE_TASK_SCHED_PRIO == prio || prio < MAX_RT_PRIO) ? prio :
 		MIN_NORMAL_PRIO + NORMAL_PRIO_MOD((prio - MIN_NORMAL_PRIO) +
 						  rq->time_edge);
 }
 
-static inline unsigned long sched_idx2prio(unsigned long idx, struct rq *rq)
+static inline int sched_idx2prio(int idx, struct rq *rq)
 {
 	return (idx < MAX_RT_PRIO) ? idx : MIN_NORMAL_PRIO +
 		NORMAL_PRIO_MOD((idx - MIN_NORMAL_PRIO) + NORMAL_PRIO_NUM -
-- 
2.32.0.93.g670b81a890


From 3f501427f59a8eba7bf0d29b9ac029c1b2da1f84 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Sun, 13 Jun 2021 11:34:41 +0000
Subject: [PATCH 28/30] sched/pds: Use common normal_prio()

---
 init/init_task.c        |  7 +------
 kernel/sched/alt_core.c | 13 +++++++++++++
 kernel/sched/bmq.h      |  8 --------
 kernel/sched/pds.h      |  6 ------
 4 files changed, 14 insertions(+), 20 deletions(-)

diff --git a/init/init_task.c b/init/init_task.c
index 89f1e6ace69a..0dfa1a63dc4e 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -75,15 +75,10 @@ struct task_struct init_task
 	.stack		= init_stack,
 	.usage		= REFCOUNT_INIT(2),
 	.flags		= PF_KTHREAD,
-#ifdef CONFIG_SCHED_BMQ
+#ifdef CONFIG_SCHED_ALT
 	.prio		= DEFAULT_PRIO + MAX_PRIORITY_ADJ,
 	.static_prio	= DEFAULT_PRIO,
 	.normal_prio	= DEFAULT_PRIO + MAX_PRIORITY_ADJ,
-#endif
-#ifdef CONFIG_SCHED_PDS
-	.prio		= MAX_RT_PRIO,
-	.static_prio	= DEFAULT_PRIO,
-	.normal_prio	= MAX_RT_PRIO,
 #else
 	.prio		= MAX_PRIO - 20,
 	.static_prio	= MAX_PRIO - 20,
diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 946983ca5763..57c34cf29956 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -1121,6 +1121,19 @@ static inline void hrtick_rq_init(struct rq *rq)
 }
 #endif	/* CONFIG_SCHED_HRTICK */
 
+/*
+ * Calculate the expected normal priority: i.e. priority
+ * without taking RT-inheritance into account. Might be
+ * boosted by interactivity modifiers. Changes upon fork,
+ * setprio syscalls, and whenever the interactivity
+ * estimator recalculates.
+ */
+static inline int normal_prio(struct task_struct *p)
+{
+	return task_has_rt_policy(p) ? (MAX_RT_PRIO - 1 - p->rt_priority) :
+		p->static_prio + MAX_PRIORITY_ADJ;
+}
+
 /*
  * Calculate the current priority, i.e. the priority
  * taken into account by the scheduler. This value might
diff --git a/kernel/sched/bmq.h b/kernel/sched/bmq.h
index 76db5eb21a01..7635c00dde7f 100644
--- a/kernel/sched/bmq.h
+++ b/kernel/sched/bmq.h
@@ -65,14 +65,6 @@ static inline int sched_idx2prio(int idx, struct rq *rq)
 	return idx;
 }
 
-static inline int normal_prio(struct task_struct *p)
-{
-	if (task_has_rt_policy(p))
-		return MAX_RT_PRIO - 1 - p->rt_priority;
-
-	return p->static_prio + MAX_PRIORITY_ADJ;
-}
-
 static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
 {
 	p->time_slice = sched_timeslice_ns;
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index ed336dd35809..c23294178c2b 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -61,12 +61,6 @@ static inline void sched_renew_deadline(struct task_struct *p, const struct rq *
 			p->static_prio - (MAX_PRIO - NICE_WIDTH);
 }
 
-static inline int normal_prio(struct task_struct *p)
-{
-	return task_has_rt_policy(p) ? (MAX_RT_PRIO - 1 - p->rt_priority) :
-		MAX_RT_PRIO;
-}
-
 int task_running_nice(struct task_struct *p)
 {
 	return task_sched_prio(p) > DEFAULT_PRIO;
-- 
2.32.0.93.g670b81a890


From 4ead4d399b07ba6a104931effffd9316e5add31c Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Fri, 25 Jun 2021 07:24:08 +0000
Subject: [PATCH 29/30] sched/alt: Optimization and code clean-up

---
 kernel/sched/alt_core.c | 20 +++++++-------------
 kernel/sched/pds.h      |  2 +-
 2 files changed, 8 insertions(+), 14 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 57c34cf29956..a8ba783b07ff 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -77,7 +77,7 @@ __read_mostly int sysctl_resched_latency_warn_once = 1;
 #define STOP_PRIO		(MAX_RT_PRIO - 1)
 
 /* Default time slice is 4 in ms, can be set via kernel parameter "sched_timeslice" */
-u64 sched_timeslice_ns __read_mostly = (2 << 20);
+u64 sched_timeslice_ns __read_mostly = (4 << 20);
 
 static inline void requeue_task(struct task_struct *p, struct rq *rq);
 
@@ -193,9 +193,8 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 			cpumask_andnot(&sched_rq_watermark[i],
 				       &sched_rq_watermark[i], cpumask_of(cpu));
 #ifdef CONFIG_SCHED_SMT
-		if (!static_branch_likely(&sched_smt_present))
-			return;
-		if (IDLE_WM == last_wm)
+		if (static_branch_likely(&sched_smt_present) &&
+		    IDLE_WM == last_wm)
 			cpumask_andnot(&sched_sg_idle_mask,
 				       &sched_sg_idle_mask, cpu_smt_mask(cpu));
 #endif
@@ -205,10 +204,9 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 	for (i = last_wm + 1; i <= watermark; i++)
 		cpumask_set_cpu(cpu, &sched_rq_watermark[i]);
 #ifdef CONFIG_SCHED_SMT
-	if (!static_branch_likely(&sched_smt_present))
-		return;
-	if (IDLE_WM == watermark) {
+	if (static_branch_likely(&sched_smt_present) && IDLE_WM == watermark) {
 		cpumask_t tmp;
+
 		cpumask_and(&tmp, cpu_smt_mask(cpu), &sched_rq_watermark[IDLE_WM]);
 		if (cpumask_equal(&tmp, cpu_smt_mask(cpu)))
 			cpumask_or(&sched_sg_idle_mask, cpu_smt_mask(cpu),
@@ -1003,13 +1001,10 @@ static void hrtick_clear(struct rq *rq)
 static enum hrtimer_restart hrtick(struct hrtimer *timer)
 {
 	struct rq *rq = container_of(timer, struct rq, hrtick_timer);
-	struct task_struct *p;
 
 	WARN_ON_ONCE(cpu_of(rq) != smp_processor_id());
 
 	raw_spin_lock(&rq->lock);
-	p = rq->curr;
-	p->time_slice = 0;
 	resched_curr(rq);
 	raw_spin_unlock(&rq->lock);
 
@@ -2733,9 +2728,7 @@ void wake_up_new_task(struct task_struct *p)
 	struct rq *rq;
 
 	raw_spin_lock_irqsave(&p->pi_lock, flags);
-
 	p->state = TASK_RUNNING;
-
 	rq = cpu_rq(select_task_rq(p));
 #ifdef CONFIG_SMP
 	rseq_migrate(p);
@@ -2743,6 +2736,7 @@ void wake_up_new_task(struct task_struct *p)
 	 * Fork balancing, do it here and not earlier because:
 	 * - cpus_ptr can change in the fork path
 	 * - any previously selected CPU might disappear through hotplug
+	 *
 	 * Use __set_task_cpu() to avoid calling sched_class::migrate_task_rq,
 	 * as we're not fully set-up yet.
 	 */
@@ -2750,8 +2744,8 @@ void wake_up_new_task(struct task_struct *p)
 #endif
 
 	raw_spin_lock(&rq->lock);
-
 	update_rq_clock(rq);
+
 	activate_task(p, rq);
 	trace_sched_wakeup_new(p);
 	check_preempt_curr(rq);
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index c23294178c2b..06d88e72b543 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -63,7 +63,7 @@ static inline void sched_renew_deadline(struct task_struct *p, const struct rq *
 
 int task_running_nice(struct task_struct *p)
 {
-	return task_sched_prio(p) > DEFAULT_PRIO;
+	return (p->prio > DEFAULT_PRIO);
 }
 
 static inline void update_rq_time_edge(struct rq *rq)
-- 
2.32.0.93.g670b81a890


From f6b918c68403c1a886ca5e91242c270cae85c184 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Tue, 6 Jul 2021 11:18:04 +0000
Subject: [PATCH 30/30] Project-C v5.13-r1

---
 kernel/sched/alt_core.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index a8ba783b07ff..b65b12c6014f 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -67,7 +67,7 @@ __read_mostly int sysctl_resched_latency_warn_once = 1;
 #define sched_feat(x)	(0)
 #endif /* CONFIG_SCHED_DEBUG */
 
-#define ALT_SCHED_VERSION "v5.13-r0"
+#define ALT_SCHED_VERSION "v5.13-r1"
 
 /* rt_prio(prio) defined in include/linux/sched/rt.h */
 #define rt_task(p)		rt_prio((p)->prio)
-- 
2.32.0.93.g670b81a890

