From 652f7223fdba978f542bc14f5365799d15831c1f Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Sun, 9 Aug 2020 08:49:45 +0800
Subject: [PATCH 035/148] sched/pds: Port of PDS

Port PDS from 5.0 to current Project C.
---
 include/linux/sched.h          |  11 +-
 include/linux/sched/deadline.h |   4 +
 include/linux/sched/prio.h     |   5 +-
 include/linux/skip_list.h      | 177 ++++++++++++++++++++++++++++
 init/Kconfig                   |   6 +
 init/init_task.c               |   7 +-
 kernel/locking/rtmutex.c       |   8 ++
 kernel/sched/alt_core.c        | 110 ++++++------------
 kernel/sched/alt_sched.h       |  11 +-
 kernel/sched/bmq.h             |   6 +
 kernel/sched/bmq_imp.h         | 127 +++++++++++++++++---
 kernel/sched/pds.h             |  14 +++
 kernel/sched/pds_imp.h         | 205 +++++++++++++++++++++++++++++++++
 13 files changed, 593 insertions(+), 98 deletions(-)
 create mode 100644 include/linux/skip_list.h
 create mode 100644 kernel/sched/pds.h
 create mode 100644 kernel/sched/pds_imp.h

diff --git a/include/linux/sched.h b/include/linux/sched.h
index dc64fdc1ed87..4c112a46c88d 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -34,6 +34,7 @@
 #include <linux/rseq.h>
 #include <linux/seqlock.h>
 #include <linux/kcsan.h>
+#include <linux/skip_list.h>
 #include <asm/kmap_size.h>
 
 /* task_struct member predeclarations (sorted alphabetically): */
@@ -715,11 +716,19 @@ struct task_struct {
 #ifdef CONFIG_SCHED_ALT
 	u64				last_ran;
 	s64				time_slice;
-	int				boost_prio;
 #ifdef CONFIG_SCHED_BMQ
+	int				boost_prio;
 	int				bmq_idx;
 	struct list_head		bmq_node;
 #endif /* CONFIG_SCHED_BMQ */
+#ifdef CONFIG_SCHED_PDS
+	u64				deadline;
+	u64				priodl;
+	/* skip list level */
+	int				sl_level;
+	/* skip list node */
+	struct skiplist_node		sl_node;
+#endif /* CONFIG_SCHED_PDS */
 	/* sched_clock time spent running */
 	u64				sched_time;
 #else /* !CONFIG_SCHED_ALT */
diff --git a/include/linux/sched/deadline.h b/include/linux/sched/deadline.h
index da0306d2fedb..45f0b0f3616c 100644
--- a/include/linux/sched/deadline.h
+++ b/include/linux/sched/deadline.h
@@ -11,6 +11,10 @@ static inline int dl_task(struct task_struct *p)
 }
 #endif
 
+#ifdef CONFIG_SCHED_PDS
+#define __tsk_deadline(p)	((p)->priodl)
+#endif
+
 #else
 
 #define __tsk_deadline(p)	((p)->dl.deadline)
diff --git a/include/linux/sched/prio.h b/include/linux/sched/prio.h
index dc7c5c159749..4d4f92bffeea 100644
--- a/include/linux/sched/prio.h
+++ b/include/linux/sched/prio.h
@@ -18,10 +18,13 @@
 #define MAX_PRIO		(MAX_RT_PRIO + NICE_WIDTH)
 #define DEFAULT_PRIO		(MAX_RT_PRIO + NICE_WIDTH / 2)
 
-#ifdef CONFIG_SCHED_ALT
 /* +/- priority levels from the base priority */
+#ifdef CONFIG_SCHED_BMQ
 #define MAX_PRIORITY_ADJ	7
 #endif
+#ifdef CONFIG_SCHED_PDS
+#define MAX_PRIORITY_ADJ	0
+#endif
 
 /*
  * Convert user-nice values [ -20 ... 0 ... 19 ]
diff --git a/include/linux/skip_list.h b/include/linux/skip_list.h
new file mode 100644
index 000000000000..47ca955a451d
--- /dev/null
+++ b/include/linux/skip_list.h
@@ -0,0 +1,177 @@
+/*
+ * Copyright (C) 2016 Alfred Chen.
+ *
+ * Code based on Con Kolivas's skip list implementation for BFS, and
+ * which is based on example originally by William Pugh.
+ *
+ * Skip Lists are a probabilistic alternative to balanced trees, as
+ * described in the June 1990 issue of CACM and were invented by
+ * William Pugh in 1987.
+ *
+ * A couple of comments about this implementation:
+ *
+ * This file only provides a infrastructure of skip list.
+ *
+ * skiplist_node is embedded into container data structure, to get rid
+ * the dependency of kmalloc/kfree operation in scheduler code.
+ *
+ * A customized search function should be defined using DEFINE_SKIPLIST_INSERT
+ * macro and be used for skip list insert operation.
+ *
+ * Random Level is also not defined in this file, instead, it should be
+ * customized implemented and set to node->level then pass to the customized
+ * skiplist_insert function.
+ *
+ * Levels start at zero and go up to (NUM_SKIPLIST_LEVEL -1)
+ *
+ * NUM_SKIPLIST_LEVEL in this implementation is 8 instead of origin 16,
+ * considering that there will be 256 entries to enable the top level when using
+ * random level p=0.5, and that number is more than enough for a run queue usage
+ * in a scheduler usage. And it also help to reduce the memory usage of the
+ * embedded skip list node in task_struct to about 50%.
+ *
+ * The insertion routine has been implemented so as to use the
+ * dirty hack described in the CACM paper: if a random level is
+ * generated that is more than the current maximum level, the
+ * current maximum level plus one is used instead.
+ *
+ * BFS Notes: In this implementation of skiplists, there are bidirectional
+ * next/prev pointers and the insert function returns a pointer to the actual
+ * node the value is stored. The key here is chosen by the scheduler so as to
+ * sort tasks according to the priority list requirements and is no longer used
+ * by the scheduler after insertion. The scheduler lookup, however, occurs in
+ * O(1) time because it is always the first item in the level 0 linked list.
+ * Since the task struct stores a copy of the node pointer upon skiplist_insert,
+ * it can also remove it much faster than the original implementation with the
+ * aid of prev<->next pointer manipulation and no searching.
+ */
+#ifndef _LINUX_SKIP_LIST_H
+#define _LINUX_SKIP_LIST_H
+
+#include <linux/kernel.h>
+
+#define NUM_SKIPLIST_LEVEL (8)
+
+struct skiplist_node {
+	int level;	/* Levels in this node */
+	struct skiplist_node *next[NUM_SKIPLIST_LEVEL];
+	struct skiplist_node *prev[NUM_SKIPLIST_LEVEL];
+};
+
+#define SKIPLIST_NODE_INIT(name) { 0,\
+				   {&name, &name, &name, &name,\
+				    &name, &name, &name, &name},\
+				   {&name, &name, &name, &name,\
+				    &name, &name, &name, &name},\
+				 }
+
+static inline void INIT_SKIPLIST_NODE(struct skiplist_node *node)
+{
+	/* only level 0 ->next matters in skiplist_empty() */
+	WRITE_ONCE(node->next[0], node);
+}
+
+/**
+ * FULL_INIT_SKIPLIST_NODE -- fully init a skiplist_node, expecially for header
+ * @node: the skip list node to be inited.
+ */
+static inline void FULL_INIT_SKIPLIST_NODE(struct skiplist_node *node)
+{
+	int i;
+
+	node->level = 0;
+	for (i = 0; i < NUM_SKIPLIST_LEVEL; i++) {
+		WRITE_ONCE(node->next[i], node);
+		node->prev[i] = node;
+	}
+}
+
+/**
+ * skiplist_empty - test whether a skip list is empty
+ * @head: the skip list to test.
+ */
+static inline int skiplist_empty(const struct skiplist_node *head)
+{
+	return READ_ONCE(head->next[0]) == head;
+}
+
+/**
+ * skiplist_entry - get the struct for this entry
+ * @ptr: the &struct skiplist_node pointer.
+ * @type:       the type of the struct this is embedded in.
+ * @member:     the name of the skiplist_node within the struct.
+ */
+#define skiplist_entry(ptr, type, member) \
+	container_of(ptr, type, member)
+
+/**
+ * DEFINE_SKIPLIST_INSERT_FUNC -- macro to define a customized skip list insert
+ * function, which takes two parameters, first one is the header node of the
+ * skip list, second one is the skip list node to be inserted
+ * @func_name: the customized skip list insert function name
+ * @search_func: the search function to be used, which takes two parameters,
+ * 1st one is the itrator of skiplist_node in the list, the 2nd is the skip list
+ * node to be inserted, the function should return true if search should be
+ * continued, otherwise return false.
+ * Returns 1 if @node is inserted as the first item of skip list at level zero,
+ * otherwise 0
+ */
+#define DEFINE_SKIPLIST_INSERT_FUNC(func_name, search_func)\
+static inline int func_name(struct skiplist_node *head, struct skiplist_node *node)\
+{\
+	struct skiplist_node *update[NUM_SKIPLIST_LEVEL];\
+	struct skiplist_node *p, *q;\
+	int k = head->level;\
+\
+	p = head;\
+	do {\
+		while (q = p->next[k], q != head && search_func(q, node))\
+			p = q;\
+		update[k] = p;\
+	} while (--k >= 0);\
+\
+	k = node->level;\
+	if (unlikely(k > head->level)) {\
+		node->level = k = ++head->level;\
+		update[k] = head;\
+	}\
+\
+	do {\
+		p = update[k];\
+		q = p->next[k];\
+		node->next[k] = q;\
+		p->next[k] = node;\
+		node->prev[k] = p;\
+		q->prev[k] = node;\
+	} while (--k >= 0);\
+\
+	return (p == head);\
+}
+
+/**
+ * skiplist_del_init -- delete skip list node from a skip list and reset it's
+ * init state
+ * @head: the header node of the skip list to be deleted from.
+ * @node: the skip list node to be deleted, the caller need to ensure @node is
+ * in skip list which @head represent.
+ * Returns 1 if @node is the first item of skip level at level zero, otherwise 0
+ */
+static inline int
+skiplist_del_init(struct skiplist_node *head, struct skiplist_node *node)
+{
+	int l, m = node->level;
+
+	for (l = 0; l <= m; l++) {
+		node->prev[l]->next[l] = node->next[l];
+		node->next[l]->prev[l] = node->prev[l];
+	}
+	if (m == head->level && m > 0) {
+		while (head->next[m] == head && m > 0)
+			m--;
+		head->level = m;
+	}
+	INIT_SKIPLIST_NODE(node);
+
+	return (node->prev[0] == head);
+}
+#endif /* _LINUX_SKIP_LIST_H */
diff --git a/init/Kconfig b/init/Kconfig
index 9f2ae1442f61..ea3f0fccbd31 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -802,6 +802,12 @@ config SCHED_BMQ
 	  responsiveness on the desktop and solid scalability on normal
 	  hardware and commodity servers.
 
+config SCHED_PDS
+	bool "PDS CPU scheduler"
+	help
+	  The Priority and Deadline based Skip list multiple queue CPU
+	  Scheduler.
+
 endchoice
 
 endif
diff --git a/init/init_task.c b/init/init_task.c
index ae55b7b6309c..f0f9fa2e6496 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -94,10 +94,15 @@ struct task_struct init_task
 		.fn = do_no_restart_syscall,
 	},
 #ifdef CONFIG_SCHED_ALT
-	.boost_prio	= 0,
 #ifdef CONFIG_SCHED_BMQ
+	.boost_prio	= 0,
 	.bmq_idx	= 15,
 	.bmq_node	= LIST_HEAD_INIT(init_task.bmq_node),
+#endif
+#ifdef CONFIG_SCHED_PDS
+	.deadline	= 0,
+	.sl_level	= 0,
+	.sl_node	= SKIPLIST_NODE_INIT(init_task.sl_node),
 #endif
 	.time_slice	= HZ,
 #else
diff --git a/kernel/locking/rtmutex.c b/kernel/locking/rtmutex.c
index c5802bafd782..31c46750fa94 100644
--- a/kernel/locking/rtmutex.c
+++ b/kernel/locking/rtmutex.c
@@ -232,6 +232,9 @@ static __always_inline bool unlock_rt_mutex_safe(struct rt_mutex *lock,
 static __always_inline int rt_mutex_waiter_less(struct rt_mutex_waiter *left,
 						struct rt_mutex_waiter *right)
 {
+#ifdef CONFIG_SCHED_PDS
+	return (left->deadline < right->deadline);
+#else
 	if (left->prio < right->prio)
 		return 1;
 
@@ -247,11 +250,15 @@ static __always_inline int rt_mutex_waiter_less(struct rt_mutex_waiter *left,
 #endif
 
 	return 0;
+#endif
 }
 
 static __always_inline int rt_mutex_waiter_equal(struct rt_mutex_waiter *left,
 						 struct rt_mutex_waiter *right)
 {
+#ifdef CONFIG_SCHED_PDS
+	return (left->deadline == right->deadline);
+#else
 	if (left->prio != right->prio)
 		return 0;
 
@@ -267,6 +274,7 @@ static __always_inline int rt_mutex_waiter_equal(struct rt_mutex_waiter *left,
 #endif
 
 	return 1;
+#endif
 }
 
 #define __node_2_waiter(node) \
diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 5db1f74f3559..407bc46de451 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -78,36 +78,6 @@ early_param("sched_timeslice", sched_timeslice);
  */
 int sched_yield_type __read_mostly = 1;
 
-#define rq_switch_time(rq)	((rq)->clock - (rq)->last_ts_switch)
-#define boost_threshold(p)	(sched_timeslice_ns >>\
-				 (15 - MAX_PRIORITY_ADJ -  (p)->boost_prio))
-
-static inline void boost_task(struct task_struct *p)
-{
-	int limit;
-
-	switch (p->policy) {
-	case SCHED_NORMAL:
-		limit = -MAX_PRIORITY_ADJ;
-		break;
-	case SCHED_BATCH:
-	case SCHED_IDLE:
-		limit = 0;
-		break;
-	default:
-		return;
-	}
-
-	if (p->boost_prio > limit)
-		p->boost_prio--;
-}
-
-static inline void deboost_task(struct task_struct *p)
-{
-	if (p->boost_prio < MAX_PRIORITY_ADJ)
-		p->boost_prio++;
-}
-
 #ifdef CONFIG_SMP
 static cpumask_t sched_rq_pending_mask ____cacheline_aligned_in_smp;
 
@@ -146,13 +116,22 @@ static cpumask_t sched_sg_idle_mask ____cacheline_aligned_in_smp;
 #endif
 static cpumask_t sched_rq_watermark[SCHED_BITS] ____cacheline_aligned_in_smp;
 
+#ifdef CONFIG_SCHED_BMQ
+#include "bmq_imp.h"
+#endif
+#ifdef CONFIG_SCHED_PDS
+#include "pds_imp.h"
+#endif
+
 static inline void update_sched_rq_watermark(struct rq *rq)
 {
-	unsigned long watermark = find_first_bit(rq->queue.bitmap, SCHED_BITS);
+	unsigned long watermark = sched_queue_watermark(rq);
 	unsigned long last_wm = rq->watermark;
 	unsigned long i;
 	int cpu;
 
+	/*printk(KERN_INFO "sched: watermark(%d) %d, last %d\n",
+	       cpu_of(rq), watermark, last_wm);*/
 	if (watermark == last_wm)
 		return;
 
@@ -187,13 +166,6 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 #endif
 }
 
-static inline int task_sched_prio(struct task_struct *p)
-{
-	return (p->prio < MAX_RT_PRIO)? p->prio : MAX_RT_PRIO / 2 + (p->prio + p->boost_prio) / 2;
-}
-
-#include "bmq_imp.h"
-
 static inline struct task_struct *rq_runnable_task(struct rq *rq)
 {
 	struct task_struct *next = sched_rq_first_task(rq);
@@ -456,6 +428,7 @@ static inline void dequeue_task(struct task_struct *p, struct rq *rq, int flags)
 {
 	lockdep_assert_held(&rq->lock);
 
+	/*printk(KERN_INFO "sched: dequeue(%d) %px %016llx\n", cpu_of(rq), p, p->priodl);*/
 	WARN_ONCE(task_rq(p) != rq, "sched: dequeue task reside on cpu%d from cpu%d\n",
 		  task_cpu(p), cpu_of(rq));
 
@@ -473,6 +446,7 @@ static inline void enqueue_task(struct task_struct *p, struct rq *rq, int flags)
 {
 	lockdep_assert_held(&rq->lock);
 
+	/*printk(KERN_INFO "sched: enqueue(%d) %px %016llx\n", cpu_of(rq), p, p->priodl);*/
 	WARN_ONCE(task_rq(p) != rq, "sched: enqueue task reside on cpu%d to cpu%d\n",
 		  task_cpu(p), cpu_of(rq));
 
@@ -498,10 +472,11 @@ static inline void enqueue_task(struct task_struct *p, struct rq *rq, int flags)
 static inline void requeue_task(struct task_struct *p, struct rq *rq)
 {
 	lockdep_assert_held(&rq->lock);
+	/*printk(KERN_INFO "sched: requeue(%d) %px %016llx\n", cpu_of(rq), p, p->priodl);*/
 	WARN_ONCE(task_rq(p) != rq, "sched: cpu[%d] requeue task reside on cpu%d\n",
 		  cpu_of(rq), task_cpu(p));
 
-	__requeue_task(p, rq);
+	__SCHED_REQUEUE_TASK(p, rq, update_sched_rq_watermark(rq));
 }
 
 /*
@@ -1428,7 +1403,7 @@ static int select_fallback_rq(int cpu, struct task_struct *p)
 	return dest_cpu;
 }
 
-static inline int select_task_rq(struct task_struct *p)
+static inline int select_task_rq(struct task_struct *p, struct rq *rq)
 {
 	cpumask_t chk_mask, tmp;
 
@@ -1441,7 +1416,7 @@ static inline int select_task_rq(struct task_struct *p)
 #endif
 	    cpumask_and(&tmp, &chk_mask, &sched_rq_watermark[IDLE_WM]) ||
 	    cpumask_and(&tmp, &chk_mask,
-			&sched_rq_watermark[task_sched_prio(p) + 1]))
+			&sched_rq_watermark[task_sched_prio(p, rq) + 1]))
 		return best_mask_cpu(task_cpu(p), &tmp);
 
 	return best_mask_cpu(task_cpu(p), &chk_mask);
@@ -1573,7 +1548,7 @@ EXPORT_SYMBOL_GPL(set_cpus_allowed_ptr);
 
 #else /* CONFIG_SMP */
 
-static inline int select_task_rq(struct task_struct *p)
+static inline int select_task_rq(struct task_struct *p, struct rq *rq)
 {
 	return 0;
 }
@@ -2039,10 +2014,9 @@ static int try_to_wake_up(struct task_struct *p, unsigned int state,
 	 */
 	smp_cond_load_acquire(&p->on_cpu, !VAL);
 
-	if(this_rq()->clock_task - p->last_ran > sched_timeslice_ns)
-		boost_task(p);
+	sched_task_ttwu(p);
 
-	cpu = select_task_rq(p);
+	cpu = select_task_rq(p, this_rq());
 
 	if (cpu != task_cpu(p)) {
 		wake_flags |= WF_MIGRATED;
@@ -2198,9 +2172,9 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 		 */
 		p->sched_reset_on_fork = 0;
 	}
+	update_task_priodl(p);
 
-	p->boost_prio = (p->boost_prio < 0) ?
-		p->boost_prio + MAX_PRIORITY_ADJ : MAX_PRIORITY_ADJ;
+	sched_task_fork(p);
 	/*
 	 * The child is not yet in the pid-hash so no cgroup attach races,
 	 * and the cgroup is pinned to this child due to cgroup_fork()
@@ -2224,6 +2198,7 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 
 	if (p->time_slice < RESCHED_NS) {
 		p->time_slice = sched_timeslice_ns;
+		time_slice_expired(p, rq);
 		resched_curr(rq);
 	}
 	raw_spin_unlock(&rq->lock);
@@ -2338,7 +2313,7 @@ void wake_up_new_task(struct task_struct *p)
 
 	p->state = TASK_RUNNING;
 
-	rq = cpu_rq(select_task_rq(p));
+	rq = cpu_rq(select_task_rq(p, this_rq()));
 #ifdef CONFIG_SMP
 	rseq_migrate(p);
 	/*
@@ -3436,11 +3411,7 @@ static inline void check_curr(struct task_struct *p, struct rq *rq)
 
 	if (p->time_slice < RESCHED_NS) {
 		p->time_slice = sched_timeslice_ns;
-		if (SCHED_FIFO != p->policy && task_on_rq_queued(p)) {
-			if (SCHED_RR != p->policy)
-				deboost_task(p);
-			requeue_task(p, rq);
-		}
+		time_slice_expired(p, rq);
 	}
 }
 
@@ -3476,6 +3447,7 @@ choose_next_task(struct rq *rq, int cpu, struct task_struct *prev)
 		if (!take_other_rq_tasks(rq, cpu)) {
 #endif
 			schedstat_inc(rq->sched_goidle);
+			/*printk(KERN_INFO "sched: choose_next_task(%d) idle %px\n", cpu, next);*/
 			return next;
 #ifdef	CONFIG_SMP
 		}
@@ -3485,6 +3457,8 @@ choose_next_task(struct rq *rq, int cpu, struct task_struct *prev)
 #ifdef CONFIG_HIGH_RES_TIMERS
 	hrtick_start(rq, next->time_slice);
 #endif
+	/*printk(KERN_INFO "sched: choose_next_task(%d) next %px\n", cpu,
+	 * next);*/
 	return next;
 }
 
@@ -3599,8 +3573,7 @@ static void __sched notrace __schedule(bool preempt)
 			 *
 			 * After this, schedule() must not care about p->state any more.
 			 */
-			if (rq_switch_time(rq) < boost_threshold(prev))
-				boost_task(prev);
+			sched_task_deactivate(prev, rq);
 			deactivate_task(prev, rq);
 
 			if (prev->in_iowait) {
@@ -3926,7 +3899,7 @@ EXPORT_SYMBOL(default_wake_function);
 static inline void check_task_changed(struct rq *rq, struct task_struct *p)
 {
 	/* Trigger resched if task sched_prio has been modified. */
-	if (task_on_rq_queued(p) && sched_task_need_requeue(p)) {
+	if (task_on_rq_queued(p) && sched_task_need_requeue(p, rq)) {
 		requeue_task(p, rq);
 		check_preempt_curr(rq);
 	}
@@ -4014,6 +3987,7 @@ void rt_mutex_setprio(struct task_struct *p, struct task_struct *pi_task)
 
 	trace_sched_pi_setprio(p, pi_task);
 	p->prio = prio;
+	update_task_priodl(p);
 
 	check_task_changed(rq, p);
 out_unlock:
@@ -4052,6 +4026,8 @@ void set_user_nice(struct task_struct *p, long nice)
 		goto out_unlock;
 
 	p->prio = effective_prio(p);
+	update_task_priodl(p);
+
 	check_task_changed(rq, p);
 out_unlock:
 	__task_access_unlock(p, lock);
@@ -4109,21 +4085,6 @@ SYSCALL_DEFINE1(nice, int, increment)
 
 #endif
 
-/**
- * task_prio - return the priority value of a given task.
- * @p: the task in question.
- *
- * Return: The priority value as seen by users in /proc.
- * RT tasks are offset by -100. Normal tasks are centered around 1, value goes
- * from 0(SCHED_ISO) up to 82 (nice +19 SCHED_IDLE).
- */
-int task_prio(const struct task_struct *p)
-{
-	if (p->prio < MAX_RT_PRIO)
-		return (p->prio - MAX_RT_PRIO);
-	return (p->prio - MAX_RT_PRIO + p->boost_prio);
-}
-
 /**
  * idle_cpu - is a given CPU idle currently?
  * @cpu: the processor in question.
@@ -4215,6 +4176,7 @@ static void __setscheduler(struct rq *rq, struct task_struct *p,
 	p->prio = normal_prio(p);
 	if (keep_boost)
 		p->prio = rt_effective_prio(p, p->prio);
+	update_task_priodl(p);
 }
 
 /*
@@ -4974,10 +4936,8 @@ static void do_sched_yield(void)
 	schedstat_inc(rq->yld_count);
 
 	if (1 == sched_yield_type) {
-		if (!rt_task(current)) {
-			current->boost_prio = MAX_PRIORITY_ADJ;
-			requeue_task(current, rq);
-		}
+		if (!rt_task(current))
+			do_sched_yield_type_1(current, rq);
 	} else if (2 == sched_yield_type) {
 		if (rq->nr_running > 1)
 			rq->skip = current;
diff --git a/kernel/sched/alt_sched.h b/kernel/sched/alt_sched.h
index d8887f377455..99be2c51c88d 100644
--- a/kernel/sched/alt_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -49,6 +49,9 @@
 #ifdef CONFIG_SCHED_BMQ
 #include "bmq.h"
 #endif
+#ifdef CONFIG_SCHED_PDS
+#include "pds.h"
+#endif
 
 /* task_struct::on_rq states: */
 #define TASK_ON_RQ_QUEUED	1
@@ -86,6 +89,9 @@ struct rq {
 
 #ifdef CONFIG_SCHED_BMQ
 	struct bmq queue;
+#endif
+#ifdef CONFIG_SCHED_PDS
+	struct skiplist_node sl_header;
 #endif
 	unsigned long watermark;
 
@@ -534,11 +540,6 @@ static inline void membarrier_switch_mm(struct rq *rq,
 }
 #endif
 
-static inline int task_running_nice(struct task_struct *p)
-{
-	return (p->prio + p->boost_prio > DEFAULT_PRIO + MAX_PRIORITY_ADJ);
-}
-
 #ifdef CONFIG_NUMA
 extern int sched_numa_find_closest(const struct cpumask *cpus, int cpu);
 #else
diff --git a/kernel/sched/bmq.h b/kernel/sched/bmq.h
index aba3c98759f8..aff0bb30a884 100644
--- a/kernel/sched/bmq.h
+++ b/kernel/sched/bmq.h
@@ -11,4 +11,10 @@ struct bmq {
 	struct list_head heads[SCHED_BITS];
 };
 
+
+static inline int task_running_nice(struct task_struct *p)
+{
+	return (p->prio + p->boost_prio > DEFAULT_PRIO + MAX_PRIORITY_ADJ);
+}
+
 #endif
diff --git a/kernel/sched/bmq_imp.h b/kernel/sched/bmq_imp.h
index 86d496ec23b3..d7df1d3f9495 100644
--- a/kernel/sched/bmq_imp.h
+++ b/kernel/sched/bmq_imp.h
@@ -1,5 +1,64 @@
 #define ALT_SCHED_VERSION_MSG "sched/bmq: BMQ CPU Scheduler 5.8-r1 by Alfred Chen.\n"
 
+/*
+ * BMQ only routines
+ */
+#define rq_switch_time(rq)	((rq)->clock - (rq)->last_ts_switch)
+#define boost_threshold(p)	(sched_timeslice_ns >>\
+				 (15 - MAX_PRIORITY_ADJ -  (p)->boost_prio))
+
+static inline void boost_task(struct task_struct *p)
+{
+	int limit;
+
+	switch (p->policy) {
+	case SCHED_NORMAL:
+		limit = -MAX_PRIORITY_ADJ;
+		break;
+	case SCHED_BATCH:
+	case SCHED_IDLE:
+		limit = 0;
+		break;
+	default:
+		return;
+	}
+
+	if (p->boost_prio > limit)
+		p->boost_prio--;
+}
+
+static inline void deboost_task(struct task_struct *p)
+{
+	if (p->boost_prio < MAX_PRIORITY_ADJ)
+		p->boost_prio++;
+}
+
+/*
+ * Common interfaces
+ */
+static inline int task_sched_prio(struct task_struct *p, struct rq *rq)
+{
+	return (p->prio < MAX_RT_PRIO)? p->prio : MAX_RT_PRIO / 2 + (p->prio + p->boost_prio) / 2;
+}
+
+static inline void requeue_task(struct task_struct *p, struct rq *rq);
+
+static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
+{
+	if (SCHED_FIFO != p->policy && task_on_rq_queued(p)) {
+		if (SCHED_RR != p->policy)
+			deboost_task(p);
+		requeue_task(p, rq);
+	}
+}
+
+static inline void update_task_priodl(struct task_struct *p) {}
+
+static inline unsigned long sched_queue_watermark(struct rq *rq)
+{
+	return find_first_bit(rq->queue.bitmap, SCHED_BITS);
+}
+
 static inline void sched_queue_init(struct rq *rq)
 {
 	struct bmq *q = &rq->queue;
@@ -61,26 +120,64 @@ sched_rq_next_task(struct task_struct *p, struct rq *rq)
 	sched_info_queued(rq, p);					\
 	psi_enqueue(p, flags);						\
 									\
-	p->bmq_idx = task_sched_prio(p);				\
+	p->bmq_idx = task_sched_prio(p, rq);				\
 	list_add_tail(&p->bmq_node, &rq->queue.heads[p->bmq_idx]);	\
 	set_bit(p->bmq_idx, rq->queue.bitmap)
 
-static inline void __requeue_task(struct task_struct *p, struct rq *rq)
+#define __SCHED_REQUEUE_TASK(p, rq, func)				\
+{									\
+	int idx = task_sched_prio(p, rq);				\
+\
+	list_del(&p->bmq_node);						\
+	list_add_tail(&p->bmq_node, &rq->queue.heads[idx]);		\
+	if (idx != p->bmq_idx) {					\
+		if (list_empty(&rq->queue.heads[p->bmq_idx]))		\
+			clear_bit(p->bmq_idx, rq->queue.bitmap);	\
+		p->bmq_idx = idx;					\
+		set_bit(p->bmq_idx, rq->queue.bitmap);			\
+		func;							\
+	}								\
+}
+
+static inline bool sched_task_need_requeue(struct task_struct *p, struct rq *rq)
 {
-	int idx = task_sched_prio(p);
-
-	list_del(&p->bmq_node);
-	list_add_tail(&p->bmq_node, &rq->queue.heads[idx]);
-	if (idx != p->bmq_idx) {
-		if (list_empty(&rq->queue.heads[p->bmq_idx]))
-			clear_bit(p->bmq_idx, rq->queue.bitmap);
-		p->bmq_idx = idx;
-		set_bit(p->bmq_idx, rq->queue.bitmap);
-		update_sched_rq_watermark(rq);
-	}
+	return (task_sched_prio(p, rq) != p->bmq_idx);
+}
+
+static void sched_task_fork(struct task_struct *p)
+{
+	p->boost_prio = (p->boost_prio < 0) ?
+		p->boost_prio + MAX_PRIORITY_ADJ : MAX_PRIORITY_ADJ;
+}
+
+/**
+ * task_prio - return the priority value of a given task.
+ * @p: the task in question.
+ *
+ * Return: The priority value as seen by users in /proc.
+ * RT tasks are offset by -100. Normal tasks are centered around 1, value goes
+ * from 0(SCHED_ISO) up to 82 (nice +19 SCHED_IDLE).
+ */
+int task_prio(const struct task_struct *p)
+{
+	if (p->prio < MAX_RT_PRIO)
+		return (p->prio - MAX_RT_PRIO);
+	return (p->prio - MAX_RT_PRIO + p->boost_prio);
+}
+
+static void do_sched_yield_type_1(struct task_struct *p, struct rq *rq)
+{
+	p->boost_prio = MAX_PRIORITY_ADJ;
+}
+
+static void sched_task_ttwu(struct task_struct *p)
+{
+	if(this_rq()->clock_task - p->last_ran > sched_timeslice_ns)
+		boost_task(p);
 }
 
-static inline bool sched_task_need_requeue(struct task_struct *p)
+static void sched_task_deactivate(struct task_struct *p, struct rq *rq)
 {
-	return (task_sched_prio(p) != p->bmq_idx);
+	if (rq_switch_time(rq) < boost_threshold(p))
+		boost_task(p);
 }
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
new file mode 100644
index 000000000000..9b9addc205a9
--- /dev/null
+++ b/kernel/sched/pds.h
@@ -0,0 +1,14 @@
+#ifndef PDS_H
+#define PDS_H
+
+/* bits:
+ * RT(0-99), (Low prio adj range, nice width, high prio adj range) / 2, cpu idle task */
+#define SCHED_BITS	(MAX_RT_PRIO + MAX_PRIORITY_ADJ * 2 + 8 + 1)
+#define IDLE_TASK_SCHED_PRIO	(SCHED_BITS - 1)
+
+static inline int task_running_nice(struct task_struct *p)
+{
+	return (p->prio > DEFAULT_PRIO);
+}
+
+#endif
diff --git a/kernel/sched/pds_imp.h b/kernel/sched/pds_imp.h
new file mode 100644
index 000000000000..b970879f1d2e
--- /dev/null
+++ b/kernel/sched/pds_imp.h
@@ -0,0 +1,205 @@
+#define ALT_SCHED_VERSION_MSG "sched/bmq: PDS CPU Scheduler 5.8-r0 by Alfred Chen.\n"
+
+static const u64 user_prio2deadline[NICE_WIDTH] = {
+/* -20 */	  6291456,   6920601,   7612661,   8373927,   9211319,
+/* -15 */	 10132450,  11145695,  12260264,  13486290,  14834919,
+/* -10 */	 16318410,  17950251,  19745276,  21719803,  23891783,
+/*  -5 */	 26280961,  28909057,  31799962,  34979958,  38477953,
+/*   0 */	 42325748,  46558322,  51214154,  56335569,  61969125,
+/*   5 */	 68166037,  74982640,  82480904,  90728994,  99801893,
+/*  10 */	109782082, 120760290, 132836319, 146119950, 160731945,
+/*  15 */	176805139, 194485652, 213934217, 235327638, 258860401
+};
+
+static const int dl_level_map[] = {
+/*      0           4           8           12           */
+	0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
+/*      16          20          24          28           */
+	1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 4, 4, 5, 6, 7
+};
+
+static inline int
+task_sched_prio(const struct task_struct *p, const struct rq *rq)
+{
+	u64 delta = (rq->clock + user_prio2deadline[39] - p->deadline) >> 23;
+
+	delta = min((size_t)delta, ARRAY_SIZE(dl_level_map) - 1);
+
+	return (p->prio < MAX_RT_PRIO)? p->prio : MAX_RT_PRIO + dl_level_map[delta];
+}
+
+static inline void update_task_priodl(struct task_struct *p)
+{
+	p->priodl = (((u64) (p->prio))<<56) | ((p->deadline)>>8);
+}
+
+static inline void requeue_task(struct task_struct *p, struct rq *rq);
+
+static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
+{
+	/*printk(KERN_INFO "sched: time_slice_expired(%d) - %px\n", cpu_of(rq), p);*/
+
+	if (p->prio >= MAX_RT_PRIO)
+		p->deadline = rq->clock + user_prio2deadline[TASK_USER_PRIO(p)];
+	update_task_priodl(p);
+
+	if (SCHED_FIFO != p->policy && task_on_rq_queued(p))
+		requeue_task(p, rq);
+}
+
+/*
+ * pds_skiplist_task_search -- search function used in PDS run queue skip list
+ * node insert operation.
+ * @it: iterator pointer to the node in the skip list
+ * @node: pointer to the skiplist_node to be inserted
+ *
+ * Returns true if key of @it is less or equal to key value of @node, otherwise
+ * false.
+ */
+static inline bool
+pds_skiplist_task_search(struct skiplist_node *it, struct skiplist_node *node)
+{
+	return (skiplist_entry(it, struct task_struct, sl_node)->priodl <=
+		skiplist_entry(node, struct task_struct, sl_node)->priodl);
+}
+
+/*
+ * Define the skip list insert function for PDS
+ */
+DEFINE_SKIPLIST_INSERT_FUNC(pds_skiplist_insert, pds_skiplist_task_search);
+
+/*
+ * Init the queue structure in rq
+ */
+static inline void sched_queue_init(struct rq *rq)
+{
+	FULL_INIT_SKIPLIST_NODE(&rq->sl_header);
+}
+
+/*
+ * Init idle task and put into queue structure of rq
+ * IMPORTANT: may be called multiple times for a single cpu
+ */
+static inline void sched_queue_init_idle(struct rq *rq, struct task_struct *idle)
+{
+	/*printk(KERN_INFO "sched: init(%d) - %px\n", cpu_of(rq), idle);*/
+	int default_prio = idle->prio;
+
+	idle->prio = MAX_PRIO;
+	idle->deadline = 0ULL;
+	update_task_priodl(idle);
+
+	FULL_INIT_SKIPLIST_NODE(&rq->sl_header);
+
+	idle->sl_node.level = idle->sl_level;
+	pds_skiplist_insert(&rq->sl_header, &idle->sl_node);
+
+	idle->prio = default_prio;
+}
+
+/*
+ * This routine assume that the idle task always in queue
+ */
+static inline struct task_struct *sched_rq_first_task(struct rq *rq)
+{
+	struct skiplist_node *node = rq->sl_header.next[0];
+
+	BUG_ON(node == &rq->sl_header);
+	return skiplist_entry(node, struct task_struct, sl_node);
+}
+
+static inline struct task_struct *
+sched_rq_next_task(struct task_struct *p, struct rq *rq)
+{
+	struct skiplist_node *next = p->sl_node.next[0];
+
+	BUG_ON(next == &rq->sl_header);
+	return skiplist_entry(next, struct task_struct, sl_node);
+}
+
+static inline unsigned long sched_queue_watermark(struct rq *rq)
+{
+	return task_sched_prio(sched_rq_first_task(rq), rq);
+}
+
+#define __SCHED_DEQUEUE_TASK(p, rq, flags, func)		\
+	psi_dequeue(p, flags & DEQUEUE_SLEEP);			\
+	sched_info_dequeued(rq, p);				\
+								\
+	if (skiplist_del_init(&rq->sl_header, &p->sl_node)) {	\
+		func;						\
+	}
+
+#define __SCHED_ENQUEUE_TASK(p, rq, flags)				\
+	sched_info_queued(rq, p);					\
+	psi_enqueue(p, flags);						\
+									\
+	p->sl_node.level = p->sl_level;					\
+	pds_skiplist_insert(&rq->sl_header, &p->sl_node)
+
+/*
+ * Requeue a task @p to @rq
+ */
+#define __SCHED_REQUEUE_TASK(p, rq, func)					\
+{\
+	bool b_first = skiplist_del_init(&rq->sl_header, &p->sl_node);		\
+\
+	p->sl_node.level = p->sl_level;						\
+	if (pds_skiplist_insert(&rq->sl_header, &p->sl_node) || b_first) {	\
+		func;								\
+	}									\
+}
+
+static inline bool sched_task_need_requeue(struct task_struct *p, struct rq *rq)
+{
+	struct skiplist_node *node = p->sl_node.prev[0];
+
+	if (node != &rq->sl_header) {
+		struct task_struct *t = skiplist_entry(node, struct task_struct, sl_node);
+
+		if (t->priodl > p->priodl)
+			return true;
+	}
+
+	node = p->sl_node.next[0];
+	if (node != &rq->sl_header) {
+		struct task_struct *t = skiplist_entry(node, struct task_struct, sl_node);
+
+		if (t->priodl < p->priodl)
+			return true;
+	}
+
+	return false;
+}
+
+static void sched_task_fork(struct task_struct *p) {}
+
+/**
+ * task_prio - return the priority value of a given task.
+ * @p: the task in question.
+ *
+ * Return: The priority value as seen by users in /proc.
+ * RT tasks are offset by -100. Normal tasks are centered around 1, value goes
+ * from 0(SCHED_ISO) up to 82 (nice +19 SCHED_IDLE).
+ */
+int task_prio(const struct task_struct *p)
+{
+	int ret;
+
+	if (p->prio < MAX_RT_PRIO)
+		return (p->prio - MAX_RT_PRIO);
+
+	preempt_disable();
+	ret = task_sched_prio(p, this_rq()) - MAX_RT_PRIO;
+	preempt_enable();
+
+	return ret;
+}
+
+static void do_sched_yield_type_1(struct task_struct *p, struct rq *rq)
+{
+	time_slice_expired(p, rq);
+}
+
+static void sched_task_ttwu(struct task_struct *p) {}
+static void sched_task_deactivate(struct task_struct *p, struct rq *rq) {}
-- 
2.32.0.93.g670b81a890

